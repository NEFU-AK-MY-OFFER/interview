# 分库分表

[MySQL：互联网公司常用分库分表方案汇总！ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/137368446)

[(2条消息) mysql分库分表理论及常用框架对比_mysql分库分表方案对比_Bob-SkyDance的博客-CSDN博客](https://blog.csdn.net/weixin_40770023/article/details/109811589)

[MySQL——百亿大表拆分实践 - 掘金 (juejin.cn)](https://juejin.cn/post/7069231909179490341)

[面试必备：分库分表经典15连问 - 掘金 (juejin.cn)](https://juejin.cn/post/7176424241531715621)

[阿里面试：我们为什么要分库分表 - 掘金 (juejin.cn)](https://juejin.cn/post/7085132195190276109)

[【数据库】分库分表策略_牛客网 (nowcoder.com)](https://www.nowcoder.com/discuss/353147187754377216?sourceSSR=search)

## 问题

### 基本概念

* 为什么要分库分表

* 什么时候分库分表

* 什么是分库分表，谈谈 4 种分库分表方式以及优缺点

* 分库分表工具？谈谈优缺点

* 分库分表步骤



### 分表方式

* 如何选择分表键

* 非分表键如何查询

* 分表策略如何选择

* 分表要停服吗？不停服怎么做？

      

### 分库分表问题

* 如何避免热点问题数据倾斜
* 分库后，事务问题如何解决
* 跨节点 Join 关联问题
* order by.group by 等聚合函数等问题
* 分库分表后的分页问题
* 分布式 ID





## 回答

### 为什么要分库分表

##### 数据库瓶颈

不管是IO瓶颈，还是CPU瓶颈，最终都会导致数据库的活跃连接数增加，进而逼近甚至达到数据库可承载活跃连接数的阈值。在业务Service来看就是，可用数据库连接少甚至无连接可用。接下来就可以想象了吧（并发量、吞吐量、崩溃）。

**1、IO瓶颈**

第一种：磁盘读IO瓶颈，热点数据太多，数据库缓存放不下，每次查询时会产生大量的IO，降低查询速度 -> **分库和垂直分表**。

第二种：网络IO瓶颈，请求的数据太多，网络带宽不够 -> **分库**。

**2、CPU瓶颈**

第一种：SQL问题，如SQL中包含join，group by，order by，非索引字段条件查询等，增加CPU运算的操作 -> SQL优化，建立合适的索引，在业务Service层进行业务计算。

第二种：单表数据量太大，查询时扫描的行太多，SQL效率低，CPU率先出现瓶颈 -> **水平分表**。



##### 单库单表存在的问题

（1）单库太大：数据库里面的表太多，所在服务器磁盘空间装不下，IO次数多CPU忙不过来。

（2）单表太大：一张表的字段太多，数据太多。查询起来困难。

此时就开始考虑如何解决问题了。

##### **主从复制架构**

单库单表下越来越

不满足需求，此时我们先考虑进行读写分离。我们将数据库的写操作和读操作进行分离， 使用多个从库副本（Slaver）负责读，使用主库（Master）负责写， 从库从主库同步更新数据，保持数据一致。

这在一定程度上可以解决问题，但是用户超级多的时候，比如几个亿用户，此时写操作会越来越多，一个主库（Master）不能满足要求了，那就把主库拆分，这时候为了保证数据的一致性就要开始进行同步，此时会带来一系列问题：

（1）写操作拓展起来比较困难，因为要保证多个主库的数据一致性。

（2）复制延时：意思是同步带来的时间消耗。

（3）锁表率上升：读写分离，命中率少，锁表的概率提升。

（4）表变大，缓存率下降：此时缓存率一旦下降，带来的就是时间上的消耗。

注意，此时主从复制还是单库单表，只不过复制了很多份并进行同步。

主从复制架构随着用户量的增加、访问量的增加、数据量的增加依然会带来大量的问题，那就要考虑换一种解决思路。就是今天所讲的主题，分库分表。



1.1 为什么要分库

如果业务量剧增，数据库可能会出现性能瓶颈，这时候我们就需要考虑拆分数据库。从这两方面来看：

- **磁盘存储**

业务量剧增，MySQL单机磁盘容量会撑爆，拆成多个数据库，磁盘使用率大大降低。

- **并发连接支撑**

我们知道数据库连接数是有限的。在**高并发的场景下**，大量请求访问数据库，MySQL单机是扛不住的！高并发场景下，会出现`too many connections`报错。

当前非常火的微服务架构出现，就是为了应对高并发。它把订单、用户、商品等不同模块，拆分成多个应用，并且把单个数据库也拆分成多个不同功能模块的数据库（订单库、用户库、商品库），以分担读写压力。

1.2 为什么要分表



**假如你的单表数据量非常大，存储和查询的性能就会遇到瓶颈了，如果你做了很多优化之后还是无法提升效率的时候，就需要考虑做分表了。一般千万级别数据量，就需要分表。**

这是因为即使`SQL`命中了索引，如果表的数据量超过一千万的话，查询也是会明显变慢的。这是因为索引一般是`B+`树结构，数据千万级别的话，B+树的高度会增高，查询就变慢啦。MySQL的B+树的高度怎么计算的呢？跟大家复习一下：

InnoDB存储引擎最小储存单元是页，一页大小就是16k。B+树叶子存的是数据，内部节点存的是键值+指针。索引组织表通过非叶子节点的二分查找法以及指针确定数据在哪个页中，进而再去数据页中找到需要的数据，B+树结构图如下：

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/735a83c5b37041d39d615ec4dce61266~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)

> 假设B+树的高度为2的话，即有一个根结点和若干个叶子结点。这棵B+树的存放总记录数为=根结点指针数*单个叶子节点记录行数。
>
> 如果一行记录的数据大小为1k，那么单个叶子节点可以存的记录数  =16k/1k =16. 非叶子节点内存放多少指针呢？我们假设主键ID为bigint类型，长度为8字节(面试官问你int类型，一个int就是32位，4字节)，而指针大小在InnoDB源码中设置为6字节，所以就是 8+6=14 字节，16k/14B =16*1024B/14B = 1170
>
> 因此，一棵高度为2的B+树，能存放1170 * 16=18720条这样的数据记录。同理一棵高度为3的B+树，能存放1170 *1170 *16 =21902400，大概可以存放两千万左右的记录。B+树高度一般为1-3层，如果B+到了4层，查询的时候会多查磁盘的次数，SQL就会变慢。

因此单表数据量太大，SQL查询会变慢，所以就需要考虑分表啦。

作者：捡田螺的小男孩
链接：https://juejin.cn/post/7176424241531715621
来源：稀土掘金
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



### 什么时候分库分表

* 不到万不得已不做分库分表，会增加业务复杂度，避免过度设计和过度优化

    * 可以优先考虑升级硬件、升级网络、读写分离、索引优化等等。当数据量达到单表的瓶颈时候，再考虑分库分表。
    * 一般什么类型业务表需要才分库分表？通用是一些**流水表、用户表**等才考虑分库分表，如果是一些配置类的表，则完全不用考虑，因为不太可能到达这个量级。

* 经验

    * 分表

        * 对于`MySQL`，`InnoDB`存储引擎的话，单表最多可以存储`10亿`级数据。但是的话，如果真的存储这么多，性能就会非常差。一般数据量千万级别，`B+`树索引高度就会到`3`层以上了，查询的时候会多查磁盘的次数，`SQL`就会变慢。
        * 阿里巴巴的`《Java开发手册》`提出：单表行数超过`500万`行或者单表容量超过`2GB`，才推荐进行分库分表。
            * 那我们是不是等到数据量到达五百万，才开始分库分表呢？不是这样的，我们应该**提前规划分库分表**，如果估算`3`年后，你的表都不会到达这个五百万，则不需要分库分表。
            * MySQL服务器如果配置更好，是不是可以超过这个500万这个量级，才考虑分库分表？虽然配置更好，可能数据量大之后，性能还是不错，但是如果持续发展的话，还是要考虑分库分表

    * 分库

        * 分库就是你一个库一般我们经验而言，最多支撑到并发 2000，一定要扩容了，而且一个健康的单库并发值你最好保持在每秒 1000 左右，不要太大。那么你可以将一个库的数据拆分到多个库中，访问的时候就访问一个库好了。
        * 对于MySQL来说的话，一般单库超过`5千万`记录，`DB`的压力就非常大了。所以分库数量多少，需要看单库处理记录能力有关。
        * 如果分库数量少，达不到分散存储和减轻`DB`性能压力的目的；如果分库的数量多，对于跨多个库的访问，应用程序需要访问多个库。一般是建议分`4~10`个库，我们公司的企业客户信息，就分了`10`个库。

        



### 什么是分库分表

##### 水平拆分

* 是把一个表的数据给弄到多个库的多个表里去
* 每个库的表结构都一样，只不过每个库表放的数据是不同的，所有库表的数据加起来就是全部数据。
* 水平拆分的意义：就是将数据均匀放更多的库里，然后用多个库来抗更高的并发，还有就是用多个库的存储容量来进行扩容。





##### **垂直拆分**

* 把一个有很多字段的表给拆分成多个表**，**或者是多个库上去。
* 每个库表的结构都不一样，每个库表都包含部分字段。
* 垂直拆分的意义：一般来说，会**将较少的访问频率很高的字段放到一个表里去**，然后**将较多的访问频率很低的字段放到另外一个表里去**。因为数据库是有缓存的，你访问频率高的行字段越少，就可以在缓存里缓存更多的行，性能就越好。这个一般在表层面做的较多一些。



##### 水平分库

![img](https://pic3.zhimg.com/80/v2-c6de98fbf6dd9cd1cef6ced803294f8e_1440w.webp)

* 概念：以**字段**为依据，按照一定策略（hash、range等），将一个**库**中的数据拆分到多个**库**中。

* 结果：

    - 每个**库**的**结构**都一样；

    - 每个**库**的**数据**都不一样，没有交集；

    - 所有**库**的**并集**是全量数据；

* 场景：系统绝对并发量上来了，分表难以根本上解决问题，并且还没有明显的业务归属来垂直分库。

* 分析：库多了，io和cpu的压力自然可以成倍缓解。

##### 水平分表

![img](https://pic4.zhimg.com/80/v2-5a5447ec26f1391c00fcb1c97e416433_1440w.webp)

* 概念：以**字段**为依据，按照一定策略（hash、range等），将一个**表**中的数据拆分到多个**表**中。

* 结果：

    - 每个**表**的**结构**都一样；

    - 每个**表**的**数据**都不一样，没有交集；

    - 所有**表**的**并集**是全量数据；

* 场景：系统绝对并发量并没有上来，只是单表的数据量太多，影响了SQL效率，加重了CPU负担，以至于成为瓶颈。

* 分析：表的数据量少了，单次SQL执行效率高，自然减轻了CPU的负担。

##### 垂直分库

![img](https://pic2.zhimg.com/80/v2-6484e798eeb54df53020af70c4d6645d_1440w.webp)

* 概念：以**表**为依据，按照业务归属不同，将不同的**表**拆分到不同的**库**中。

* 结果：

    - 每个**库**的**结构**都不一样；

    - 每个**库**的**数据**也不一样，没有交集；

    - 所有**库**的**并集**是全量数据；

* 场景：系统绝对并发量上来了，并且可以抽象出单独的业务模块。

* 分析：到这一步，基本上就可以服务化了。例如，随着业务的发展一些公用的配置表、字典表等越来越多，这时可以将这些表拆到单独的库中，甚至可以服务化。再有，随着业务的发展孵化出了一套业务模式，这时可以将相关的表拆到单独的库中，甚至可以服务化。

##### 垂直分表

![img](https://pic4.zhimg.com/80/v2-cd623744d880d155a6c513079e52b7af_1440w.webp)

* 概念：以**字段**为依据，按照字段的活跃性，将**表**中字段拆到不同的**表**（主表和扩展表）中。

* 结果：

    - 每个**表**的**结构**都不一样；

    - 每个**表**的**数据**也不一样，一般来说，每个表的**字段**至少有一列交集，一般是主键，用于关联数据；

    - 所有**表**的**并集**是全量数据；

* 场景：系统绝对并发量并没有上来，表的记录并不多，但是字段多，并且热点数据和非热点数据在一起，单行数据所需的存储空间较大。以至于数据库缓存的数据行减少，查询时会去读磁盘数据产生大量的随机读IO，产生IO瓶颈。

* 分析：可以用列表页和详情页来帮助理解。垂直分表的拆分原则是将热点数据（可能会冗余经常一起查询的数据）放在一起作为主表，非热点数据放在一起作为扩展表。这样更多的热点数据就能被缓存下来，进而减少了随机读IO。拆了之后，要想获得全部数据就需要关联两个表来取数据。但记住，千万别用join，因为join不仅会增加CPU负担并且会讲两个表耦合在一起（必须在一个数据库实例上）。关联数据，应该在业务Service层做文章，分别获取主表和扩展表数据然后用关联字段关联得到全部数据。





### 用过哪些分库分表工具？优缺点

- cobar
    - 阿里 b2b 团队开发和开源的
    - 代理方式，属于 proxy 层方案。
    - 早些年还可以用，但是最近几年都没更新了，基本没啥人用，差不多算是被抛弃的状态吧。而且不支持读写分离、存储过程、跨库 join 和分页等操作。

- Atlas
    - 360 开源的
    - 代理方式，属于 proxy 层方案
    - 以前是有一些公司在用的，但是确实有一个很大的问题就是社区最新的维护都在 5 年前了。所以，现在用的公司基本也很少了。
    - 不能实现分布式分表，所有表在同一个库中

- sharding-sphere：jar，前身是sharding-jdbc；
- TDDL：jar，Taobao Distribute Data Layer；
    - 淘宝团队开发的
    - 非代理方式，属于 client 层方案。
    - 支持基本的 crud 语法和读写分离，但不支持 join、多表查询等语法。目前使用的也不多，因为还依赖淘宝的 diamond 配置管理系统。
    - 功能强大，过于复杂，部分开源

- Mycat：中间件。
    - 基于 cobar 改造的，属于 proxy 层方案，支持的功能非常完善，而且目前应该是非常火的而且不断流行的数据库中间件，社区很活跃，也有一些公司开始在用了。但是确实相比于 sharding jdbc 来说，年轻一些，经历的锤炼少一些。

- Sharding-jdbc
    - 当当开源的
    - 非代理方式，属于 client 层方案，JDBC 层面操作。
    - 确实之前用的还比较多一些，因为 SQL 语法支持也比较多，没有太多限制，而且目前推出到了 2.0 版本，支持分库分表、读写分离、分布式 id 生成、柔性事务（最大努力送达型事务、TCC 事务）。而且确实之前使用的公司会比较多一些（这个在官网有登记使用的公司，可以看到从 2017 年一直到现在，是有不少公司在用的），目前社区也还一直在开发和维护，还算是比较活跃，个人认为算是一个现在也**可以选择的方案**。

- vitess
    - 谷歌产品
    - 汲取基于 ZooKeeper 管理，通过 RPC 进行数据处理，可支撑高流量








### 分表方式

#### 如何选择分表键

分表键，即用来**分库/分表**的字段，换种说法就是，你以哪个维度来分库分表的。比如你**按用户ID分表、按时间分表、按地区分表**，这些**用户ID、时间、地区**就是分表键。

一般数据库表拆分的原则，需要先找到**业务的主题**。比如你的数据库表是一张企业客户信息表，就可以考虑用了**客户号**做为分表键。

**为什么考虑用客户号做分表键呢？**

> 这是因为表是基于客户信息的，所以，需要将同一个客户信息的数据，落到一个表中，**避免触发全表路由**。



#### 非分表键如何查询

分库分表后，有时候无法避免一些业务场景，**需要通过非分表键来查询**。

假设一张用户表，根据`userId`做分表键，来分库分表。但是用户登录时，需要根据**用户手机号**来登陆。这时候，就需要通过手机号查询用户信息。而**手机号是非分表键**。

非分表键查询，一般有这几种方案：

- **遍历**：最粗暴的方法，就是遍历所有的表，找出符合条件的手机号记录（**不建议**）
- **将用户信息冗余同步到ES**，同步发送到ES，然后通过ES来查询（**推荐**）

其实还有**基因法**：比如非分表键可以解析出分表键出来，比如常见的，订单号生成时，可以包含客户号进去，通过订单号查询，就可以解析出客户号。但是这个场景除外，**手机号似乎不适合冗余userId**。



#### 分表策略

##### range范围

`range`，即范围策略划分表。比如我们可以将表的主键`order_id`，按照从`0~300万`的划分为一个表，`300万~600万`划分到另外一个表。如下图：

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7b04405ba6244a5ab9e5114154711870~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)

有时候我们也可以按时间范围来划分，如不同年月的订单放到不同的表，它也是一种`range`的划分策略。

- 优点： Range范围分表，有利于扩容。
- 缺点： 可能会有热点问题。因为`订单id`是一直在增大的，也就是说最近一段时间都是汇聚在一张表里面的。比如最近一个月的订单都在`300万~600万`之间，平时用户一般都查最近一个月的订单比较多，请求都打到`order_1`表啦。

##### hash取模

**hash取模策略：**

> 指定的路由key（一般是`user_id、order_id、customer_no`作为key）对分表总数进行取模，把数据分散到各个表中。

比如原始订单表信息，我们把它分成4张分表：

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3ffd2c112edf4a06a875171ca19e8300~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)

- 比如id=1，对4取模，就会得到1，就把它放到t_order_1;
- id=3，对4取模，就会得到3，就把它放到t_order_3;

一般，我们会取**哈希值，再做取余**：

```matlab
Math.abs(orderId.hashCode()) % table_number
复制代码
```

- 优点：hash取模的方式，**不会存在明显的热点问题**。
- 缺点：如果未来某个时候，表数据量又到瓶颈了，需要扩容，就比较麻烦。所以一般建议提前规划好，一次性分够。（可以考虑**一致性哈希**）

##### 一致性Hash

如果**用hash方式**分表，前期规划不好，需要**扩容二次分表，表的数量需要增加，所以hash值需要重新计算**，这时候需要迁移数据了。

> 比如我们开始分了`10`张表，之后业务扩展需要，增加到`20`张表。那问题就来了，之前根据`orderId`取模`10`后的数据分散在了各个表中，现在需要重新对所有数据重新取模`20`来分配数据

为了解决这个**扩容迁移**问题，可以使用**一致性hash思想**来解决。

> **一致性哈希**：在移除或者添加一个服务器时，能够尽可能小地改变已存在的服务请求与处理请求服务器之间的映射关系。一致性哈希解决了简单哈希算法在分布式哈希表存在的**动态伸缩**等问题



##### range + hash 取模



#### 分表要停服吗？不停服怎么做？

不用停服。不停服的时候，应该怎么做呢，主要分五个步骤：

1. 编写代理层，加个开关（控制访问新的`DAO`还是老的`DAO`，或者是都访问），灰度期间，还是访问老的`DAO`。
2. 发版全量后，开启双写，既在旧表新增和修改，也在新表新增和修改。日志或者临时表记下新表ID起始值，旧表中小于这个值的数据就是存量数据，这批数据就是要迁移的。
3. 通过脚本把旧表的存量数据写入新表。
4. 停读旧表改读新表，此时新表已经承载了所有读写业务，但是这时候不要立刻停写旧表，需要保持双写一段时间。
5. 当读写新表一段时间之后，如果没有业务问题，就可以停写旧表啦



### 分库分表分表步骤

1. 根据容量（当前容量和增长量）评估分库或分表个数 
2. 选key（均匀）
3. 分表规则（hash或range等）
4. 执行（一般双写）
5. 扩容问题（尽量减少数据的移动）。



### 分库分表问题

##### **1、非partition key的查询问题**

基于水平分库分表，拆分策略为常用的hash法。

1. **端上**除了partition key只有一个非partition key作为条件查询

- **映射法**

![img](https://pic2.zhimg.com/80/v2-c39e8b19c10f1ccc719508a3f65a0441_1440w.webp)



- **基因法**

![img](https://pic2.zhimg.com/80/v2-238884fb2fc24eef1c96cf3e4d48012d_1440w.webp)


注：写入时，基因法生成user_id，如图。关于xbit基因，例如要分8张表，23=8，故x取3，即3bit基因。根据user_id查询时可直接取模路由到对应的分库或分表。根据user_name查询时，先通过user_name_code生成函数生成user_name_code再对其取模路由到对应的分库或分表。id生成常用**snowflake算法**。

1. **端上**除了partition key不止一个非partition key作为条件查询

- **映射法**

![img](https://pic1.zhimg.com/80/v2-738b045d0bb5d67952376f2201fd80ec_1440w.webp)



- **冗余法**

![img](https://pic3.zhimg.com/80/v2-e9612281a3049388feae9d74097d5e5e_1440w.webp)


注：按照order_id或buyer_id查询时路由到db_o_buyer库中，按照seller_id查询时路由到db_o_seller库中。感觉有点本末倒置！有其他好的办法吗？改变技术栈呢？

1. **后台**除了partition key还有各种非partition key组合条件查询

- NoSQL法

![img](https://pic4.zhimg.com/80/v2-df65243ff3b05283ce96e0ed4f2bf89b_1440w.webp)



- 冗余法

![img](https://pic3.zhimg.com/80/v2-caf0bd6f63526de291b3162ccf1abe0e_1440w.webp)



##### **2、非partition key跨库跨表分页查询问题**

基于水平分库分表，拆分策略为常用的hash法。

注：用**NoSQL法**解决（ES等）。

##### **3、扩容问题**

基于水平分库分表，拆分策略为常用的hash法。

1. 水平扩容库（升级从库法）

![img](https://pic2.zhimg.com/80/v2-789f54a192017f3fa3138d3bf768cbad_1440w.webp)


注：扩容是成倍的。

1. 水平扩容表（双写迁移法）

![img](https://pic2.zhimg.com/80/v2-4847eb9e869a90b8eb293ecdfeb19acd_1440w.webp)

第一步：（同步双写）修改应用配置和代码，加上双写，部署；第二步：（同步双写）将老库中的老数据复制到新库中；第三步：（同步双写）以老库为准校对新库中的老数据；第四步：（同步双写）修改应用配置和代码，去掉双写，部署；

注：**双写**是通用方案。







#### 如何避免热点问题数据倾斜

如果我们根据**时间范围**分片，某电商公司11月搞营销活动，那么大部分的数据都落在11月份的表里面了，其他分片表可能很少被查询，即**数据倾斜**了，有热点数据问题了。

我们可以使用`range范围+ hash哈希取模`结合的分表策略，简单的做法就是：

> 在拆分库的时候，我们可以先用range范围方案，比如订单id在`0~4000万`的区间，划分为订单库1;id在`4000万~8000万`的数据，划分到`订单库2`,将来要扩容时，id在`8000万~1.2亿`的数据，划分到订单库3。然后订单库内，再用`hash取模`的策略，把不同订单划分到不同的表。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9fefe85f8f3e4fcf93bb9b1a3edb0247~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)





#### 分库后，事务问题如何解决

分库分表后，假设两个表在不同的数据库，那么**本地事务已经无效**啦，需要使用**分布式事务**了。

常用的分布式事务解决方案有：

- 两阶段提交
- 三阶段提交
- TCC
- 本地消息表
- 最大努力通知
- saga

大家可以看下这几篇文章：

- [后端程序员必备：分布式事务基础篇](https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg3NzU5NTIwNg%3D%3D%26mid%3D2247487969%26idx%3D1%26sn%3D1a7c255439810aa12d0417a69c709bce%26chksm%3Dcf21cec8f85647dee38af93bb6747fc4e597c9918ad83f5ad30fe726f2918e017e390b2b7413%26token%3D162724582%26lang%3Dzh_CN%26scene%3D21%23wechat_redirect)
- [看一遍就理解：分布式事务详解](https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg3NzU5NTIwNg%3D%3D%26mid%3D2247498358%26idx%3D1%26sn%3Daa6c7ceb61b73267d68d1b4fb7ccc2ed%26chksm%3Dcf22255ff855ac495861d57df276517e89779006267fa8413fe925cc15b0c3e0b0f1b1a5675e%26token%3D810040944%26lang%3Dzh_CN%23rd)
- [框架篇：分布式一致性解决方案](https://link.juejin.cn?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMzg3NzU5NTIwNg%3D%3D%26mid%3D2247490468%26idx%3D2%26sn%3D91b8e5dd2ce3db218708b5c736fce700%26chksm%3Dcf21c48df8564d9b30164e1dbf9b5ebcc1847a9450d08ee146c98eb53107af475149ad12a748%26token%3D810040944%26lang%3Dzh_CN%23rd)



#### 跨节点 Join 关联问题

在单库未拆分表之前，我们如果要使用`join`关联多张表操作的话，简直`so easy`啦。但是分库分表之后，两张表可能都不在同一个数据库中了，那么如何跨库`join`操作呢？

跨库Join的几种解决思路：

- **字段冗余**：把需要关联的字段放入主表中，避免关联操作；比如订单表保存了卖家ID（`sellerId`），你把卖家名字`sellerName`也保存到订单表，这就不用去关联卖家表了。这是一种空间换时间的思想。
- **全局表**：比如系统中所有模块都可能会依赖到的一些基础表（即全局表），在每个数据库中均保存一份。
- **数据抽象同步**：比如A库中的a表和B库中的b表有关联，可以定时将指定的表做同步，将数据汇合聚集，生成新的表。一般可以借助`ETL`工具。
- **应用层代码组装**：分开多次查询，调用不同模块服务，获取到数据后，代码层进行字段计算拼装。



#### order by.group by 等聚合函数等问题

跨节点的`count,order by,group by`以及聚合函数等问题，都是一类的问题，它们一般都需要基于全部数据集合进行计算。可以分别在各个节点上得到结果后，再在应用程序端进行合并。



#### 分库分表后的分页问题

- 方案1（**全局视野法**）：在各个数据库节点查到对应结果后，在代码端汇聚再分页。这样优点是业务无损，精准返回所需数据；缺点则是会**返回过多数据，增大网络传输**，也会造成空查，

> 比如分库分表前，你是根据**创建时间排序**，然后**获取第2页数据**。如果你是分了**两个库**，那你就可以每个库都根据时间排序，然后都返回**2页**数据，然后把两个数据库查询回来的数据**汇总**，再根据创建时间进行**内存排序**，最后再取第**2**页的数据。

- 方案2（**业务折衷法-禁止跳页查询**）：这种方案需要业务妥协一下，只有上一页和下一页，不允许跳页查询了。

> 这种方案，查询第一页时，是跟全局视野法一样的。但是下一页时，需要把当前最大的创建时间传过来，然后每个节点，都查询大于创建时间的一页数据，接着汇总，内存排序返回。



#### 分布式 ID

数据库被切分后，不能再依赖数据库自身的主键生成机制啦，最简单可以考虑`UUID`，或者使用**雪花算法**生成`分布式ID`。

> 雪花算法是一种生成分布式全局唯一ID的算法，生成的ID称为Snowflake IDs。这种算法由Twitter创建，并用于推文的ID。

一个`Snowflake ID`有`64`位。

- 第`1`位：Java中long的最高位是符号位代表正负，正数是0，负数是1，一般生成ID都为正数，所以默认为0。
- 接下来前`41`位是时间戳，表示了自选定的时期以来的毫秒数。
- 接下来的`10`位代表计算机ID，防止冲突。
- 其余`12`位代表每台机器上生成ID的序列号，这允许在同一毫秒内创建多个Snowflake ID。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c690f328d9f440cc86d420abe098903e~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp)



### 数据库扩容

[数据库扩容方案 - 秦羽的思考 - 博客园 (cnblogs.com)](https://www.cnblogs.com/xuwc/p/14122187.html)

#### 一、缘起

------

（1）并发量大，流量大的互联网架构，一般来说，数据库上层都有一个**服务层**，服务层记录了“业务库名”与“数据库实例”的映射关系，通过**数据库连接池**向数据库路由sql语句以执行：

![单库服务层](https://atts.w3cschool.cn/attachments/image/20170508/1494236128225666.png)
如上图：服务层配置用户库user对应的数据库实例物理位置为ip（其实是一个内网域名）。

（2）随着数据量的增大，数据要进行**水平切分**，分库后将数据分布到不同的数据库实例（甚至物理机器）上，以达到降低数据量，增强性能的扩容目的：
![数据水平切分](https://atts.w3cschool.cn/attachments/image/20170508/1494236224378471.png)
如上图：用户库user分布在两个实例上，ip0和ip1，服务层通过用户标识uid取模的方式进行寻库路由，模2余0的访问ip0上的user库，模2余1的访问ip1上的user库。

关于数据库水平切分，垂直切分的更多细节，详见《[一分钟掌握数据库垂直拆分](https://www.w3cschool.cn/architectroad/architectroad-vertical-split-database.html)》。

（3）互联网架构需要保证**数据库高可用**，常见的一种方式，使用双主同步+keepalived+虚ip的方式保证数据库的可用性：
![虚ip，高可用](https://atts.w3cschool.cn/attachments/image/20170508/1494236441421302.png)
如上图：两个相互同步的主库使用相同的虚ip。
![虚ip漂移](https://atts.w3cschool.cn/attachments/image/20170508/1494236486967203.png)
如上图：当主库挂掉的时候，虚ip自动漂移到另一个主库，**整个过程对调用方透明**，通过这种方式保证数据库的高可用。

关于高可用的更多细节，详见《[究竟啥才是互联网架构“高可用”](https://www.w3cschool.cn/architectroad/architectroad-high-availability.html)》。

（4）综合上文的（2）和（3），线上实际的架构，既有水平切分，又有高可用保证，所以实际的数据库架构是这样的： 
![水平切分+高可用](https://atts.w3cschool.cn/attachments/image/20170508/1494236627780870.png)



**提问**：如果数据量持续增大，分2个库性能扛不住了，该怎么办呢？

**回答**：继续水平拆分，拆成更多的库，降低单库数据量，增加库主库实例（机器）数量，提高性能。


**最终问题抛出**：分成x个库后，随着数据量的增加，要增加到y个库，数据库扩容的过程中，能否平滑，持续对外提供服务，保证服务的可用性，是本文要讨论的问题。



#### 二、停服务方案

------

在讨论平滑方案之前，先简要说明下“x库拆y库”停服务的方案：

（1）站点挂一个公告“为了为广大用户提供更好的服务，本站点/游戏将在今晚00:00-2:00之间升级，届时将不能登录，用户周知”

（2）停服务

（3）新建y个库，做好高可用

（4）数据迁移，重新分布，写一个数据迁移程序，从x个库里导入到y个库里，路由规则由%x升级为%y

（5）修改服务配置，原来x行配置升级为y行

（6）重启服务，连接新库重新对外提供服务

整个过程中，**最耗时的是第四步数据迁移**。

 

**回滚方案**：

如果数据迁移失败，或者迁移后测试失败，则将配置改回x库，恢复服务，改天再挂公告。


**方案优点**：简单



**方案缺点**：

（1）停服务，不高可用

（2）技术同学压力大，所有工作要在规定时间内做完，根据经验，压力越大约容易出错（这一点很致命）

（3）如果有问题第一时间没检查出来，启动了服务，运行一段时间后再发现有问题，难以回滚，需要回档，可能会丢失一部分数据

 

**适用：**

1. 小型网站；
2. 大部分游戏；
3. 对高可用要求不高的服务。


有没有更平滑的方案呢？



#### 三、秒级、平滑、帅气方案

------

![水平切分+高可用](https://atts.w3cschool.cn/attachments/image/20170508/1494236954429441.png)
再次看一眼扩容前的架构，分两个库，假设每个库1亿数据量，如何**平滑扩容，增加实例数，降低单库数据量**呢？三个简单步骤搞定。

**（1）修改配置**
![修改配置](https://atts.w3cschool.cn/attachments/image/20170508/1494237033432458.png)

主要修改两处：

a）数据库实例所在的机器做双虚ip，原来%2=0的库是虚ip0，现在增加一个虚ip00，%2=1的另一个库同理

b）修改服务的配置（不管是在配置文件里，还是在配置中心），将2个库的数据库配置，改为4个库的数据库配置，**修改的时候要注意旧库与新库的映射关系**：

%2=0的库，会变为%4=0与%4=2；

%2=1的部分，会变为%4=1与%4=3；

这样修改是为了保证，拆分后依然能够路由到正确的数据。


**（2）reload配置，实例扩容**
![重启reload配置，完成扩库](https://atts.w3cschool.cn/attachments/image/20170508/1494237158839818.jpg)

服务层reload配置，reload可能是这么几种方式：

a）比较原始的，重启服务，读新的配置文件

b）高级一点的，配置中心给服务发信号，重读配置文件，重新初始化数据库连接池


不管哪种方式，reload之后，数据库的**实例扩容就完成了**，原来是2个数据库实例提供服务，现在变为4个数据库实例提供服务，这个过程一般可以在秒级完成。



整个过程可以逐步重启，**对服务的正确性和可用性完全没有影响**：

a）即使%2寻库和%4寻库同时存在，也不影响数据的正确性，因为此时仍然是双主数据同步的

b）服务reload之前是不对外提供服务的，冗余的服务能够保证高可用


完成了实例的扩展，会发现每个数据库的数据量依然没有下降，所以第三个步骤还要做一些收尾工作。

**（3）收尾工作，数据收缩**
![收尾工作](https://atts.w3cschool.cn/attachments/image/20170508/1494237302762032.jpg)

有这些一些**收尾工作**：

a）把双虚ip修改回单虚ip

b）解除旧的双主同步，让成对库的数据不再同步增加

c）增加新的双主同步，保证高可用

d）删除掉冗余数据，例如：ip0里%4=2的数据全部干掉，只为%4=0的数据提供服务啦


这样下来，**每个库的数据量就降为原来的一半，数据收缩完成**。



 

**优点**

1. 扩容期间，服务照常进行，**保证高可用**；
2. 时间长，项目组压力没有这么大，**出错率低**；
3. 扩容期间，遇到什么问题，都**可以随时调试，不怕影响线上服务**；
4. 每个数据库**少了一半**的数据量。

 

**缺点**

1. 程序复杂，需要配置双主、主主双写、检测数据同步等额外技术；
2. 但后期数据库成千上万台的时候，扩容复杂（情况非常少，除非将很多业务数据放在同一个数据库）。

 

**适用**：

1. 大型网站；
2. 对高可用要求高的服务。

#### 四、总结

------

![2个1亿库到4个5KW库](https://atts.w3cschool.cn/attachments/image/20170508/1494237659671501.jpg)
该帅气方案能够实现n库扩2n库的秒级、平滑扩容，增加数据库服务能力，降低单库一半的数据量，其核心原理是：**成倍扩容，避免数据迁移**。【4扩8也同样适用】



**迁移步骤**：

（1）修改配置

（2）reload配置，**实例扩容**完成

（3）删除冗余数据等收尾工作，**数据量收缩**完成

 

- **停机扩容：**简单，不高可用，易出错，扩容后不能回滚，只能回档，会丢失一部分数据。
- **平滑扩容：**复杂，高可用，出错调试容易，易回滚，不会造成数据丢失
