# 优化

[SQL优化13连问，收藏好 - 掘金 (juejin.cn)](https://juejin.cn/post/7208571916154847288)

## 问题

* 日常工作中，你是怎么优化 SQL 的？
* 是否遇到过深分页问题，如何解决
* 谈谈 explain 执行计划
* 说说大表的优化方案
* 哪些因素可能导致 MySQL 慢查询
* 如何使用索引优化 SQL 查询
* 聊聊慢 SQL 的优化思路
* 一条sql执行过长的时间，你如何优化，从哪些方面入手？
* 列举一下，常用的数据库设计优化技巧？
* 列举日常开发中，列举十个书写高质量SQL的小技巧
* index merge了解过嘛？
* order by查询效率慢,如何优化？
* group by 查询慢的话,如何优化呀？



* 如何定位及优化 SQL 语句的性能问题
* 大表数据查询，怎么优化
* 超大分页怎么处理
* 统计过慢查询吗？对慢查询都怎么优化过？



* MySQL 优化了解吗？说一下从哪些方面可以做到性能优化
* 你知道哪些数据库结构优化手段
* 数据库高并发解决方案
* 使用explain优化sql和索引?
* MySQL 慢查询怎么解决
* count 优化



## 回答

### 日常工作中，你是怎么优化 SQL 的？

* 分析慢查询日志
* 使用explain查看执行计划
* 索引优化
* 深分页优化
* 避免全表扫描
* 避免返回不必要的数据（如`select`具体字段而不是`select*`）
* 使用合适的数据类型（如可以使用`int`类型的话，就不要设计为`varchar`）
* 优化sql结构（如`join`优化等等）
* 适当分批量进行 (如批量更新、删除)
* 定期清理无用的数据
* 适当分库分表
* 读写分离

### 是否遇到过深分页问题，如何解决

我们可以通过减少回表次数来优化。一般有**标签记录法**和**延迟关联法**。

**标签记录法**

> 就是标记一下上次查询到哪一条了，下次再来查的时候，从该条开始往下扫描。就好像看书一样，上次看到哪里了，你就折叠一下或者夹个书签，下次来看的时候，直接就翻到啦。

假设上一次记录到100000，则SQL可以修改为：

```bash
select  id,name,balance FROM account where id > 100000 limit 10;
```

这样的话，后面无论翻多少页，性能都会不错的，因为命中了`id`索引。但是这种方式有局限性：需要一种类似连续自增的字段。

**延迟关联法**

延迟关联法，就是把条件转移到主键索引树，然后减少回表。 假设原生SQL是这样的的，其中`id`是主键，`create_time`是普通索引

```bash
select id,name,balance from account where create_time> '2020-09-19' limit 100000,10;
```

使用延迟关联法优化，如下：

```vbnet
select  acct1.id,acct1.name,acct1.balance FROM account acct1 INNER JOIN 
(SELECT a.id FROM account a WHERE a.create_time > '2020-09-19' limit 100000, 10) 
AS acct2 on acct1.id= acct2.id;
```

优化思路就是，先通过`idx_create_time`二级索引树查询到满足条件的`主键ID`，再与原表通过`主键ID`内连接，这样后面直接走了主键索引了，同时也减少了回表。



### 谈谈 explain 执行计划

当`explain`与`SQL`一起使用时，MySQL将显示来自优化器的有关语句执行计划的信息。即`MySQL`解释了它将如何处理该语句，包括有关如何连接表以及以何种顺序连接表等信息。

一条简单SQL，使用了`explain`的效果如下：

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b3c428b7ab8042389d078669bf2a7c7c~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

一般来说，我们需要重点关注`type、rows、filtered、extra、key`。

#### 1 type

type表示**连接类型**，查看索引执行情况的一个重要指标。以下性能从好到坏依次：`system  > const > eq_ref > ref  > ref_or_null > index_merge > unique_subquery > index_subquery > range > index > ALL`

- system：这种类型要求数据库表中只有一条数据，是`const`类型的一个特例，一般情况下是不会出现的。
- const：通过一次索引就能找到数据，一般用于主键或唯一索引作为条件，这类扫描效率极高，，速度非常快。
- eq_ref：常用于主键或唯一索引扫描，一般指使用主键的关联查询
- ref : 常用于非主键和唯一索引扫描。
- ref_or_null：这种连接类型类似于`ref`，区别在于`MySQL`会额外搜索包含`NULL`值的行
- index_merge：使用了索引合并优化方法，查询使用了两个以上的索引。
- unique_subquery：类似于`eq_ref`，条件用了`in`子查询
- index_subquery：区别于`unique_subquery`，用于非唯一索引，可以返回重复值。
- range：常用于范围查询，比如：between ... and 或 In 等操作
- index：全索引扫描
- ALL：全表扫描

#### 2 rows

该列表示MySQL估算要找到我们所需的记录，需要读取的行数。对于InnoDB表，此数字是估计值，并非一定是个准确值。

#### 3 filtered

该列是一个百分比的值，表里符合条件的记录数的百分比。简单点说，这个字段表示存储引擎返回的数据在经过过滤后，剩下满足条件的记录数量的比例。

#### 4 extra

该字段包含有关MySQL如何解析查询的其他信息，它一般会出现这几个值：

- Using filesort：表示按文件排序，一般是在指定的排序和索引排序不一致的情况才会出现。一般见于order by语句
- Using index ：表示是否用了覆盖索引。
- Using temporary: 表示是否使用了临时表,性能特别差，需要重点优化。一般多见于group by语句，或者union语句。
- Using where : 表示使用了where条件过滤.
- Using index condition：MySQL5.6之后新增的索引下推。在存储引擎层进行数据过滤，而不是在服务层过滤，利用索引现有的数据减少回表的数据。

#### 5 key

该列表示实际用到的索引。一般配合`possible_keys`列一起看。

**注意**:有时候，`explain`配合`show WARNINGS; `（可以查看优化后,最终执行的sql），效果更佳哦。



#### 6 a

**table：**显示这一行的数据是关于哪张表的

**all:**full table scan ;MySQL将遍历全表以找到匹配的行；

**index:** index scan; index 和 all的区别在于index类型只遍历索引；

**range：**索引范围扫描，对索引的扫描开始于某一点，返回匹配值的行，常见与between ，等查询；

**ref：**非唯一性索引扫描，返回匹配某个单独值的所有行，常见于使用非唯一索引即唯一索引的非唯一前缀进行查找；

**eq_ref：**唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配，常用于主键或者唯一索引扫描；

**const，system：**当MySQL对某查询某部分进行优化，并转为一个常量时，使用这些访问类型。如果将主键置于where列表中，MySQL就能将该查询转化为一个常量。

**possible_keys：**显示可能应用在这张表中的索引。如果为空，没有可能的索引。可以为相关的域从WHERE语句中选择一个合适的语句

**key_len：**使用的索引的长度。在不损失精确性的情况下，长度越短越好

**ref：**显示索引的哪一列被使用了，如果可能的话，是一个常数

**rows：**MySQL认为必须检查的用来返回请求数据的行数



### 说说大表的优化方案

- **数据库设计优化**

合理的数据库设计可以极大地提高查询效率。我们在设计大表时，可以考虑**拆分表、使用分区表、添加索引等方式**来优化表结构。同时也要**避免使用大量冗余字段、避免频繁使用join查询**等操作。

- **索引优化**

对于大表的查询操作，索引优化是非常重要的一环。可以考虑**增加或者修改索引、使用覆盖索引、使用联合索引等方式来提高查询效率**。同时也要注意**定期清理冗余的索引以及对于经常使用的查询语句建立索引**。

- **分区优化**

将大表按照某个列分成多个分区表，每个分区表的数据量较小，可以提高查询和更新的性能。分区表还可以帮助在维护表结构的同时，减少锁表时间，提高并发处理能力。

- **数据清理归档**

对于一些历史数据或者无用数据，**可以进行定期归档，避免数据过多造成SQL查询效率降低**。同时也要注意对于大表进行定期的数据备份以及紧急数据恢复的准备工作。

- **缓存优化**

对于一些经常被查询的数据，可以使用缓存优化。使用`Redis`等缓存中间件来缓存常用的数据，以减少查询数据库的次数，提高查询效率。

- **SQL语句优化**

在编写SQL查询语句时，要尽可能地简单明了，避免复杂的查询语句，同时也要避免一些不必要的查询操作。对于复杂的查询语句，可以使用`Explain`执行计划来进行优化。同时也要注意避免使用`OR`等耗费性能的操作符。

- **分库分表**

如果数据量千万级别，需要考虑分库分表哈。



### 哪些因素可能导致 MySQL 慢查询

#### 1. SQL没加索引

很多时候，我们的慢查询，都是因为**没有加索引**。如果没有加索引的话，会导致全表扫描的。因此，应考虑在`where`的条件列，**建立索引**，尽量避免全表扫描。

**反例：**

```
select * from user_info where name ='捡田螺的小男孩公众号' ;
```

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMicFJL5n6wNwtxHNcRaPiagc05Yel7a1q9Rw5MGfJvhMmcw0Sh8FfReZQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**正例：**

```
//添加索引
alter table user_info add index idx_name (name);
```

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMZ2qoYlib6e8sYicn3iaicoN9WmcwvDOgtQNOTLTx0K4kBNOKc6lgcYFBLA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

#### 2. SQL 索引不生效

有时候我们明明加了索引了，但是索引却不生效。在哪些场景，索引会不生效呢？主要有以下十大经典场景：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMOGwQyutHR1DibCfegpdBT0kvQFJcZgwyNNccLHsTxXfhickgzPBQ25Kg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

##### 2.1 隐式的类型转换，索引失效

我们创建一个用户user表

```
CREATE TABLE user (
  id int(11) NOT NULL AUTO_INCREMENT,
  userId varchar(32) NOT NULL,
  age  varchar(16) NOT NULL,
  name varchar(255) NOT NULL,
  PRIMARY KEY (id),
  KEY idx_userid (userId) USING BTREE
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
```

`userId`字段为**字串类型**，是B+树的普通索引，如果查询条件传了一个**数字**过去，会导致索引失效。如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMNhp4DmDpUb7Np9uVGkszOxhcVN464pNpeAaoDJMuRevB2Rxzscugmg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

如果给数字加上`''`,也就是说，传的是一个字符串呢，**当然是走索引**，如下图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMjdcwhzj4q26iatUQzSgy3JXK2gAMjJC4NKm5BxvkgV9BCnhA2yhXUYA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

> 为什么第一条语句**未加单引号就不走索引**了呢？这是因为不加单引号时，是字符串跟数字的比较，它们类型不匹配，MySQL会做**隐式的类型转换**，把它们转换为浮点数再做比较。隐式的类型转换，索引会失效。

##### 2.2 查询条件包含or，可能导致索引失效

我们还是用这个表结构：

```
CREATE TABLE user (
  id int(11) NOT NULL AUTO_INCREMENT,
  userId varchar(32) NOT NULL,
  age  varchar(16) NOT NULL,
  name varchar(255) NOT NULL,
  PRIMARY KEY (id),
  KEY idx_userid (userId) USING BTREE
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
```

其中`userId`加了索引，但是`age`没有加索引的。我们使用了`or`，以下SQL是不走索引的，如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMaibDUh2d2bSFhyfv828Fe6vUspF1A1ftjPBgEib8BNbjWe9KxCiaTBkMg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

对于`or`+没有索引的`age`这种情况，假设它走了`userId`的索引，但是走到`age`查询条件时，它还得全表扫描，也就是需要三步过程：**全表扫描+索引扫描+合并**。如果它一开始就走**全表扫描**，直接一遍扫描就完事。Mysql优化器出于效率与成本考虑，遇到`or`条件，让索引失效，看起来也合情合理嘛。

**注意**：如果`or`条件的列都加了索引，**索引可能会走也可能不走**，大家可以自己试一试哈。但是平时大家使用的时候，还是要注意一下这个`or`，学会用`explain`分析。遇到不走索引的时候，考虑拆开两条SQL。

##### 2.3. like通配符可能导致索引失效。

并不是用了`like`通配符，索引一定会失效，而是like查询是以`%`开头，才会导致索引失效。

like查询以`%`开头，索引失效

```
explain select * from user where userId like '%123';
```

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMq6TibibnNNsHLdpSlbo6YlqtmsC0SeuWhhmFZzBmvunKjsulayrR8V4A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

把`%`放后面，发现索引还是正常走的，如下：

```
explain select * from user where userId like '123%';
```

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMdyibtVic7fXpTmS6O5iaFjTuve14AOsLKiawupzv8s16axCGyuvSctWLZw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

既然`like`查询以`%`开头，会导致索引失效。我们如何优化呢？

- 使用覆盖索引
- 把`%`放后面

##### 2.4 查询条件不满足联合索引的最左匹配原则

MySQl建立联合索引时，会遵循最左前缀匹配的原则，即最左优先。如果你建立一个`（a,b,c）`的联合索引，相当于建立了`(a)、(a,b)、(a,b,c)`三个索引。

假设有以下表结构：

```
CREATE TABLE user (
  id int(11) NOT NULL AUTO_INCREMENT,
  user_id varchar(32) NOT NULL,
  age  varchar(16) NOT NULL,
  name varchar(255) NOT NULL,
  PRIMARY KEY (id),
  KEY idx_userid_name (user_id,name) USING BTREE
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
```

有一个联合索引`idx_userid_name`，我们执行这个SQL，查询条件是`name`，索引是无效：

```
explain select * from user where name ='捡田螺的小男孩';
```

因为查询条件列`name`不是联合索引`idx_userid_name`中的第一个列，索引不生效

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

在联合索引中，查询条件满足**最左匹配原则**时，索引才正常生效。

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMEG8mDQRSdr5mTAaYYWz1EWsibtK3pMiatWZlzkVAsv19ZvaFOqzjokmA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

##### 2.5 在索引列上使用mysql的内置函数

表结构：

```
CREATE TABLE `user` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `userId` varchar(32) NOT NULL,
  `login_time` datetime NOT NULL,
  PRIMARY KEY (`id`),
  KEY `idx_userId` (`userId`) USING BTREE,
  KEY `idx_login_time` (`login_Time`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;
```

虽然`login_time`加了索引，但是因为使用了`mysql`的内置函数`Date_ADD()`，索引直接GG，如图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMRwMGlfjveMTDeZHgrMjImWo37RU55rDANek09NlhylIV4J7MJ7DLFw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

一般这种情况怎么优化呢？可以把**内置函数的逻辑转移到右边**，如下：

```
explain  select * from user where login_time = DATE_ADD('2022-05-22 00:00:00',INTERVAL -1 DAY);
```

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMbXEln1hGz84fVe4L7gib60cwLdGibG7EYNCaC2DWeX80ZTNjMXorDJyg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

##### 2.6 对索引进行列运算（如，+、-、*、/）,索引不生效

表结构：

```
CREATE TABLE `user` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `userId` varchar(32) NOT NULL,
  `age` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `idx_age` (`age`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;
```

虽然`age`加了索引，但是因为它进行运算，索引直接迷路了。。。如图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMIHO4Ya9w4Kenf5icE0Jynuk0B30FllickFYUvcUmfknlKohcqEpRYn6w/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

所以**不可以对索引列进行运算，可以在代码处理好，再传参进去**。

##### 2.7 索引字段上使用（！= 或者 < >），索引可能失效

表结构：

```
CREATE TABLE `user` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `userId` int(11) NOT NULL,
  `age` int(11) DEFAULT NULL,
  `name` varchar(255) NOT NULL,
  PRIMARY KEY (`id`),
  KEY `idx_age` (`age`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;
```

虽然`age`加了索引，但是使用了`！= `或者`< >，not in`这些时，索引如同虚设。如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMhHIEza4iaOiaQ2Oy8dVmjhywUyCA28KYrcUdYTicrlXNAibgIteNfnF3Aw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMHuVrDPDBQxTROiblINe8bmauqbGVzXHyfQpNHwNXqZxgjwYc8rj5URg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

其实这个也是跟`mySQL优化器`有关，如果优化器觉得即使走了索引，还是需要扫描很多很多行的哈，它觉得不划算，**不如直接不走索引**。平时我们用`！= `或者`< >，not in`的时候，留点心眼哈。

##### 2.8 索引字段上使用is null， is not null，索引可能失效

表结构:

```
CREATE TABLE `user` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `card` varchar(255) DEFAULT NULL,
  `name` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `idx_name` (`name`) USING BTREE,
  KEY `idx_card` (`card`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;
```

单个`name`字段加上索引，并查询`name`为非空的语句，其实会走索引的，如下:

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMia8KiadbbGqaOb2F2NfCfiaSaslTSDDicH5KW8w07uXPbmxICdgkjWlvIA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

单个`card`字段加上索引，并查询name为非空的语句，其实会走索引的，如下:![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMX1rwCUrSjdHOGn2GeSAj7xaicMmyxl1GHNvj7RPh0whWjFcbWicdYibbw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

但是它两用or连接起来，索引就失效了，如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMybiaic55XJFuLwx8gDhcLYiaYqMRRsppzTptK7oha9y7ISqE5cUpXULfw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

很多时候，也是因为数据量问题，导致了MySQL优化器放弃走索引。同时，平时我们用explain分析SQL的时候，如果`type=range`,要注意一下哈，因为这个可能因为数据量问题，导致索引无效。

##### 2.9 左右连接，关联的字段编码格式不一样

新建两个表，一个`user`，一个`user_job`

```
CREATE TABLE `user` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(255) CHARACTER SET utf8mb4 DEFAULT NULL,
  `age` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  KEY `idx_name` (`name`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;

CREATE TABLE `user_job` (
  `id` int(11) NOT NULL,
  `userId` int(11) NOT NULL,
  `job` varchar(255) DEFAULT NULL,
  `name` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `idx_name` (`name`) USING BTREE
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
```

`user`表的`name`字段编码是`utf8mb4`，而`user_job`表的`name`字段编码为`utf8`。

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMN6ib92NfZYd2Q04qoZHpRyib6kdxOOu954PUyrCF1kc5AjgqbQvUK5jw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMLEj93379j40hicrFk4ey9MMXVTrIMiazI7iadAaNqes1iaCa1stIljFp3w/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

执行左外连接查询,`user_job`表还是走全表扫描，如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkM0HRWKLKygqhkUkkJBsgqYo3Dpw1VspbzaDXGDyP78ZHAGeicpmTGqKQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

如果把它们的`name`字段改为编码一致，相同的SQL，还是会走索引。

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMzJvjQlnK1TSwvxFwG6ymMBiaq2kVKpicIeB9cWF5Lh13PRo3qdNH3XTw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

所以大家在做表关联时，注意一下**关联字段的编码问题**哈。

##### 2.10 优化器选错了索引

MySQL 中一张表是可以支持多个索引的。你写`SQL`语句的时候，没有主动指定使用哪个索引的话，用哪个索引是由`MySQL`来确定的。

我们日常开发中，不断地删除历史数据和新增数据的场景，有可能会导致MySQL选错索引。那么有哪些解决方案呢？

- 使用`force index` 强行选择某个索引
- 修改你的SQl，引导它使用我们期望的索引
- 优化你的业务逻辑
- 优化你的索引，新建一个更合适的索引，或者删除误用的索引。

#### 3. limit深分页问题

limit深分页问题，会导致慢查询，应该大家都司空见惯了吧。

##### 3.1 limit深分页为什么会变慢

limit深分页为什么会导致**SQL变慢**呢？假设我们有表结构如下：

```
CREATE TABLE account (
  id int(11) NOT NULL AUTO_INCREMENT COMMENT '主键Id',
  name varchar(255) DEFAULT NULL COMMENT '账户名',
  balance int(11) DEFAULT NULL COMMENT '余额',
  create_time datetime NOT NULL COMMENT '创建时间',
  update_time datetime NOT NULL ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
  PRIMARY KEY (id),
  KEY idx_name (name),
  KEY idx_create_time (create_time) //索引
) ENGINE=InnoDB AUTO_INCREMENT=1570068 DEFAULT CHARSET=utf8 ROW_FORMAT=REDUNDANT COMMENT='账户表';
```

你知道以下`SQL`，执行过程是怎样的嘛？

```
select id,name,balance from account where create_time> '2020-09-19' limit 100000,10;
```

这个SQL的执行流程：

1. 通过普通二级索引树`idx_create_time`，过滤`create_time`条件，找到满足条件的主键`id`。
2. 通过`主键id`，回到`id主键索引树`，找到满足记录的行，然后取出需要展示的列（回表过程）
3. 扫描满足条件的`100010`行，然后扔掉前`100000`行，返回。

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMzvicfiba4r9qnmvibLyyI7jGRX0CGwk816KL1EtiaqadU01ZUn9qOuWAdA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

`limit`深分页，导致`SQL`变慢原因有两个：

- `limit`语句会先扫描`offset+n`行，然后再丢弃掉前`offset`行，返回后`n`行数据。也就是说`limit 100000,10`，就会扫描`100010`行，而limit `0,10`，只扫描`10`行。
- `limit 100000,10` 扫描更多的行数，也意味着**回表更多的次数**。

##### 3.2 如何优化深分页问题

我们可以通过减少回表次数来优化。一般有**标签记录法和延迟关联法**。

**标签记录法**

> 就是标记一下上次查询到哪一条了，下次再来查的时候，从该条开始往下扫描。就好像看书一样，上次看到哪里了，你就折叠一下或者夹个书签，下次来看的时候，直接就翻到啦。

假设上一次记录到`100000`，则SQL可以修改为：

```
select  id,name,balance FROM account where id > 100000 limit 10;
```

这样的话，后面无论翻多少页，性能都会不错的，因为命中了`id索引`。但是这种方式有局限性：需要一种类似连续自增的字段。

**延迟关联法**

延迟关联法，就是把条件转移到**主键索引树**，然后减少回表。如下：

```
select  acct1.id,acct1.name,acct1.balance FROM account acct1 INNER JOIN (SELECT a.id FROM account a WHERE a.create_time > '2020-09-19' limit 100000, 10) AS acct2 on acct1.id= acct2.id;
```

**优化思路**就是，先通过`idx_create_time`二级索引树查询到满足条件的`主键ID`，再与原表通过`主键ID`内连接，这样后面直接走了主键索引了，同时也减少了回表。

#### 4. 单表数据量太大

##### 4.1 单表数据量太大为什么会变慢？

一个表的数据量达到好几千万或者上亿时，加索引的效果没那么明显啦。性能之所以会变差，是因为维护索引的`B+`树结构层级变得更高了，查询一条数据时，需要经历的磁盘IO变多，因此查询性能变慢。

##### 4.2 一棵B+树可以存多少数据量

**大家是否还记得，一个B+树大概可以存放多少数据量呢？**

InnoDB存储引擎最小储存单元是页，一页大小就是`16k`。

B+树叶子存的是数据，内部节点存的是键值+指针。索引组织表通过非叶子节点的二分查找法以及指针确定数据在哪个页中，进而再去数据页中找到需要的数据；

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMdS8D4Nz6GUOzKoCxtFdx2SaRsXAZYxw4smrOCBeCFfVtv0aP7ef0NQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

假设B+树的高度为`2`的话，即有一个根结点和若干个叶子结点。这棵B+树的存放总记录数为=根结点指针数*单个叶子节点记录行数。

- 如果一行记录的数据大小为1k，那么单个叶子节点可以存的记录数 =16k/1k =16.
- 非叶子节点内存放多少指针呢？我们假设主键ID为**bigint类型，长度为8字节**(**面试官问你int类型，一个int就是32位，4字节**)，而指针大小在InnoDB源码中设置为6字节，所以就是8+6=14字节，16k/14B =16*1024B/14B = 1170

因此，一棵高度为2的B+树，能存放`1170 * 16=18720`条这样的数据记录。同理一棵高度为3的B+树，能存放`1170 *1170 *16 =21902400`，也就是说，可以存放两千万左右的记录。B+树高度一般为1-3层，已经满足千万级别的数据存储。

如果B+树想存储更多的数据，那树结构层级就会更高，查询一条数据时，需要经历的磁盘IO变多，因此查询性能变慢。

##### 4.3 如何解决单表数据量太大，查询变慢的问题

一般超过千万级别，我们可以考虑**分库分表**了。

分库分表可能导致的问题：

- 事务问题
- 跨库问题
- 排序问题
- 分页问题
- 分布式ID

因此，大家在评估是否分库分表前，先考虑下，是否可以把部分历史数据归档先，如果可以的话，先不要急着**分库分表**。如果真的要分库分表，综合考虑和评估方案。比如可以考虑垂直、水平分库分表。水平分库分表策略的话，**range范围、hash取模、range+hash取模混合**等等。

#### 5. join 或者子查询过多

一般来说，不建议使用子查询，可以把子查询改成`join`来优化。而数据库有个规范约定就是：**尽量不要有超过3个以上的表连接**。为什么要这么建议呢? 我们来聊聊，`join`哪些方面可能导致慢查询吧。

MySQL中，join的执行算法，分别是：`Index Nested-Loop Join`和`Block Nested-Loop Join`。

- `Index Nested-Loop Join`：这个join算法，跟我们写程序时的嵌套查询类似，并且可以用上**被驱动表的索引**。
- `Block Nested-Loop Join`：这种join算法，**被驱动表上没有可用的索引**,它会先把驱动表的数据读入线程内存`join_buffer`中，再扫描被驱动表，把被驱动表的每一行取出来，跟`join_buffer`中的数据做对比，满足join条件的，作为结果集的一部分返回。

`join`过多的问题：

> 一方面，过多的表连接，会大大增加SQL复杂度。另外一方面，如果可以使用被驱动表的**索引**那还好，并且使用**小表来做驱动表**，**查询效率更佳**。如果被驱动表**没有可用的索引**，join是在`join_buffer`内存做的，如果匹配的数据量比较小或者`join_buffer`设置的比较大，速度也不会太慢。但是，如果`join`的数据量比较大时，mysql会采用在硬盘上创建临时表的方式进行多张表的关联匹配，这种显然效率就极低，本来磁盘的 IO 就不快，还要关联。

一般情况下，如果业务需要的话，关联`2~3`个表是可以接受的，但是**关联的字段需要加索引**哈。如果需要关联更多的表，建议从代码层面进行拆分，在业务层先查询一张表的数据，然后以关联字段作为条件查询关联表形成`map`，然后在业务层进行数据的拼装。

#### 6. in元素过多

如果使用了`in`，即使后面的条件加了索引，还是要注意`in`后面的元素不要过多哈。`in`元素一般建议不要超过`500`个，如果超过了，建议分组，每次`500`一组进行哈。

**反例：**

```
select user_id,name from user where user_id in (1,2,3...1000000); 
```

如果我们对`in的条件`不做任何限制的话，该查询语句一次性可能会查询出非常多的数据，很容易导致接口超时。尤其有时候，我们是用的子查询，in后面的子查询，你都不知道数量有多少那种，更容易采坑（**所以我把in元素过多抽出来作为一个小节**）。如下这种子查询：

```
select * from user where user_id in (select author_id from artilce where type = 1);
```

正例是，**分批进行**，每批500个：

```
select user_id,name from user where user_id in (1,2,3...500);
```

如果传参的ids太多，还可以做个参数校验什么的

```
if (userIds.size() > 500) {
    throw new Exception("单次查询的用户Id不能超过200");
}
```

#### 7. 数据库在刷脏页

##### 7.1 什么是脏页

当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“**脏页**”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“**干净页**”。一般有更新SQL才可能会导致脏页，我们回忆一下：一条更新语句是如何执行的

##### 7.2 一条更新语句是如何执行的？

以下的这个更新SQL，如何执行的呢？

```
update t set c=c+1 where id=666;
```

1. 对于这条更新SQL，执行器会先找引擎取`id=666`这一行。如果这行所在的数据页本来就在内存中的话，就直接返回给执行器。如果不在内存，就去磁盘读入内存，再返回。
2. 执行器拿到引擎给的行数据后，给这一行`C`的值加一，得到新的一行数据，再调用引擎接口写入这行新数据。
3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到`redo log`里面，但是此时`redo log `是处于`prepare`状态的哈。
4. 执行器生成这个操作的`binlog`，并把`binlog`写入磁盘。
5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的`redo log`改成提交（commit）状态，更新完成。

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMfVbQ3rF6fylcKVQ8HrUlDTpovZLiaU8k7aOAvKC2gHYMYrwicb2FbvpA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

InnoDB 在处理更新语句的时候，只做了写日志这一个磁盘操作。这个日志叫作`redo log`（重做日志）。平时更新SQL执行得很快，其实是因为它只是在写内存和`redo log`日志，等到空闲的时候，才把`redo log`日志里的数据同步到磁盘中。

> 有些小伙伴可能有疑惑，`redo log`日志不是在磁盘嘛？那为什么不慢？其实是因为写`redo log`的过程是顺序写磁盘的。**磁盘顺序写**会减少寻道等待时间，速度比随机写要快很多的。

##### 7.3 为什么会出现脏页呢？

更新SQL只是在写内存和`redo log`日志，等到空闲的时候，才把`redo log`日志里的数据同步到磁盘中。这时内存数据页跟磁盘数据页内容不一致,就出现脏页。

##### 7.4 什么时候会刷脏页（flush）？

InnoDB存储引擎的`redo log`大小是固定，且是环型写入的，如下图（图片来源于MySQL 实战 45 讲）：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMjqKrXExJOkQjopuKAian1P7lM3tKR0q9xshlZlxcM0LvDfSdATDrLcg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

那什么时候会刷脏页？有几种场景：

1. `redo log`写满了，要刷脏页。这种情况要尽量避免的。因为出现这种情况时，整个系统就不能再接受更新啦，即所有的更新都必须堵住。
2. 内存不够了，需要新的内存页，就要淘汰一些数据页，这时候会刷脏页

> InnoDB 用缓冲池（buffer pool）管理内存,而当要读入的数据页没有在内存的时候，就必须到缓冲池中申请一个数据页。这时候只能把最久不使用的数据页从内存中淘汰掉：如果要淘汰的是一个干净页，就直接释放出来复用；但如果是脏页呢，就必须**将脏页先刷到磁盘**，变成干净页后才能复用。

1. MySQL 认为**系统空闲**的时候，也会刷一些脏页
2. MySQL 正常关闭时，会把内存的脏页都 flush 到磁盘上

##### 7.5 为什么刷脏页会导致SQL变慢呢？

1. `redo log`写满了，要刷脏页，这时候会导致系统所有的更新堵住，写性能都跌为0了，肯定慢呀。一般要杜绝出现这个情况。
2. 一个查询要淘汰的脏页个数太多，一样会导致查询的响应时间明显变长。

#### 8. order by 文件排序

`order by`就一定会导致慢查询吗？**不是这样的哈**，因为`order by`平时用得多，并且数据量一上来，还是走**文件排序**的话，很容易有慢SQL的。听我娓娓道来，`order by`哪些时候可能会导致慢SQL哈。

##### 8.1 order by 的 Using filesort文件排序

我们平时经常需要用到`order by` ，主要就是用来给某些字段排序的。比如以下SQL:

```
select name,age,city from staff where city = '深圳' order by age limit 10;
```

它表示的意思就是：**查询前10个，来自深圳员工的姓名、年龄、城市，并且按照年龄小到大排序。**

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMdzH6oxuEbKvyiaSrWA6OjkB9rO6icbX7bbNsmkoqKo99j0fncFXfBzsA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

查看`explain`执行计划的时候，可以看到`Extra`这一列，有一个`Using filesort`，它表示用到**文件排序**。

##### 8.2 order by文件排序效率为什么较低

`order by`用到文件排序时，为什么查询效率会相对低呢？

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMJqqZUUCJedaCtWoLJkSk5XJ0jibIfojyaUic4slJUjrgYvibsvMVbZDPQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

`order by`排序，分为全字段排序和rowid排序。它是拿`max_length_for_sort_data`和结果行数据长度对比，如果结果行数据长度超过`max_length_for_sort_data`这个值，就会走**rowid排序**，相反，则走**全字段排序**。

**rowid排序**

rowid排序，一般需要**回表去找满足条件的数据，所以效率会慢一点**。以下这个SQL，使用`rowid`排序，执行过程是这样：

```
select name,age,city from staff where city = '深圳' order by age limit 10;
```

1. MySQL 为对应的线程初始化`sort_buffer`，放入需要排序的`age`字段，以及`主键id`；
2. 从索引树`idx_city`， 找到第一个满足 `city='深圳’`条件的`主键id`，也就是图中的`id=9`；
3. 到`主键id索引树`拿到`id=9`的这一行数据， 取`age和主键id`的值，存到`sort_buffer`；
4. 从索引树`idx_city`拿到下一个记录的`主键id`，即图中的`id=13`；
5. 重复步骤 3、4 直到`city`的值不等于深圳为止；
6. 前面5步已经查找到了所有`city`为深圳的数据，在`sort_buffer`中，将所有数据根据age进行排序；
7. 遍历排序结果，取前10行，并按照`id`的值回到原表中，取出`city、name 和 age`三个字段返回给客户端。

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMNQwVsBKqGhVs8LRpRtK0dwDtomLEn8gicmOvP3WWibDRXqicsQb4ZnribQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**全字段排序**

同样的SQL，如果是走全字段排序是这样的：

```
select name,age,city from staff where city = '深圳' order by age limit 10;
```

1. MySQL 为对应的线程初始化`sort_buffer`，放入需要查询的`name、age、city`字段；
2. 从索引树`idx_city`， 找到第一个满足 `city='深圳’`条件的主键 id，也就是图中的`id=9`；
3. 到主键`id索引树`拿到`id=9`的这一行数据， 取`name、age、city`三个字段的值，存到`sort_buffer`；
4. 从索引树`idx_city `拿到下一个记录的主键`id`，即图中的`id=13`；
5. 重复步骤 3、4 直到`city`的值不等于深圳为止；
6. 前面5步已经查找到了所有`city`为深圳的数据，在`sort_buffer`中，将所有数据根据`age`进行排序；
7. 按照排序结果取前10行返回给客户端。

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMt6exckpoQbYKoJXv97BlEKrljamuCY0nJCQVDR3cHC2GpMnQTOIDNQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

`sort_buffer`的大小是由一个参数控制的：`sort_buffer_size`。

- 如果要排序的数据小于`sort_buffer_size`，排序在`sort_buffer`内存中完成
- 如果要排序的数据大于`sort_buffer_size`，则借助磁盘文件来进行排序。

> 借助磁盘文件排序的话，**效率就更慢一点**。因为先把数据放入`sort_buffer`，当快要满时。会排一下序，然后把`sort_buffer`中的数据，放到临时磁盘文件，等到所有满足条件数据都查完排完，再用归并算法把磁盘的临时排好序的小文件，合并成一个有序的大文件。

##### 8.3 如何优化order by的文件排序

`order by`使用**文件排序**，效率会低一点。我们怎么优化呢？

- 因为数据是无序的，所以就需要排序。如果数据本身是有序的，那就不会再用到文件排序啦。而索引数据本身是有序的，我们通过建立索引来优化`order by`语句。
- 我们还可以通过调整`max_length_for_sort_data`、`sort_buffer_size`等参数优化；

大家有兴趣可以看下我之前这篇文章哈：[看一遍就理解：order by详解](https://mp.weixin.qq.com/s?__biz=Mzg3NzU5NTIwNg==&mid=2247490571&idx=1&sn=e8638573ec8d720fd25da5b2b0d90ed2&scene=21#wechat_redirect)

#### 9. 拿不到锁

有时候，我们查询一条很简单的SQL，但是却等待很长的时间，不见结果返回。一般这种时候就是表被锁住了，或者要查询的某一行或者几行被锁住了。我们只能慢慢等待锁被释放。

> 举一个生活的例子哈，你和别人合租了一间房子，这个房子只有一个卫生间的话。假设某一时刻，你们都想去卫生间，但是对方比你早了一点点。那么此时你只能等对方出来后才能进去。

这时候，我们可以用`show processlist`命令，看看当前语句处于什么状态哈。

#### 10. delete + in子查询不走索引！

之前见到过一个生产慢SQL问题，当`delete`遇到`in`子查询时，即使有索引，也是不走索引的。而对应的`select + in`子查询，却可以走索引。

MySQL版本是5.7，假设当前有两张表account和old_account,表结构如下：

```
CREATE TABLE `old_account` (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键Id',
  `name` varchar(255) DEFAULT NULL COMMENT '账户名',
  `balance` int(11) DEFAULT NULL COMMENT '余额',
  `create_time` datetime NOT NULL COMMENT '创建时间',
  `update_time` datetime NOT NULL ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
  PRIMARY KEY (`id`),
  KEY `idx_name` (`name`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=1570068 DEFAULT CHARSET=utf8 ROW_FORMAT=REDUNDANT COMMENT='老的账户表';

CREATE TABLE `account` (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键Id',
  `name` varchar(255) DEFAULT NULL COMMENT '账户名',
  `balance` int(11) DEFAULT NULL COMMENT '余额',
  `create_time` datetime NOT NULL COMMENT '创建时间',
  `update_time` datetime NOT NULL ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
  PRIMARY KEY (`id`),
  KEY `idx_name` (`name`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=1570068 DEFAULT CHARSET=utf8 ROW_FORMAT=REDUNDANT COMMENT='账户表';
```

执行的SQL如下：

```
delete from account where name in (select name from old_account);
```

查看执行计划，发现不走索引：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMmcV1bBgnAYPguhckV9NyqU4PD1hGCibmYOaQKEe6elLciaFpt3MNCZqw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

但是如果把`delete`换成`select`，就会走索引。如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMNbttbblgxbp7JOxibiakbxY2JUSW79uTibRgsVQYrsYzzWFoZBjZl5Iwg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

为什么`select + in`子查询会走索引，`delete + in`子查询却不会走索引呢？

我们执行以下SQL看看：

```
explain select * from account where name in (select name from old_account);
show WARNINGS; //可以查看优化后,最终执行的sql
```

结果如下：

```
select `test2`.`account`.`id` AS `id`,`test2`.`account`.`name` AS `name`,`test2`.`account`.`balance` AS `balance`,`test2`.`account`.`create_time` AS `create_time`,`test2`.`account`.`update_time` AS `update_time` from `test2`.`account` 
semi join (`test2`.`old_account`)
where (`test2`.`account`.`name` = `test2`.`old_account`.`name`)
```

可以发现，实际执行的时候，MySQL对`select in`子查询做了优化，把子查询改成`join`的方式，所以可以走索引。但是很遗憾，对于`delete in`子查询，MySQL却没有对它做这个优化。

日常开发中，大家注意一下这个场景哈，大家有兴趣可以看下这篇文章哈：[生产问题分析！delete in子查询不走索引？！](https://mp.weixin.qq.com/s?__biz=Mzg3NzU5NTIwNg==&mid=2247495170&idx=1&sn=ce914de3abdb0d887e286b680b25111f&chksm=cf22312bf855b83d31a00da110626747df8e69fca1bc310642c56e39d663b006a8105f9fb1e1&token=1495321435&lang=zh_CN&scene=21#wechat_redirect)

#### 11、group by使用临时表

`group by`一般用于分组统计，它表达的逻辑就是根据**一定的规则，进行分组**。日常开发中，我们使用得比较频繁。如果不注意，很容易产生慢SQL。

##### 11.1 group by的执行流程

假设有表结构：

```
CREATE TABLE `staff` (
  `id` bigint(11) NOT NULL AUTO_INCREMENT COMMENT '主键id',
  `id_card` varchar(20) NOT NULL COMMENT '身份证号码',
  `name` varchar(64) NOT NULL COMMENT '姓名',
  `age` int(4) NOT NULL COMMENT '年龄',
  `city` varchar(64) NOT NULL COMMENT '城市',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=15 DEFAULT CHARSET=utf8 COMMENT='员工表';
```

我们查看一下这个SQL的执行计划：

```
explain select city ,count(*) as num from staff group by city;
```

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMhiciaeo5Q3jPztcoibd5TTBJc3k5YJAp0JWskXNxnicNRcjUtlmSwb3cTQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- Extra 这个字段的`Using temporary`表示在执行分组的时候使用了**临时表**
- Extra 这个字段的`Using filesort`表示使用了**文件排序**

`group by`是怎么使用到临时表和排序了呢？我们来看下这个SQL的执行流程

```
select city ,count(*) as num from staff group by city;
```

1. 创建内存临时表，表里有两个字段`city和num`；
2. 全表扫描`staff`的记录，依次取出`city = 'X'`的记录。

- 判断临时表中是否有为 `city='X'`的行，没有就插入一个记录` (X,1)`;
- 如果临时表中有`city='X'`的行，就将X这一行的num值加 1；

1. 遍历完成后，再根据字段`city`做排序，得到结果集返回给客户端。这个流程的执行图如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpzT0uCAjOiaqJtsiaX04ZsLkMXvysbISyKl5Jr8Pb97osrlkSMicia3fuEkgEDgHZbyZVJ0To1iaTms6hQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**临时表的排序是怎样的呢？**

就是把需要排序的字段，放到sort buffer，排完就返回。在这里注意一点哈，排序分**全字段排序和rowid排序**

- 如果是全字段排序，需要查询返回的字段，都放入sort buffer，根据排序字段排完，直接返回
- 如果是rowid排序，只是需要排序的字段放入sort buffer，然后多一次回表操作，再返回。

##### 11.2  group by可能会慢在哪里？

`group by`使用不当，很容易就会产生慢SQL 问题。因为它既用到临时表，又默认用到排序。有时候还可能用到磁盘临时表。

- 如果执行过程中，会发现`内存临时表`大小到达了上限（控制这个上限的参数就是tmp_table_size），会把内存临时表转成磁盘临时表。
- 如果数据量很大，很可能这个查询需要的磁盘临时表，就会占用大量的磁盘空间。

##### 11.3 如何优化group by呢？

从哪些方向去优化呢？

- 方向1：既然它默认会排序，我们不给它排是不是就行啦。
- 方向2：既然临时表是影响`group by`性能的X因素，我们是不是可以不用临时表？

我们一起来想下，执行`group by`语句为什么需要临时表呢？`group by`的语义逻辑，就是统计不同的值出现的个数。如果这个这些值一开始就是有序的，我们是不是直接往下扫描统计就好了，就不用临时表来记录并统计结果啦?

可以有这些优化方案：

- group by 后面的字段加索引
- order by null 不用排序
- 尽量只使用内存临时表
- 使用SQL_BIG_RESULT

大家可以看下我这篇文章哈：[看一遍就理解：group by详解](https://mp.weixin.qq.com/s?__biz=Mzg3NzU5NTIwNg==&mid=2247497527&idx=1&sn=1f30251d88b0e935bfffc3e8eaf53f28&chksm=cf22281ef855a1084fe84a7b257db5734c7b982c6ddaf9ef497d4e31e60faebd5f329e3c55a6&token=352114508&lang=zh_CN&scene=21#wechat_redirect)

#### 12. 系统硬件或网络资源

- 如果数据库服务器内存、硬件资源，或者网络资源配置不是很好，就会慢一些哈。这时候可以升级配置。这就好比你的计算机有时候很卡，你可以加个内存条什么的一个道理。
- 如果数据库压力本身很大，比如高并发场景下，大量请求到数据库来，数据库服务器`CPU`占用很高或者`IO利用率`很高，这种情况下所有语句的执行都有可能变慢的哈。





### 如何使用索引优化 SQL 查询

添加合适索引（在`where、group by、order by`等后面的字段添加合适索引）

选择合适的索引类型 (`B-tree`索引适合范围查询、哈希索引适合等值查询)

注意不适合加索引的场景（数据量少的表，更新频繁的字段，区分度低的字段）

加索引的时候，需要考虑覆盖索引，减少回表，**考虑联合索引的最左前缀原则**

`explain`查看`SQL`的执行计划，确认是否会命中索引。

注意索引并不是越多越好，**通常建议在单个表中不要超过5个索引**。因为索引会占用磁盘空间，索引更新代价高。





### 聊聊慢 SQL 的优化思路

1. 查看慢查询日志记录，分析慢SQL
2. explain分析SQL的执行计划
3. profile 分析执行耗时
4. Optimizer Trace分析详情
5. 确定问题并采用相应的措施



#### 1 查看慢查询日志记录，分析慢SQL

如何定位慢SQL呢、我们可以通过**slow log**来查看慢`SQL`。默认的情况下呢，MySQL数据库是不开启慢查询日志（`slow query log`）呢。所以我们需要手动把它打开。

查看下慢查询日志配置，我们可以使用`show variables like 'slow_query_log%'`命令，如下：

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2a4287a80b0b49cbbb9ca72c3525baf6~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

- `slow query log`表示慢查询开启的状态
- `slow_query_log_file`表示慢查询日志存放的位置

我们还可以使用`show variables like 'long_query_time'`命令，查看超过多少时间，才记录到慢查询日志，如下：

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/92680af73c4540f184ab1669946a921f~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

- `long_query_time`表示查询超过多少秒才记录到慢查询日志。

**我们可以通过慢查日志，定位那些执行效率较低的SQL语句，重点关注分析。**

#### 2 explain查看分析SQL的执行计划

当定位出查询效率低的SQL后，可以使用`explain`查看`SQL`的执行计划。

当`explain`与`SQL`一起使用时，MySQL将显示来自优化器的有关语句执行计划的信息。即`MySQL`解释了它将如何处理该语句，包括有关如何连接表以及以何种顺序连接表等信息。

一条简单SQL，使用了`explain`的效果如下：

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b3c428b7ab8042389d078669bf2a7c7c~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

一般来说，我们需要重点关注`type、rows、filtered、extra、key`。

#### 3 profile 分析执行耗时

`explain`只是看到`SQL`的预估执行计划，如果要了解`SQL`**真正的执行线程状态及消耗的时间**，需要使用`profiling`。开启`profiling`参数后，后续执行的`SQL`语句都会记录其资源开销，包括`IO，上下文切换，CPU，内存`等等，我们可以根据这些开销进一步分析当前慢SQL的瓶颈再进一步进行优化。

`profiling`默认是关闭，我们可以使用`show variables like '%profil%'`查看是否开启，如下：

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2521d265d88a4326857950636f6e52ad~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

可以使用`set profiling=ON`开启。开启后，可以运行几条SQL，然后使用`show profiles`查看一下。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/685bd45b9d8d4ff287e114cadfee9e3f~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

`show profiles`会显示最近发给服务器的多条语句，条数由变量`profiling_history_size`定义，默认是15。如果我们需要看单独某条SQL的分析，可以`show profile`查看最近一条SQL的分析。也可以使用`show profile for query id`（其中id就是show profiles中的QUERY_ID）查看具体一条的SQL语句分析。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ff60f8eb9c78485db068c8e747aeb17c~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

除了查看profile ，还可以查看cpu和io，如上图。

#### 4 Optimizer Trace分析详情

profile只能查看到SQL的执行耗时，但是无法看到SQL真正执行的过程信息，即不知道MySQL优化器是如何选择执行计划。这时候，我们可以使用`Optimizer Trace`，它可以跟踪执行语句的解析优化执行的全过程。

我们可以使用`set optimizer_trace="enabled=on"`打开开关，接着执行要跟踪的SQL，最后执行`select * from information_schema.optimizer_trace`跟踪，如下：

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/104017072a6e444388ce0841e8294641~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

大家可以查看分析其执行树，会包括三个阶段：

- join_preparation：准备阶段
- join_optimization：分析阶段
- join_execution：执行阶段

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d68c711ee82844d7a47c32cbbcac4900~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

#### 5 确定问题并采用相应的措施

最后确认问题，就采取对应的措施。

- 多数慢SQL都跟索引有关，比如不加索引，索引不生效、不合理等，这时候，我们可以**优化索引**。
- 我们还可以优化SQL语句，比如一些in元素过多问题（分批），深分页问题（基于上一次数据过滤等），进行时间分段查询
- SQl没办法很好优化，可以改用ES的方式，或者数仓。
- 如果单表数据量过大导致慢查询，则可以考虑分库分表
- 如果数据库在刷脏页导致慢查询，考虑是否可以优化一些参数，跟DBA讨论优化方案
- 如果存量数据量太大，考虑是否可以让部分数据归档



### 一条sql执行过长的时间，你如何优化，从哪些方面入手？

这道面试题，**其实跟慢SQl排查解决有点像**，所以大家回答得时候，可以参考上一小节哈。我们可以从这几个方面入手哈：

- 确定瓶颈
- 索引优化
- 优化SQL语句
- 数据库参数优化
- 分析锁的情况
- 数据库硬件升级

**确定瓶颈**

首先，通过**查看MySQL日志，慢查询日志，explain分析SQL的执行计划，profile 分析执行耗时，Optimizer Trace分析详情**等操作，确定查询执行的瓶颈在哪里。只有确定了瓶颈，才能有针对性地进行优化。

**索引优化**

在确定了瓶颈之后，可以考虑通过增加索引来优化查询效率。可以根据查询语句的条件，增加相应的索引，从而加快查询速度。但是索引也会带来一些负面影响，如占用磁盘空间，降低写入效率等，所以需要根据具体情况权衡。

**优化SQL语句**

有些SQL语句本身可能存在一些问题，如join操作过于频繁，使用了不必要的子查询等，这些都会导致查询效率低下。可以通过优化SQL语句来减少不必要的操作，从而提高查询效率。

**数据库参数优化**

数据库参数也会影响查询效率，可以通过修改数据库参数来优化查询效率，如修改内存缓存大小、修改连接池大小等。不同的数据库参数优化方式不同，需要根据具体情况进行调整。

**分析锁的情况**

查询执行时间过长有可能是由于锁的问题导致的，需要分析查询语句中是否存在锁的问题，如果存在锁的问题，可以考虑增加锁的并发度，从而提高查询效率。

**数据库硬件升级**

如果以上方法都无法解决问题，可以考虑对数据库硬件进行升级，如增加 CPU 数量、加快磁盘读写速度等，从而提高数据库的整体性能。





### 列举一下，常用的数据库设计优化技巧？

- 字段尽量避免使用NULL
- 合理选择数据类型
- 字段选择合适的长度
- 正确使用索引
- 尽量少定义text类型
- 合理的数据表结构设计
- 适当的冗余设计
- 优化SQL查询语句
- 一张表的字段不宜过多





### 列举日常开发中，列举十个书写高质量SQL的小技巧

* 查询SQL尽量不要使用select *，而是select具体字段。
* 小表驱动大表
* 优化你的like语句
* 尽量避免在索引列上使用mysql的内置函数
* 如果插入数据过多，考虑批量操作。
* 多用limit
* 小表驱动大表
* exist & in合理利用
* in元素不要过多
* 尽量用union all替换union







### index merge了解过嘛？

`index merge`是什么？

> 在`MySQL`中，当执行一个查询语句需要使用多个索引时，MySQL可以使用索引合并(`Index Merge`)来优化查询性能。具体来说，索引合并是将多个单列索引或多个联合索引合并使用，以满足查询语句的需要。

当使用索引合并时，`MySQL`会选择最优的索引组合来执行查询，**从而避免了全表扫描和排序操作，提高了查询效率**。而对于使用多个单列索引的查询语句，MySQL也可以使用索引合并来优化查询性能。

大家可以看一个使用`index merge`的例子：

假设有一个名为`orders`的表，包含`order_id、customer_id、product_id、order_date等字段，其中order_id、customer_id、product_id`三个字段都建有索引。

如果要查询`customer_id`为`1`，`order_date`在2022年1月1日到2022年2月1日之间的订单记录，可以使用以下SQL语句：

```sql
SELECT *
FROM orders
WHERE customer_id = 1
AND order_date >= '2022-01-01'
AND order_date < '2022-02-01'
```

在执行该查询语句时，MySQL可以使用`customer_id`索引和`order_date`索引来优化查询。如果使用单个索引，则需要扫描整个索引树来匹配查询条件；但如果使用索引合并，则可以先使用`customer_id`索引来过滤出符合条件的记录，然后再使用`order_date`索引来进一步过滤记录，从而大大减少了扫描的记录数，提高了查询效率。

大家可以使用EXPLAIN关键字可以查看查询计划，确认是否使用了索引合并。例如，执行以下语句：

```sql
EXPLAIN SELECT *
FROM orders
WHERE customer_id = 1
AND order_date >= '2022-01-01'
AND order_date < '2022-02-01'
```

如果查询计划中出现了`Using index merge`的信息，则表示该查询使用了索引合并优化。



### order by查询效率慢,如何优化？

大家是否还记得`order by`查询为什么会慢嘛?

`order by`排序，分为全字段排序和`rowid`排序。它是拿`max_length_for_sort_data`和结果行数据长度对比，如果结果行数据长度超过`max_length_for_sort_data`这个值，就会走`rowid`排序，相反，则走全字段排序。

`rowid`排序，一般需要回表去找满足条件的数据，所以效率会慢一点.如果是`order by`排序,可能会借助磁盘文件排序的话，效率就更慢一点.

如何优化`order by`的文件排序?

- 因为数据是无序的，所以就需要排序。如果数据本身是有序的，那就不会再用到文件排序啦。而索引数据本身是有序的，我们通过建立索引来优化`order by`语句。
- 我们还可以通过调整`max_length_for_sort_data、sort_buffer_size`等参数优化；



### group by 查询慢的话,如何优化呀？

`group by`一般用于分组统计，它表达的逻辑就是根据一定的规则，进行分组。日常开发中，我们使用得比较频繁。如果不注意，很容易产生慢`SQL`。

` group by`可能会慢在哪里？**因为它既用到临时表，又默认用到排序。有时候还可能用到磁盘临时表。**

- 如果执行过程中，会发现内存临时表大小到达了上限（控制这个上限的参数就是`tmp_table_size`），会把内存临时表转成磁盘临时表。
- 如果数据量很大，很可能这个查询需要的磁盘临时表，就会占用大量的磁盘空间。

如何优化group by呢?

- group by 后面的字段加索引
- order by null 不用排序
- 尽量只使用内存临时表
- 使用SQL_BIG_RESULT



### 你知道哪些数据库结构优化的手段？

* **范式优化**： 比如消除冗余（节省空间。。）
* **反范式优化**：比如适当加冗余等（减少join）
* **限定数据的范围**： 务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内。
* **读/写分离**： 经典的数据库拆分方案，主库负责写，从库负责读；
* **拆分表**：分区将数据在物理上分隔开，不同分区的数据可以制定保存在处于不同磁盘上的数据文件里。这样，当对这个表进行查询时，只需要在表分区中进行扫描，而不必进行全表扫描，明显缩短了查询时间，另外处于不同磁盘的分区也将对这个表的数据传输分散在不同的磁盘I/O，一个精心设置的分区可以将数据传输对磁盘I/O竞争均匀地分散开。对数据量大的时时表可采取此方法。可按月自动建表分区。



### 数据库高并发解决方案

* 在web服务框架中加入缓存。在服务器与数据库层之间加入缓存层，将高频访问的数据存入缓存中，减少数据库的读取负担。
* 增加数据库索引，进而提高查询速度。（不过索引太多会导致速度变慢，并且数据库的写入会导致索引的更新，也会导致速度变慢）
* 主从读写分离，让主服务器负责写，从服务器负责读。
* 将数据库进行拆分，使得数据库的表尽可能小，提高查询的速度。
* 使用分布式架构，分散计算压力。



### MySQL 慢查询怎么解决

- slow_query_log 慢查询开启状态。
- slow_query_log_file 慢查询日志存放的位置（这个目录需要MySQL的运行帐号的可写权限，一般设置为MySQL的数据存放目录）。
- long_query_time 查询超过多少秒才记录。



### MySQL 磁盘 I/O 很高，有什么优化的方法？

现在我们知道事务在提交的时候，需要将 binlog 和 redo log 持久化到磁盘，那么如果出现 MySQL 磁盘 I/O 很高的现象，我们可以通过控制以下参数，来 “延迟” binlog 和 redo log 刷盘的时机，从而降低磁盘 I/O 的频率：

- 设置组提交的两个参数： binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数，延迟 binlog 刷盘的时机，从而减少 binlog 的刷盘次数。这个方法是基于“额外的故意等待”来实现的，因此可能会增加语句的响应时间，但即使 MySQL 进程中途挂了，也没有丢失数据的风险，因为 binlog 早被写入到 page cache 了，只要系统没有宕机，缓存在 page cache 里的 binlog 就会被持久化到磁盘。
- 将 sync_binlog 设置为大于 1 的值（比较常见是 100~1000），表示每次提交事务都 write，但累积 N 个事务后才 fsync，相当于延迟了 binlog 刷盘的时机。但是这样做的风险是，主机掉电时会丢 N 个事务的 binlog 日志。
- 将 innodb_flush_log_at_trx_commit 设置为 2。表示每次事务提交时，都只是缓存在 redo log buffer 里的 redo log 写到 redo log 文件，注意写入到「 redo log 文件」并不意味着写入到了磁盘，因为操作系统的文件系统中有个 Page Cache，专门用来缓存文件数据的，所以写入「 redo log文件」意味着写入到了操作系统的文件缓存，然后交由操作系统控制持久化到磁盘的时机。但是这样做的风险是，主机掉电的时候会丢数据。



### count 优化

#### count 是什么

count() 是一个聚合函数，对于返回的结果集，一行行地判断，如果 count 函数的参数不是 NULL，累计值就加 1，否则不加，最后返回累计值。



#### InnoDB 引擎和 MyISAM 引擎 count 区别

MyISAM 引擎

* 使用 MyISAM 引擎时，执行 count 函数只需要 O(1 )复杂度，这是因为每张 MyISAM 的数据表都有一个 meta 信息有存储了row_count值，由表级锁保证一致性，所以直接读取 row_count 值就是 count 函数的执行结果。
* 而当带上 where 条件语句之后，MyISAM 跟 InnoDB 就没有区别了，它们都需要扫描表来进行记录个数的统计。

 InnoDB 引擎

* InnoDB 引擎就麻烦了，它执行 `count(*)` 的时候，需要把数据一行一行地从引擎里面读出 来，然后累积计数。
* 而 InnoDB 存储引擎是支持事务的，同一个时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB 表“应该返回多少行”也是不确定的，所以无法像 MyISAM一样，只维护一个 row_count 变量。





#### 性能比较

count(*) = count(1) > count(主键字段) > count(字段)



#### 执行过程

| count 用法  | 执行过程                                                     |
| ----------- | ------------------------------------------------------------ |
| count(主键) | InnoDB 引擎会遍历整张表，把每一行的 主键id 值都取出来，返回给服务层。 服务层拿到主键后，直接按行进行累加(主键不可能为null)<br />**如果表里存在二级索引**，优化器就会选择二级索引进行扫描。 |
| count(字段) | **全表扫描**<br />没有not null 约束 : InnoDB 引擎会遍历整张表把每一行的字段值都取出 来，返回给服务层，服务层判断是否为null，不为null，计数累加。 <br />有not null 约束：InnoDB 引擎会遍历整张表把每一行的字段值都取出来，返回给服务层，直接按行进行累加。 |
| count(数字) | **InnoDB 引擎遍历整张表，但不取值**。服务层对于返回的每一行，放一个数字“1” 进去，直接按行进行累加。<br />**如果表里存在二级索引**，优化器就会选择二级索引进行扫描。 |
| count(*)    | InnoDB引擎并不会把全部字段取出来，而是专门做了优化，**不取值**，服务层直接按行进行累加。<br />**如果表里存在二级索引**，优化器就会选择二级索引进行扫描。<br />**count(`*`) 其实等于 count(`0`)**，也就是说，当你使用 count(`*`) 时，MySQL 会将 `*` 参数转化为参数 0 来处理。 |



#### 如何优化 count(*)？

**第一种，近似值：show table status 或者 explain** 

如果你的业务对于统计个数不需要很精确，比如搜索引擎在搜索关键词的时候，给出的搜索结果条数是一个大概值。

这时，我们就可以使用 show table status 或者 explain 命令来表进行估算。

执行 explain 命令效率是很高的，因为它并不会真正的去查询，下图中的 rows 字段值就是 explain 命令对表 t_order 记录的估算值。



**第二种，额外表保存计数值**

如果是想精确的获取表的记录总数，我们可以将这个计数值保存到单独的一张计数表中。

当我们在数据表插入一条记录的同时，将计数表中的计数字段 + 1。也就是说，在新增和删除操作时，我们需要额外维护这个计数表。



### MySQL 单表不要超过 2000W 行，靠谱吗

作为在后端圈开车的多年老司机，是不是经常听到过：

- “MySQL 单表最好不要超过 2000W”
- “单表超过 2000W 就要考虑数据迁移了”
- “你这个表数据都马上要到 2000W 了，难怪查询速度慢”

这些名言民语就和 “群里只讨论技术，不开车，开车速度不要超过 120 码，否则自动踢群”，只听过，没试过，哈哈。

下面我们就把车速踩到底，干到 180 码试试…….

> 原文链接：https://my.oschina.net/u/4090830/blog/5559454

#### **实验**

实验一把看看… 建一张表

```sql
CREATE TABLE person(
    id int NOT NULL AUTO_INCREMENT PRIMARY KEY comment '主键',
    person_id tinyint not null comment '用户id',
    person_name VARCHAR(200) comment '用户名称',
    gmt_create datetime comment '创建时间',
    gmt_modified datetime comment '修改时间'
) comment '人员信息表';
```

插入一条数据

```sql
insert into person values(1, 1,'user_1', NOW(), now());
```

利用 MySQL 伪列 rownum 设置伪列起始点为 1

```sql
select (@i:=@i+1) as rownum, person_name from person, (select @i:=100) as init; 
set @i=1;
```

运行下面的 sql，连续执行 20 次，就是 2 的 20 次方约等于 100w 的数据；执行 23 次就是 2 的 23 次方约等于 800w , 如此下去即可实现千万测试数据的插入。

如果不想翻倍翻倍的增加数据，而是想少量，少量的增加，有个技巧，就是在 SQL 的后面增加 where 条件，如 id > 某一个值去控制增加的数据量即可。

```sql
insert into person(id, person_id, person_name, gmt_create, gmt_modified)
select @i:=@i+1,
left(rand()*10,10) as person_id,
concat('user_',@i%2048),
date_add(gmt_create,interval + @i*cast(rand()*100 as signed) SECOND),
date_add(date_add(gmt_modified,interval +@i*cast(rand()*100 as signed) SECOND), interval + cast(rand()*1000000 as signed) SECOND)
from person;
```

此处需要注意的是，也许你在执行到近 800w 或者 1000w 数据的时候，会报错：The total number of locks exceeds the lock table size。

这是由于你的临时表内存设置的不够大，只需要扩大一下设置参数即可。

```sql
SET GLOBAL tmp_table_size =512*1024*1024; （512M）
SET global innodb_buffer_pool_size= 1*1024*1024*1024 (1G);
```

先来看一组测试数据，这组数据是在 MySQL 8.0 的版本，并且是在我本机上，由于本机还跑着 idea , 浏览器等各种工具，所以并不是机器配置就是用于数据库配置，所以测试数据只限于参考。

![图片](https://img-blog.csdnimg.cn/img_convert/db5d7c556b3e8e31b3b90d4ace54fe7e.png)

![图片](https://img-blog.csdnimg.cn/img_convert/d556bf1db3d3ebd0e781ebda33a916d6.png)

看到这组数据似乎好像真的和标题对应，当数据达到 2000W 以后，查询时长急剧上升，难道这就是铁律吗？

那下面我们就来看看这个建议值 2000W 是怎么来的？

#### **单表数量限制**

首先我们先想想数据库单表行数最大多大？

```sql
CREATE TABLE person(
    id int(10) NOT NULL AUTO_INCREMENT PRIMARY KEY comment '主键',
    person_id tinyint not null comment '用户id',
    person_name VARCHAR(200) comment '用户名称',
    gmt_create datetime comment '创建时间',
    gmt_modified datetime comment '修改时间'
) comment '人员信息表';
```

看看上面的建表 sql。id 是主键，本身就是唯一的，也就是说主键的大小可以限制表的上限：

- 如果主键声明 `int` 类型，也就是 32 位，那么支持 2^32-1 ~~21 亿；
- 如果主键声明 `bigint` 类型，那就是 2^62-1 （36893488147419103232），难以想象这个的多大了，一般还没有到这个限制之前，可能数据库已经爆满了！！

有人统计过，如果建表的时候，自增字段选择无符号的 bigint , 那么自增长最大值是 18446744073709551615，按照一秒新增一条记录的速度，大约什么时候能用完？

![图片](https://img-blog.csdnimg.cn/img_convert/87031fea63547be0f8ea692781d5b068.png)

#### **表空间**

下面我们再来看看索引的结构，我们下面讲内容都是基于 Innodb 引擎的，大家都知道 Innodb 的索引内部用的是 B+ 树。

![图片](https://img-blog.csdnimg.cn/img_convert/cb6f4fddc960cde55575a1b80a563b9a.png)

这张表数据，在硬盘上存储也是类似如此的，它实际是放在一个叫 person.ibd （innodb data）的文件中，也叫做表空间；虽然数据表中，他们看起来是一条连着一条，但是实际上在文件中它被分成很多小份的数据页，而且每一份都是 16K。

大概就像下面这样，当然这只是我们抽象出来的，在表空间中还有段、区、组等很多概念，但是我们需要跳出来看。

![图片](https://img-blog.csdnimg.cn/img_convert/ee29f7137057b226658627ff61a6a514.png)

#### **页的数据结构**

实际页的内部结构像是下面这样的：

![图片](https://img-blog.csdnimg.cn/img_convert/c34b589e12e5bc0855c9bdeab0c63a88.png)

从图中可以看出，一个 InnoDB 数据页的存储空间大致被划分成了 7 个部分，有的部分占用的字节数是确定的，有的部分占用的字节数是不确定的。

在页的 7 个组成部分中，我们自己存储的记录会按照我们指定的行格式存储到 `User Records` 部分。

但是在一开始生成页的时候，其实并没有 User Records 这个部分，每当我们插入一条记录，都会从 Free Space 部分，也就是尚未使用的存储空间中申请一个记录大小的空间划分到 User Records 部分。

当 Free Space 部分的空间全部被 User Records 部分替代掉之后，也就意味着这个页使用完了，如果还有新的记录插入的话，就需要去申请新的页了。

这个过程的图示如下：

![图片](https://img-blog.csdnimg.cn/img_convert/ea5cc8c67b7656d3f2a11e42293a0244.png)

刚刚上面说到了数据的新增的过程。

那下面就来说说，数据的查找过程，假如我们需要查找一条记录，我们可以把表空间中的每一页都加载到内存中，然后对记录挨个判断是不是我们想要的。

在数据量小的时候，没啥问题，内存也可以撑。但是现实就是这么残酷，不会给你这个局面。

为了解决这问题，MySQL 中就有了索引的概念，大家都知道索引能够加快数据的查询，那到底是怎么个回事呢？下面我就来看看。

#### **索引的数据结构**

在 MySQL 中索引的数据结构和刚刚描述的页几乎是一模一样的，而且大小也是 16K,。

但是在索引页中记录的是页 (数据页，索引页) 的最小主键 id 和页号，以及在索引页中增加了层级的信息，从 0 开始往上算，所以页与页之间就有了上下层级的概念。

![图片](https://img-blog.csdnimg.cn/img_convert/6374409c6c404d446855dc6a694b6d26.png)

看到这个图之后，是不是有点似曾相似的感觉，是不是像一棵二叉树啊，对，没错！它就是一棵树。

只不过我们在这里只是简单画了三个节点，2 层结构的而已，如果数据多了，可能就会扩展到 3 层的树，这个就是我们常说的 B+ 树，最下面那一层的 page level =0, 也就是叶子节点，其余都是非叶子节点。

![图片](https://img-blog.csdnimg.cn/img_convert/baf6644df710e1639e8e956b1f67d502.png)

看上图中，我们是单拿一个节点来看，首先它是一个非叶子节点（索引页），在它的内容区中有 id 和 页号地址两部分：

- id ：对应页中记录的最小记录 id 值；
- 页号：地址是指向对应页的指针；

而数据页与此几乎大同小异，区别在于数据页记录的是真实的行数据而不是页地址，而且 id 的也是顺序的。

#####**单表建议值**

下面我们就以 3 层，2 分叉（实际中是 M 分叉）的图例来说明一下查找一个行数据的过程。

![图片](https://img-blog.csdnimg.cn/img_convert/585429e5078566bda9b2fa18f85215af.png)

比如说我们需要查找一个 id=6 的行数据：

- 因为在非叶子节点中存放的是页号和该页最小的 id，所以我们从顶层开始对比，首先看页号 10 中的目录，有 [id=1, 页号 = 20],[id=5, 页号 = 30], 说明左侧节点最小 id 为 1，右侧节点最小 id 是 5。6>5, 那按照二分法查找的规则，肯定就往右侧节点继续查找；
- 找到页号 30 的节点后，发现这个节点还有子节点（非叶子节点），那就继续比对，同理，6>5 && 6<7, 所以找到了页号 60；
- 找到页号 60 之后，发现此节点为叶子节点（数据节点），于是将此页数据加载至内存进行一一对比，结果找到了 id=6 的数据行。

从上述的过程中发现，我们为了查找 id=6 的数据，总共查询了三个页，如果三个页都在磁盘中（未提前加载至内存），那么最多需要经历三次的磁盘 IO。

需要注意的是，图中的页号只是个示例，实际情况下并不是连续的，在磁盘中存储也不一定是顺序的。

至此，我们大概已经了解了表的数据是怎么个结构了，也大概知道查询数据是个怎么的过程了，这样我们也就能大概估算这样的结构能存放多少数据了。

从上面的图解我们知道 B+ 数的叶子节点才是存在数据的，而非叶子节点是用来存放索引数据的。

所以，同样一个 16K 的页，非叶子节点里的每条数据都指向新的页，而新的页有两种可能

- 如果是叶子节点，那么里面就是一行行的数据
- 如果是非叶子节点的话，那么就会继续指向新的页

假设

- 非叶子节点内指向其他页的数量为 x
- 叶子节点内能容纳的数据行数为 y
- B+ 数的层数为 z

如下图中所示，**Total =x^(z-1) \*y 也就是说总数会等于 x 的 z-1 次方 与 Y 的乘积**。

![图片](https://img-blog.csdnimg.cn/img_convert/e741373dcb282fce80d1522d33c6b53b.png)

> X =？

在文章的开头已经介绍了页的结构，索引也也不例外，都会有 File Header (38 byte)、Page Header (56 Byte)、Infimum + Supermum（26 byte）、File Trailer（8byte）, 再加上页目录，大概 1k 左右。

我们就当做它就是 1K, 那整个页的大小是 16K, 剩下 15k 用于存数据，在索引页中主要记录的是主键与页号，主键我们假设是 Bigint (8 byte), 而页号也是固定的（4Byte）, 那么索引页中的一条数据也就是 12byte。

所以 x=15*1024/12≈1280 行。

> Y=？

叶子节点和非叶子节点的结构是一样的，同理，能放数据的空间也是 15k。

但是叶子节点中存放的是真正的行数据，这个影响的因素就会多很多，比如，字段的类型，字段的数量。每行数据占用空间越大，页中所放的行数量就会越少。

这边我们暂时按一条行数据 1k 来算，那一页就能存下 15 条，Y = 15*1024/1000 ≈15。

算到这边了，是不是心里已经有谱了啊。

根据上述的公式，Total =x^(z-1) *y，已知 x=1280，y=15：

- 假设 B+ 树是两层，那就是 z = 2， Total = （1280 ^1 ）*15 = 19200
- 假设 B+ 树是三层，那就是 z = 3， Total = （1280 ^2） *15 = 24576000 （约 2.45kw）

哎呀，妈呀！这不是正好就是文章开头说的最大行数建议值 2000W 嘛！对的，一般 B+ 数的层级最多也就是 3 层。

你试想一下，如果是 4 层，除了查询的时候磁盘 IO 次数会增加，而且这个 Total 值会是多少，大概应该是 3 百多亿吧，也不太合理，所以，3 层应该是比较合理的一个值。

> 到这里难道就完了？

不。

我们刚刚在说 Y 的值时候假设的是 1K ，那比如我实际当行的数据占用空间不是 1K , 而是 5K, 那么单个数据页最多只能放下 3 条数据。

同样，还是按照 z = 3 的值来计算，那 Total = （1280 ^2） *3 = 4915200 （近 500w）

所以，在保持相同的层级（相似查询性能）的情况下，在行数据大小不同的情况下，其实这个最大建议值也是不同的，而且影响查询性能的还有很多其他因素，比如，数据库版本，服务器配置，sql 的编写等等。

MySQL 为了提高性能，会将表的索引装载到内存中，在 InnoDB buffer size 足够的情况下，其能完成全加载进内存，查询不会有问题。

但是，当单表数据库到达某个量级的上限时，导致内存无法存储其索引，使得之后的 SQL 查询会产生磁盘 IO，从而导致性能下降，所以增加硬件配置（比如把内存当磁盘使），可能会带来立竿见影的性能提升哈。

#### **总结**

- MySQL 的表数据是以页的形式存放的，页在磁盘中不一定是连续的。
- 页的空间是 16K, 并不是所有的空间都是用来存放数据的，会有一些固定的信息，如，页头，页尾，页码，校验码等等。
- 在 B+ 树中，叶子节点和非叶子节点的数据结构是一样的，区别在于，叶子节点存放的是实际的行数据，而非叶子节点存放的是主键和页号。
- 索引结构不会影响单表最大行数，2000W 也只是推荐值，超过了这个值可能会导致 B + 树层级更高，影响查询性能。

