# 同步互斥

## 问题

### 锁

* 知道哪些种类的锁？各自特点和适用环境？
  * 互斥锁与自旋锁？
  * 读写锁？
  * 乐观锁与悲观锁？
  * CAS 锁？
* 谈谈死锁
  * 死锁是什么？
  * 形成死锁的条件？
  * 死锁处理？
  * 死锁检测？
  * 死锁恢复？
  * 死锁预防？
  * 死锁避免？
  * 如何用工具排查死锁？
* 锁的可重入不可重入了解吗 ？
* 忙等待锁怎么实现？
* 无等待锁怎么实现？
* 谈谈 Linux futex？
* 谈谈 Linux RCU？

### 条件变量

* 什么是条件变量
* 为什么 pthread_cond_wait 需要加锁？
* 在生产者线程中修改条件时为什么要加 mutex？
* 消费者线程中判断条件为什么要放在 while 中？
* signal 到底是放在 unlock 之前还是之后？
* 条件变量实现机制？
* 虚假唤醒和唤醒丢失问题？

### 信号量

* 记录型和整型信号量的区别与使用方法？
* 信号量机制怎么实现的？
* 信号量与互斥量的区别

### 原子操作

* 原子操作的是如何实现的？

### 经典问题

* 谈谈生产者-消费者模式？
  * 什么是生产者-消费者模式？
  * 为什么要使用生产者消费者模式？
  * 生产者-消费者模式的特点？
  * 生产者-消费者模式的应用场景？
  * 生产者-消费者模式的优点？
* 谈谈不同的生产者消费者模型？
  * 单生产者单消费者 SPSC
  * 多生产者单消费者 MPSC
  * 单生产者多消费者 SPMC
  * 多生产者多消费者-单缓冲区 MPMC-SB
  * 多生产者多消费者-双缓冲区 MPMC-MB
  * 多生产者多消费者-多缓冲区 MPMC-MB
  * 多生产者多消费者-环形缓冲区 MPMC-RingBuffer
* 哲学家就餐问题？
* 读者写者问题？

 <https://blog.csdn.net/weixin_47187147/article/details/122235160>

## 回答

### 锁

#### 知道哪些种类的锁？各自特点和适用环境？

**互斥锁与自旋锁**

最底层的两种就是会「互斥锁和自旋锁」，有很多高级的锁都是基于它们实现的，你可以认为它们是各种锁的地基，所以我们必须清楚它俩之间的区别和应用。

加锁的目的就是保证共享资源在任意时间里，只有一个线程访问，这样就可以避免多线程导致共享数据错乱的问题。

当已经有一个线程加锁后，其他线程加锁则就会失败，互斥锁和自旋锁对于加锁失败后的处理方式是不一样的：

* **互斥锁**加锁失败后，线程会**释放 CPU** ，给其他线程；
* **自旋锁**加锁失败后，线程会**忙等待**，直到它拿到锁；

互斥锁是一种「独占锁」，比如当线程 A 加锁成功后，此时互斥锁已经被线程 A 独占了，只要线程 A 没有释放手中的锁，线程 B 加锁就会失败，于是就会释放 CPU 让给其他线程，**既然线程 B 释放掉了 CPU，自然线程 B 加锁的代码就会被阻塞**。

**对于互斥锁加锁失败而阻塞的现象，是由操作系统内核实现的**。当加锁失败时，内核会将线程置为「睡眠」状态，等到锁被释放后，内核会在合适的时机唤醒线程，当这个线程成功获取到锁后，于是就可以继续执行。如下图：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%94%81/%E4%BA%92%E6%96%A5%E9%94%81%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png)

所以，互斥锁加锁失败时，会从用户态陷入到内核态，让内核帮我们切换线程，虽然简化了使用锁的难度，但是存在一定的性能开销成本。

那这个开销成本是什么呢？会有**两次线程上下文切换的成本**：

* 当线程加锁失败时，内核会把线程的状态从「运行」状态设置为「睡眠」状态，然后把 CPU 切换给其他线程运行；
* 接着，当锁被释放时，之前「睡眠」状态的线程会变为「就绪」状态，然后内核会在合适的时间，把 CPU 切换给该线程运行。

线程的上下文切换的是什么？当两个线程是属于同一个进程，**因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据。**

上下切换的耗时有大佬统计过，大概在几十纳秒到几微秒之间，如果你锁住的代码执行时间比较短，那可能上下文切换的时间都比你锁住的代码执行时间还要长。

所以，**如果你能确定被锁住的代码执行时间很短，就不应该用互斥锁，而应该选用自旋锁，否则使用互斥锁。**

自旋锁是通过 CPU 提供的 `CAS` 函数（*Compare And Swap*），在「用户态」完成加锁和解锁操作，不会主动产生线程上下文切换，所以相比互斥锁来说，会快一些，开销也小一些。

一般加锁的过程，包含两个步骤：

* 第一步，查看锁的状态，如果锁是空闲的，则执行第二步；
* 第二步，将锁设置为当前线程持有；

CAS 函数就把这两个步骤合并成一条硬件级指令，形成**原子指令**，这样就保证了这两个步骤是不可分割的，要么一次性执行完两个步骤，要么两个步骤都不执行。

比如，设锁为变量 lock，整数 0 表示锁是空闲状态，整数 pid 表示线程 ID，那么 CAS(lock, 0, pid) 就表示自旋锁的加锁操作，CAS(lock, pid, 0) 则表示解锁操作。

使用自旋锁的时候，当发生多线程竞争锁的情况，加锁失败的线程会「忙等待」，直到它拿到锁。这里的「忙等待」可以用 `while` 循环等待实现，不过最好是使用 CPU 提供的 `PAUSE` 指令来实现「忙等待」，因为可以减少循环等待时的耗电量。

自旋锁是最比较简单的一种锁，一直自旋，利用 CPU 周期，直到锁可用。**需要注意，在单核 CPU 上，需要抢占式的调度器（即不断通过时钟中断一个线程，运行其他线程）。否则，自旋锁在单 CPU 上无法使用，因为一个自旋的线程永远不会放弃 CPU。**

自旋锁开销少，在多核系统下一般不会主动产生线程切换，适合异步、协程等在用户态切换请求的编程方式，但如果被锁住的代码执行时间过长，自旋的线程会长时间占用 CPU 资源，所以自旋的时间和被锁住的代码执行的时间是成「正比」的关系，我们需要清楚的知道这一点。

自旋锁与互斥锁使用层面比较相似，但实现层面上完全不同：**当加锁失败时，互斥锁用「线程切换」来应对，自旋锁则用「忙等待」来应对**。

它俩是锁的最基本处理方式，更高级的锁都会选择其中一个来实现，比如读写锁既可以选择互斥锁实现，也可以基于自旋锁实现。

------

**读写锁**

读写锁从字面意思我们也可以知道，它由「读锁」和「写锁」两部分构成，如果只读取共享资源用「读锁」加锁，如果要修改共享资源则用「写锁」加锁。

所以，**读写锁适用于能明确区分读操作和写操作的场景**。

读写锁的工作原理是：

* 当「写锁」没有被线程持有时，多个线程能够并发地持有读锁，这大大提高了共享资源的访问效率，因为「读锁」是用于读取共享资源的场景，所以多个线程同时持有读锁也不会破坏共享资源的数据。
* 但是，一旦「写锁」被线程持有后，读线程的获取读锁的操作会被阻塞，而且其他写线程的获取写锁的操作也会被阻塞。

所以说，写锁是独占锁，因为任何时刻只能有一个线程持有写锁，类似互斥锁和自旋锁，而读锁是共享锁，因为读锁可以被多个线程同时持有。

知道了读写锁的工作原理后，我们可以发现，**读写锁在读多写少的场景，能发挥出优势**。

另外，根据实现的不同，读写锁可以分为「读优先锁」和「写优先锁」。

读优先锁期望的是，读锁能被更多的线程持有，以便提高读线程的并发性，它的工作方式是：当读线程 A 先持有了读锁，写线程 B 在获取写锁的时候，会被阻塞，并且在阻塞过程中，后续来的读线程 C 仍然可以成功获取读锁，最后直到读线程 A 和 C 释放读锁后，写线程 B 才可以成功获取写锁。如下图：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%94%81/%E8%AF%BB%E4%BC%98%E5%85%88%E9%94%81%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png)

而「写优先锁」是优先服务写线程，其工作方式是：当读线程 A 先持有了读锁，写线程 B 在获取写锁的时候，会被阻塞，并且在阻塞过程中，后续来的读线程 C 获取读锁时会失败，于是读线程 C 将被阻塞在获取读锁的操作，这样只要读线程 A 释放读锁后，写线程 B 就可以成功获取写锁。如下图：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%94%81/%E5%86%99%E4%BC%98%E5%85%88%E9%94%81%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png)

读优先锁对于读线程并发性更好，但也不是没有问题。我们试想一下，如果一直有读线程获取读锁，那么写线程将永远获取不到写锁，这就造成了写线程「饥饿」的现象。

写优先锁可以保证写线程不会饿死，但是如果一直有写线程获取写锁，读线程也会被「饿死」。

**既然不管优先读锁还是写锁，对方可能会出现饿死问题，那么我们就不偏袒任何一方，搞个「公平读写锁」。**

**公平读写锁比较简单的一种方式是：用队列把获取锁的线程排队，不管是写线程还是读线程都按照先进先出的原则加锁即可，这样读线程仍然可以并发，也不会出现「饥饿」的现象。**

互斥锁和自旋锁都是最基本的锁，读写锁可以根据场景来选择这两种锁其中的一个进行实现。

------

**乐观锁与悲观锁**

前面提到的互斥锁、自旋锁、读写锁，都是属于悲观锁。

悲观锁做事比较悲观，它认为**多线程同时修改共享资源的概率比较高，于是很容易出现冲突，所以访问共享资源前，先要上锁**。

那相反的，如果多线程同时修改共享资源的概率比较低，就可以采用乐观锁。

乐观锁做事比较乐观，它假定冲突的概率很低，它的工作方式是：**先修改完共享资源，再验证这段时间内有没有发生冲突，如果没有其他线程在修改资源，那么操作完成，如果发现有其他线程已经修改过这个资源，就放弃本次操作**。

放弃后如何重试，这跟业务场景息息相关，虽然重试的成本很高，但是冲突的概率足够低的话，还是可以接受的。

可见，乐观锁的心态是，不管三七二十一，先改了资源再说。另外，你会发现**乐观锁全程并没有加锁，所以它也叫无锁编程**。

这里举一个场景例子：在线文档。

我们都知道在线文档可以同时多人编辑的，如果使用了悲观锁，那么只要有一个用户正在编辑文档，此时其他用户就无法打开相同的文档了，这用户体验当然不好了。

那实现多人同时编辑，实际上是用了乐观锁，它允许多个用户打开同一个文档进行编辑，编辑完提交之后才验证修改的内容是否有冲突。

怎么样才算发生冲突？这里举个例子，比如用户 A 先在浏览器编辑文档，之后用户 B 在浏览器也打开了相同的文档进行编辑，但是用户 B 比用户 A 提交早，这一过程用户 A 是不知道的，当 A 提交修改完的内容时，那么 A 和 B 之间并行修改的地方就会发生冲突。

服务端要怎么验证是否冲突了呢？通常方案如下：

* 由于发生冲突的概率比较低，所以先让用户编辑文档，但是浏览器在下载文档时会记录下服务端返回的文档版本号；
* 当用户提交修改时，发给服务端的请求会带上原始文档版本号，服务器收到后将它与当前版本号进行比较，如果版本号不一致则提交失败，如果版本号一致则修改成功，然后服务端版本号更新到最新的版本号。

实际上，我们常见的 SVN 和 Git 也是用了乐观锁的思想，先让用户编辑代码，然后提交的时候，通过版本号来判断是否产生了冲突，发生了冲突的地方，需要我们自己修改后，再重新提交。

乐观锁虽然去除了加锁解锁的操作，但是一旦发生冲突，重试的成本非常高，所以**只有在冲突概率非常低，且加锁成本非常高的场景时，才考虑使用乐观锁。**

**CAS 锁？**

CAS 是乐观锁的一种实现，CAS 全称是比较和替换，CAS 的操作主要由以下几个步骤组成：

1. 先查询原始值
2. 操作时比较原始值是否修改
3. 如果修改，则操作失败，禁止更新操作，如果没有发生修改，则更新为新值

上述三个步骤是一个原子性操作，不可以被拆分执行。

![img](https://pic4.zhimg.com/v2-de62c2ce5711fe38b4b57ff20e511e87_b.jpg)

CAS 的优势：CAS 是一种无锁操作，不需要加锁，避免了线程切换的开销。

CAS 虽然在低并发量的情况下可以减少系统的开销，但是CAS也有一些问题：

* CPU 开销过大问题
* ABA 问题
* 只能针对一个共享变量

CPU 开销过大

在我们使用 CAS 时，如果并发量过大，我们的程序有可能会一直自旋，长时间占用CPU资源。

ABA 问题

假设有个共享变量 J，原始值为 1。

1. 线程 A 读取变量 J ，值为 1
2. 线程 B 读取变量 J，值为 1
3. 线程 A 变量 J+1，CAS 成功从 1 修改为 2
4. 线程 C 读取变量 J，值为 2
5. 线程 C 将变量 J-1，CAS 成功从 2 修改为 1
6. 线程 B 通过 CAS 比较和替换，依然可以改为自己想修改的值

上述过程，线程 A 和 C 已经将变量 J 的值已经改变了，但是线程 B 无法发现，依然可以修改共享变量，这就产生了 ABA 问题。

如何防止 CAS 的 ABA：加标志位（version）。至于标志位可以是自增的数字，也可以是时间戳。通过标志位我们可以精确的知道每次修改。

共享变量单一

CAS操作单个共享变量的时候可以保证原子的操作，无法操作多个变量。但是在JDK5之后，AtomicReference可以用来保证对象之间的原子性，我们可以把多个对象放入CAS中操作。

**CAS 不是乐观锁吗，为什么基于 CAS 实现的自旋锁是悲观锁？**

乐观锁是先修改同步资源，再验证有没有发生冲突。

悲观锁是修改共享数据前，都要先加锁，防止竞争。

CAS 是乐观锁没错，但是 CAS 和自旋锁不同之处，自旋锁基于 CAS 加了while 或者睡眠 CPU 的操作而产生自旋的效果，加锁失败会忙等待直到拿到锁，自旋锁是要需要事先拿到锁才能修改数据的，所以算悲观锁。

#### 谈谈死锁

**死锁是什么**？

那么，当两个线程为了保护两个不同的共享资源而使用了两个互斥锁，那么这两个互斥锁应用不当的时候，可能会造成**两个线程都在等待对方释放锁**，在没有外力的作用下，这些线程会一直相互等待，就没办法继续运行，这种情况就是发生了**死锁**。

**形成死锁的条件**？

同时满足

* 互斥条件：多个线程不能同时使用同一个资源
* 持有并等待条件：持有并等待条件是指，当线程 A 已经持有了资源 1，又想申请资源 2，而资源 2 已经被线程 C 持有了，所以线程 A 就会处于等待状态，但是线程 A 在等待资源 2 的同时并不会释放自己已经持有的资源 1
* 不可剥夺条件：当线程已经持有了资源 ，在自己使用完之前不能被其他线程获取
* 环路等待条件：在死锁发生的时候，两个线程获取资源的顺序构成了环形链。

**死锁的处理方法？**

* 鸵鸟策略：把头埋在沙子里，假装根本没发生问题
* 死锁检测与死锁恢复：不试图阻止死锁，而是当检测到死锁发生时，采取措施进行恢复
* 死锁预防：破除四个条件
* 死锁避免：在程序运行时避免发生死锁，安全状态 + 银行家算法

**鸵鸟策略**

把头埋在沙子里，假装根本没发生问题。

因为解决死锁问题的代价很高，因此鸵鸟策略这种不采取任务措施的方案会获得更高的性能。

当发生死锁时不会对用户造成多大影响，或发生死锁的概率很低，可以采用鸵鸟策略。

大多数操作系统，包括 Unix，Linux 和 Windows，处理死锁问题的办法仅仅是忽略它。

**死锁检测**

<https://blog.csdn.net/qq_42956653/article/details/126313099#t6>

死锁问题转换为有向图环路问题。

* 用 pair 把锁和线程绑定
* hook pthread_mutex_lock 函数
  * 加锁前调用 lock_before 函数：检测这个锁有没有被别的线程占用，如果被占用，那么我们就需要往图里面加一条边
  * 如果没有被占用，那么我们就往里面走。也就是说加锁完，调用系统提供的lock之后， 我们需要告诉后面的线程，这个锁被我占用了，即添加一项pair，供别人lock之前去检测。
* hook pthread_mutex_unlock 函数
  * 如果被占用了，然后锁被释放，本线程获取到了这个以前被占用的锁，那么我们lock之后，需要把原来添加的一条边删除掉，因为这个锁已经属于自己了，并且将锁对应的pair中的th_id改成自己。
  * 在调用系统提供的unlock之后，解锁了一个锁之后，我们去看看还有没有渴望得到这个锁的，如果没有，则将锁对应的pair置空，如果有则不管pair。
* dfs 判环

**死锁恢复（解除）？**

* 利用抢占恢复
* 利用回滚恢复
* 通过杀死进程恢复

**死锁预防？**

在程序运行之前预防发生死锁。

1. 破坏互斥条件：例如假脱机打印机技术允许若干个进程同时输出，唯一真正请求物理打印机的进程是打印机守护进程。
2. 破坏请求和保持条件：一种实现方式是规定所有进程在开始执行前请求所需要的全部资源。
3. 破坏不剥夺条件：允许抢占资源
4. 破坏循环请求等待：给资源统一编号，进程只能按编号顺序来请求资源。

**死锁避免？**

一、安全序列

![img](https://img2018.cnblogs.com/i-beta/1358881/201911/1358881-20191125164249992-1856910147.png)

 **注意：**

（1）系统在某一时刻的安全状态可能不唯一，但这不影响对系统安全性的判断。
（2）安全状态是非死锁状态，而不安全状态并不一定是死锁状态。即系统处于安全状态一定可以避免死锁，而系统处于不安全状态则仅仅可能进入死锁状态。

![img](https://img2018.cnblogs.com/i-beta/1358881/201911/1358881-20191125164352815-1516999888.png)

二、银行家算法

银行家算法的实质就是**要设法保证系统动态分配资源后不进入不安全状态，以避免可能产生的死锁。**即没当进程提出资源请求且系统的资源能够满足该请求时，系统将判断满足此次资源请求后系统状态是否安全，如果判断结果为安全，则给该进程分配资源，否则不分配资源，申请资源的进程将阻塞。

银行家算法的执行有个前提条件，即要求进程预先提出自己的最大资源请求，并假设系统拥有固定的资源总量。下面介绍银行家算法所用的主要的数据结构。

![img](https://img2018.cnblogs.com/i-beta/1358881/201911/1358881-20191125165114981-1954446360.png)

 ![img](https://img2018.cnblogs.com/i-beta/1358881/201911/1358881-20191125171020518-1171969079.png)

![img](https://img2018.cnblogs.com/i-beta/1358881/201911/1358881-20191125171200431-333419697.png)

**如何用工具排查死锁**？

由于小林的死锁代码例子是 C 写的，在 Linux 下，我们可以使用 `pstack` + `gdb` 工具来定位死锁问题。

pstack 命令可以显示每个线程的栈跟踪信息（函数调用过程），它的使用方式也很简单，只需要 `pstack <pid>` 就可以了。

那么，在定位死锁问题时，我们可以多次执行 pstack 命令查看线程的函数调用过程，多次对比结果，确认哪几个线程一直没有变化，且是因为在等待锁，那么大概率是由于死锁问题导致的。

我用 pstack 输出了我前面模拟死锁问题的进程的所有线程的情况，我多次执行命令后，其结果都一样，如下：

```shell
$ pstack 87746
Thread 3 (Thread 0x7f60a610a700 (LWP 87747)):
#0  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0
#1  0x0000003720e093ca in _L_lock_829 () from /lib64/libpthread.so.0
#2  0x0000003720e09298 in pthread_mutex_lock () from /lib64/libpthread.so.0
#3  0x0000000000400725 in threadA_proc ()
#4  0x0000003720e07893 in start_thread () from /lib64/libpthread.so.0
#5  0x00000037206f4bfd in clone () from /lib64/libc.so.6
Thread 2 (Thread 0x7f60a5709700 (LWP 87748)):
#0  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0
#1  0x0000003720e093ca in _L_lock_829 () from /lib64/libpthread.so.0
#2  0x0000003720e09298 in pthread_mutex_lock () from /lib64/libpthread.so.0
#3  0x0000000000400792 in threadB_proc ()
#4  0x0000003720e07893 in start_thread () from /lib64/libpthread.so.0
#5  0x00000037206f4bfd in clone () from /lib64/libc.so.6
Thread 1 (Thread 0x7f60a610c700 (LWP 87746)):
#0  0x0000003720e080e5 in pthread_join () from /lib64/libpthread.so.0
#1  0x0000000000400806 in main ()

....

$ pstack 87746
Thread 3 (Thread 0x7f60a610a700 (LWP 87747)):
#0  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0
#1  0x0000003720e093ca in _L_lock_829 () from /lib64/libpthread.so.0
#2  0x0000003720e09298 in pthread_mutex_lock () from /lib64/libpthread.so.0
#3  0x0000000000400725 in threadA_proc ()
#4  0x0000003720e07893 in start_thread () from /lib64/libpthread.so.0
#5  0x00000037206f4bfd in clone () from /lib64/libc.so.6
Thread 2 (Thread 0x7f60a5709700 (LWP 87748)):
#0  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0
#1  0x0000003720e093ca in _L_lock_829 () from /lib64/libpthread.so.0
#2  0x0000003720e09298 in pthread_mutex_lock () from /lib64/libpthread.so.0
#3  0x0000000000400792 in threadB_proc ()
#4  0x0000003720e07893 in start_thread () from /lib64/libpthread.so.0
#5  0x00000037206f4bfd in clone () from /lib64/libc.so.6
Thread 1 (Thread 0x7f60a610c700 (LWP 87746)):
#0  0x0000003720e080e5 in pthread_join () from /lib64/libpthread.so.0
#1  0x0000000000400806 in main ()
```

可以看到，Thread 2 和 Thread 3 一直阻塞获取锁（*pthread_mutex_lock*）的过程，而且 pstack 多次输出信息都没有变化，那么可能大概率发生了死锁。

但是，还不能够确认这两个线程是在互相等待对方的锁的释放，因为我们看不到它们是等在哪个锁对象，于是我们可以使用 gdb 工具进一步确认。

整个 gdb 调试过程，如下：

```shell
// gdb 命令
$ gdb -p 87746

// 打印所有的线程信息
(gdb) info thread
  3 Thread 0x7f60a610a700 (LWP 87747)  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0
  2 Thread 0x7f60a5709700 (LWP 87748)  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0
* 1 Thread 0x7f60a610c700 (LWP 87746)  0x0000003720e080e5 in pthread_join () from /lib64/libpthread.so.0
//最左边的 * 表示 gdb 锁定的线程，切换到第二个线程去查看

// 切换到第2个线程
(gdb) thread 2
[Switching to thread 2 (Thread 0x7f60a5709700 (LWP 87748))]#0  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0 

// bt 可以打印函数堆栈，却无法看到函数参数，跟 pstack 命令一样 
(gdb) bt
#0  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0
#1  0x0000003720e093ca in _L_lock_829 () from /lib64/libpthread.so.0
#2  0x0000003720e09298 in pthread_mutex_lock () from /lib64/libpthread.so.0
#3  0x0000000000400792 in threadB_proc (data=0x0) at dead_lock.c:25
#4  0x0000003720e07893 in start_thread () from /lib64/libpthread.so.0
#5  0x00000037206f4bfd in clone () from /lib64/libc.so.6

// 打印第三帧信息，每次函数调用都会有压栈的过程，而 frame 则记录栈中的帧信息
(gdb) frame 3
#3  0x0000000000400792 in threadB_proc (data=0x0) at dead_lock.c:25
27    printf("thread B waiting get ResourceA \n");
28    pthread_mutex_lock(&mutex_A);

// 打印mutex_A的值 ,  __owner表示gdb中标示线程的值，即LWP
(gdb) p mutex_A
$1 = {__data = {__lock = 2, __count = 0, __owner = 87747, __nusers = 1, __kind = 0, __spins = 0, __list = {__prev = 0x0, __next = 0x0}}, 
  __size = "\002\000\000\000\000\000\000\000\303V\001\000\001", '\000' <repeats 26 times>, __align = 2}

// 打印mutex_B的值 ,  __owner表示gdb中标示线程的值，即LWP
(gdb) p mutex_B
$2 = {__data = {__lock = 2, __count = 0, __owner = 87748, __nusers = 1, __kind = 0, __spins = 0, __list = {__prev = 0x0, __next = 0x0}}, 
  __size = "\002\000\000\000\000\000\000\000\304V\001\000\001", '\000' <repeats 26 times>, __align = 2}  
```

我来解释下，上面的调试过程：

1. 通过 `info thread` 打印了所有的线程信息，可以看到有 3 个线程，一个是主线程（LWP 87746），另外两个都是我们自己创建的线程（LWP 87747 和 87748）；
2. 通过 `thread 2`，将切换到第 2 个线程（LWP 87748）；
3. 通过 `bt`，打印线程的调用栈信息，可以看到有 threadB_proc 函数，说明这个是线程 B 函数，也就说 LWP 87748 是线程 B;
4. 通过 `frame 3`，打印调用栈中的第三个帧的信息，可以看到线程 B 函数，在获取互斥锁 A 的时候阻塞了；
5. 通过 `p mutex_A`，**打印互斥锁 A 对象信息，可以看到它被 LWP 为 87747（线程 A） 的线程持有着**；
6. 通过 `p mutex_B`，**打印互斥锁 B 对象信息，可以看到他被 LWP 为 87748 （线程 B） 的线程持有着**；

因为线程 B 在等待线程 A 所持有的 mutex_A, 而同时线程 A 又在等待线程 B 所拥有的mutex_B, 所以可以断定该程序发生了死锁。



#### 锁的可重入不可重入了解吗

java 里的概念

不可重入锁：只判断这个锁有没有被锁上，只要被锁上申请锁的线程都会被要求等待。实现简单

可重入锁：不仅判断锁有没有被锁上，还会判断锁是谁锁上的，当就是自己锁上的时候，那么他依旧可以再次访问临界资源，并把加锁次数加一。
设计了加锁次数，以在解锁的时候，可以确保所有加锁的过程都解锁了，其他线程才能访问。不然没有加锁的参考值，也就不知道什么时候解锁？解锁多少次？才能保证本线程已经访问完临界资源了可以唤醒其他线程访问了。实现相对复杂。

#### 忙等待锁怎么实现？

在说明「忙等待锁」的实现之前，先介绍现代 CPU 体系结构提供的特殊**原子操作指令 —— 测试和置位（Test-and-Set）指令**。

如果用 C 代码表示 Test-and-Set 指令，形式如下：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/13-TestAndSet.jpg)

测试并设置指令做了下述事情:

* 把 `old_ptr` 更新为 `new` 的新值
* 返回 `old_ptr` 的旧值；

当然，**关键是这些代码是原子执行**。因为既可以测试旧值，又可以设置新值，所以我们把这条指令叫作「测试并设置」。

那什么是原子操作呢？**原子操作就是要么全部执行，要么都不执行，不能出现执行到一半的中间状态**

我们可以运用 Test-and-Set 指令来实现「忙等待锁」，代码如下：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/14-%E8%87%AA%E6%97%8B%E9%94%81.jpg)

我们来确保理解为什么这个锁能工作：

* 第一个场景是，首先假设一个线程在运行，调用 `lock()`，没有其他线程持有锁，所以 `flag` 是 0。当调用 `TestAndSet(flag, 1)` 方法，返回 0，线程会跳出 while 循环，获取锁。同时也会原子的设置 flag 为1，标志锁已经被持有。当线程离开临界区，调用 `unlock()` 将 `flag` 清理为 0。
* 第二种场景是，当某一个线程已经持有锁（即 `flag` 为1）。本线程调用 `lock()`，然后调用 `TestAndSet(flag, 1)`，这一次返回 1。只要另一个线程一直持有锁，`TestAndSet()` 会重复返回 1，本线程会一直**忙等**。当 `flag` 终于被改为 0，本线程会调用 `TestAndSet()`，返回 0 并且原子地设置为 1，从而获得锁，进入临界区。

很明显，当获取不到锁时，线程就会一直 while 循环，不做任何事情，所以就被称为「忙等待锁」，也被称为**自旋锁（spin lock）**。

这是最简单的一种锁，一直自旋，利用 CPU 周期，直到锁可用。在单处理器上，需要抢占式的调度器（即不断通过时钟中断一个线程，运行其他线程）。否则，自旋锁在单 CPU 上无法使用，因为一个自旋的线程永远不会放弃 CPU。

#### 无等待锁怎么实现？

无等待锁顾明思议就是获取不到锁的时候，不用自旋。

既然不想自旋，那当没获取到锁的时候，就把当前线程放入到锁的等待队列，然后执行调度程序，把 CPU 让给其他线程执行。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/15-%E6%97%A0%E7%AD%89%E5%BE%85%E9%94%81.jpg)



#### 谈谈 Linux futex

##### 1 背景

万物皆连。尽管应用程序通常是可以拆分成多个独立子系统，但往往这些子系统的进程通常必须相互通信并在彼此之间共享状态。如数据库系统，它通常在用户空间中维护共享的I/O缓冲区。缓冲区由各种数据库引擎和预取进程并发访问。

在传统的UNIX系统中，System-V IPC（进程间通信）如信号量、消息队列、套接字和文件锁（flock ()) 是两个进程同步的基本机制。**这些机制在内核中会记录共享状态和原子操作的内核对象，然后暴露给用户一个句柄。这就意味着服务加解锁必须通过系统调用（例如，Semop ()). 这种方法的缺点是每次锁访问都需要系统调用，当锁具有低争用率时，系统调用可能会构成显著的开销。**

经研究发现，很多同步是无竞争的，即某个进程进入互斥区，到再从某个互斥区出来这段时间，常常是没有进程也要进这个互斥区或者请求同一同步变量的。但是在这种情况下，这个进程也要陷入内核去看看有没有人和它竞争，退出的时还要陷入内核去看看有没有进程等待在同一同步变量上。这些不必要的系统调用(或者说内核陷入)造成了大量的性能开销。为了解决这个问题，Futex就应运而生。futex是一种快速的用户级别的锁，他其实是由用户态和内核态协助完成。

**在无竞争的情况下，应用程序会自动更改锁状态字，而无需进入内核。因此，原子操作，如atomic_inc (), atomic_dec、cmpxchg (), 和test_and_set () 在用户空间中是必不可少的。**

**在有竞争环境下，应用程序需要等待锁的释放，或者在解锁操作的情况下需要唤醒等待任务。此时需要一个内核对象以及等待队列。**

其目标：

- 尽可能避免系统调用，因为系统调用通常会消耗几百个指令。
- 避免不必要的上下文切换： 上下文切换会使TLB无效等相关的开销，保存内核和用户态的栈。



futex 全称 fast userspace mutex，那么容易思考几个问题：
* 是否有所谓的 slow kernel mutex 呢？
* 是否有所谓的 fast kernel mutex 呢？
* 是否有所谓的 slow userspace mutex 呢？

接下来我将从两个方面来思考这些问题：userspace/kernel 和 slow path/fast path

**userspace/kernel**
众所周知并发控制需要同步与互斥机制的帮助，其中互斥机制我们一般有用户级和内核级两个层面
比较典型的例子如下：
用户级：利用原子操作实现的自旋锁，如果获取不到锁，通过自旋忙等待（直接通过共享访问锁）
内核级：利用睡眠锁(互斥锁），如果获取不到就让出 CPU，然后挂起睡眠（这需要内核的帮助，通过系统调用）

**slow path/fast path**
基于用户级和内核级这一分类，我们再来看看 slow path/fast path。

* slow path：上锁失败的路径
* fast path：上锁成功的路径
---
自旋锁
* 更快的 fast path
	* xchg 成功，立即进入临界区 
* 更慢的 slow path
	* xchg 失败，浪费 CPU 自旋等待 

自旋锁的缺陷：
* 自旋（共享变量）会触发处理器间的缓存同步，延迟增加
* 除了进入临界区的线程，其他处理器上的线程都在空转
* 争抢锁的处理器越多，利用率越低
* 获得自旋锁的线程可能被操作系统切换出去（因为操作系统不感知线程在做什么）
* 100% 的资源浪费

---

睡眠锁
* 更慢的 fast path
	* 即便上锁成功也要通过系统调用进出内核 
* 更快的 slow path
  * 上锁失败就不再占用 CPU 	



睡眠锁的缺陷：
* 无论是否上锁成功都需要进行系统调用，如果竞争比较少，那么开销还是比较大的
---

**两阶段锁：更快的 fast path，更快的 slow path**

那么能否取二者之长，弃二者只短呢？可以的，这种 **杂合的思想** 在 操作系统设计中还是很常见的。
与过去参与的 ACM 之类的算法竞赛所不同的是，这里我们更关心平均性能而不是最坏性能。这体现了两个系统设计的重要定律

> **二八定律**
> 在所有大型系统中大部分的性能，由少数几项关键因素决定。高达80%的效果由20%的关键因素决定。

>**Amdahl 定律**
>假设系统某部分所需执行时间与总时间的比例为$\alpha$，该部分性能提升比例为 $k$， 则总的执行时间为 $T_{new} = (1 -  \alpha) T_{old} + \alpha T_{new}/ k$ ，故加速比为
>简单分析，可得，将一个小部分的性能，即使提升很大的比例，总加速比 $S$ 还是很难提高。
>即使将占比40%的部分加速到 $\alpha T_{new} /k= o (1 -  \alpha) T_{old}$   的程度，也就是和其它部分的耗时相比，所耗时间可以忽略不计，加速比也只有 $S = T_{new}/T_{old} = \frac {1} {1 - \alpha}$
>所以说要对整个系统加速，必须优化系统的绝大部分；未被加速的部分，也就是慢步骤，是大幅削弱了快步骤的加速作用的。
>《CSAPP》 第一章

经过研究统计，需要同步互斥的场景中，真正发生竞争的情况并不多。也就是说，很有可能在上锁之后并没有其他人来和你争抢这把锁。在没有竞争的时候仍然要通过系统调用进入内核检查竞争情况，这实际上是比较浪费的。、

因此两阶段锁做了以下融合：
* 第一阶段：在用户态使用原子操作和共享内存机制来尝试获取锁，如果获取成功，那么立即上锁无须再系统调用进入内核，否则进入第二阶段；（也就是允许在用户态通过原子操作改变锁的状态）
* 第二阶段：如果获取失败则系统调用进入内核然后睡眠挂起。

这样既能避免自旋过多造成的 CPU 资源浪费，也可以避免成功上锁时额外的系统调用等开销，实现了双赢。futex 就是一种典型的两阶段锁



如果还要优化可以考虑下面这些需求

* 公平性
* 锁的释放
* 性能
	*	如果可能避免系统调用，因为系统调用通常会消耗几百条指令。
	*	避免不必要的上下文切换：上下文切换导致与 TLB 失效相关的开销等。
*	快速用户级锁定应该足够简单，以提供有效支持更复杂的同步构造的基础
*	还应该允许对内核的阻塞需求进行清晰的分离，因此只需用一小组不同的构造实现阻塞。





##### 2 原理

考虑到更加方便的管理锁对象，所以futex**直接通过虚拟地址来处理锁**，而不是像以前一样创建一个需要初始化和以及跨进程全局句柄的内核句柄。

2.1 三个关键点

- 我们为每个futex使用一个唯一的标识符（它可以在不同的地址空间中共享，所以每个空间中可能有不同的虚拟地址）：这个标识符是“strcut page”指针和该page中的偏移量
- 该结构体可以表示进程放在哈希表中的futex对象
- 给上层提供一个简单的唯一标识符

2.2 具备的功能：

- 支持一种锁粒度的睡眠与唤醒操作；
- 管理进程挂起时的等待队列。 

2.3具体实现

数据结构

* futex在用户空间只是一个地址对齐的整形--->uaddr。
* 内核中的futex系统设计了三个基本结构。futex_key，futex_q， 以及futex_queues。
	* futex_key：**是一个用户空间中futex的地址+地址映射对象作为键的标识**，调用futex系统调用时，必须传入futex的地址uaddr，同时才能标识出这个futex。它也是futex可以在进程间共享使用的保证。
	* futex_queues：是一个等待队列。
	* futex_q：代表一个任务（进程或线程）在某个futex上的等待关系，链入到futex_queues队列中。



uaddr 和 futex_key 的关系：

* 通过 hash_futex()，用 uaddr 去生成 futex_key 来进行唯一的标识。因为不同的进程可能会使用冲突的虚拟内存地址，所以要哈希一下。
* uaddr 是用户态的，futex_key 是内核态的

![img](https://hardcore.feishu.cn/space/api/box/stream/download/asynccode/?code=NWU1MjUwMjkyNGMxMWUwNDc4MTk0MTEwZTdlZmI3OGVfek5Uc3JtRzlTOFR1RG02WFRJUkF6QzI0RXhBN1VvQ2VfVG9rZW46Ym94Y25JOEJ6YkppdDdFbVBIbUdORlN3MlNnXzE2Njk5NzEzNzY6MTY2OTk3NDk3Nl9WNA)

```c
// 在uaddr指向的这个锁变量上挂起等待（仅当*uaddr==val时）

int futex_wait(int *uaddr, int val);

// 唤醒n个在uaddr指向的锁变量上挂起等待的进程

int futex_wake(int *uaddr, int n);
```

uaddr：就是用户态下共享内存的地址，里面存放的是一个对齐的整型计数器。

uaddr参数的比较：

* 通过支持虚拟地址的内存对象和该对象中的偏移来定义锁标识。我们用元组[B，O]来标记这一点。可以区分三种基本的记忆类型来表示B：（a）匿名内存(/dev/zero)，（b）共享内存段，和（c）内存映射文件。虽然（b）和（c）可以在多个进程之间使用，但（a）只能在同一个进程的线程之间使用。利用锁的虚拟地址作为内核句柄也提供了一种集成的访问机制，可以自动将虚拟地址与其内核对象联系起来。

 

val：FUTEX_WAIT: 原子性的检查uaddr中计数器的值是否为val,如果是则让进程休眠，



n:FUTEX_WAKE: 最多唤醒n个等待在uaddr上进程。



futex变量的值有 2 种：

- 0代表当前锁空闲
- 1代表有线程持有当前锁



**futex_wait函数分析**

```c
static int futex_wait(u32 __user *uaddr, int fshared, u32 val, ktime_t *abs_time, u32 bitset, int clockrt)
{
        //上图中的hash表结构
        struct futex_hash_bucket *hb;
        //锁对象
        struct futex_q q;
        ...
retry:

        //该函数中判断uaddr指向的值是否等于val，以及一些初始化操作
        ret = futex_wait_setup(uaddr, val, fshared, &q, &hb);
        //如果val发生了改变，说明获取锁失败
        if (ret)
                goto out;
 

        //将当前进程状态改为TASK_INTERRUPTIBLE，并插入到futex等待队列，然后重新调度。
        futex_wait_queue_me(hb, &q, to);
        ...
        return ret;
}
```

futex_wait_queue_me

```c
static int futex_wait_setup(u32 __user *uaddr, u32 val, int fshared, struct futex_q *q, struct futex_hash_bucket **hb)
{
        u32 uval;
        int ret;
retry:
        q->key = FUTEX_KEY_INIT;
        //初始化futex_q
        ret = get_futex_key(uaddr, fshared, &q->key, VERIFY_READ);
        if (unlikely(ret != 0))
                return ret;

retry_private:

        //获得自旋锁
        *hb = queue_lock(q);
        //原子的将uaddr的值设置到uval中
        ret = get_futex_value_locked(&uval, uaddr);
   ... 

        //如果当期uaddr指向的值不等于val，即说明其他进程修改了

        //uaddr指向的值，等待条件不再成立，不用阻塞直接返回。
        if (uval != val) {
                //释放锁
                queue_unlock(q, *hb);
                ret = -EWOULDBLOCK;
        }

   ...

        return ret;
}
```

futex_wait_queue_me(写入队列是受spinlock锁保护的，所以线程安全的)

```c
static void futex_wait_queue_me(struct futex_hash_bucket *hb, struct futex_q *q, struct hrtimer_sleeper *timeout)
{
        //设置进程状态为TASK_INTERRUPTIBLE，cpu调度时只会选择

        //状态为TASK_RUNNING的进程

        set_current_state(TASK_INTERRUPTIBLE);

        //将当期进程（q封装）插入到等待队列中去，然后释放自旋锁

        queue_me(q, hb);

        //启动定时任务

        if (timeout) {
                hrtimer_start_expires(&timeout->timer, HRTIMER_MODE_ABS);
                if (!hrtimer_active(&timeout->timer))
                        timeout->task = NULL;
        }
 
        /*

         * If we have been removed from the hash list, then another task

         * has tried to wake us, and we can skip the call to schedule().

         */

        if (likely(!plist_node_empty(&q->list))) {
                 //如果没有设置过期时间 || 设置了过期时间且还没过期
                if (!timeout || timeout->task)
                        //系统重新进行进程调度，这个时候cpu会去执行其他进程，该进程会阻塞在这里
                        schedule();
        }

        //走到这里说明又被cpu选中运行了

        __set_current_state(TASK_RUNNING);
}
```



**futex_wake函数分析**

```c
static int futex_wake(u32 __user *uaddr, int fshared, int nr_wake, u32 bitset)
{
        struct futex_hash_bucket *hb;
        struct futex_q *this, *next;
        struct plist_head *head;
        union futex_key key = FUTEX_KEY_INIT;

        int ret;

        ...

        //根据uaddr的值填充&key的内容

        ret = get_futex_key(uaddr, fshared, &key, VERIFY_READ);

        if (unlikely(ret != 0))
                goto out;

        //根据&key获得对应uaddr所在的futex_hash_bucket

        hb = hash_futex(&key);

        //对该hb加自旋锁

        spin_lock(&hb->lock);

        head = &hb->chain;

        //遍历该hb的链表，注意链表中存储的节点是plist_node类型，而而这里的this却是futex_q类型，这种类型转换是通过c中的container_of机制实现的

        plist_for_each_entry_safe(this, next, head, list) {
                if (match_futex (&this->key, &key)) {
                        ...
                        //唤醒对应进程

                        wake_futex(this);
                        if (++ret >= nr_wake)
                                break;
                }
        }

        //释放自旋锁

        spin_unlock(&hb->lock);

        put_futex_key(fshared, &key);

out:

        return ret;
}
```



##### **3 上层应用**

3.1 Semaphore

| 函数名              | 用途                                                         |
| ------------------- | ------------------------------------------------------------ |
| sem_init(&sem,0,1); | 初始化                                                       |
| sem_wait(&sem);     | 挂起当前进程，直到semaphore的值为非0，它会原子性的减少semaphore计数值。加锁操作 |
| sem_post(&sem);     | 走出临界区，释放semaphore的时候，将其值由0改 1，解锁操作     |

```c
int sem_wait (sem_t *sem)
{
    int *futex = (int *) sem;
    if (atomic_decrement_if_positive (futex) > 0) //如果大于0，那么说明已经拿到锁了
        return 0;
    int   err = lll_futex_wait (futex, 0);
        return -1;
}
int sem_post (sem_t *sem)
{
    int *futex = (int *) sem;
    int nr = atomic_increment_val (futex);
    int err = lll_futex_wake (futex, nr);
    return 0;
}
```

 

```c
#define lll_futex_wait(futex, val) \
({                                          \
    ...
    __asm __volatile (LLL_EBX_LOAD                          \
              LLL_ENTER_KERNEL                          \
              LLL_EBX_LOAD                          \
              : "=a" (__status)                          \
              : "0" (SYS_futex), LLL_EBX_REG (futex), "S" (0),          \
            "c" (FUTEX_WAIT), "d" (_val),                  \
            "i" (offsetof (tcbhead_t, sysinfo))              \
              : "memory");                          \
    ...                                      \
})
```



当线程退出临界区后，执行sem_post(),释放semaphore的时候，将其值由0改 1，并不知道是否有线程阻塞在这个semaphore上，所以只好不管怎么样都执行futex(uaddr, FUTEX_WAKE, 1)尝试着唤醒一个进程。而相反的，当sem_wait()，如果semaphore由1变0，则意味着没有竞争发生，所以不必去执行futex系统调 用。



3.2 mutex

```c
pthread_mutex_trylock
```

futex的值初始化时是0；当调用try_lock的时候会利用cas操作改为1（见trylock函数）；当调用`lll_lock`时，如果不存在锁冲突，则将其改为1，否则改为2。

```c
//pthread_mutex_t是posix中的互斥锁结构体
int
__pthread_mutex_trylock (mutex)
     pthread_mutex_t *mutex;
{
  int oldval;
  pid_t id = THREAD_GETMEM (THREAD_SELF, tid);
switch (__builtin_expect (PTHREAD_MUTEX_TYPE (mutex),
                            PTHREAD_MUTEX_TIMED_NP))
    {
    
    case PTHREAD_MUTEX_ERRORCHECK_NP:
    case PTHREAD_MUTEX_TIMED_NP:
    case PTHREAD_MUTEX_ADAPTIVE_NP:
      /* Normal mutex.  */
      if (lll_trylock (mutex->__data.__lock) != 0)
        break;

      /* Record the ownership.  */
      mutex->__data.__owner = id;
      ++mutex->__data.__nusers;

      return 0;
    }
    
} 
 //以下代码在lowlevellock.h中  
   #define __lll_trylock(futex) \
  (atomic_compare_and_exchange_val_acq (futex, 1, 0) != 0)
  #define lll_trylock(futex) __lll_trylock (&(futex))
#define __lll_lock(futex, private)                                              \
  ((void) ({                                                                      \
    int *__futex = (futex);                                                      \
    if (__builtin_expect (atomic_compare_and_exchange_bool_acq (__futex,      \
                                                                1, 0), 0))    \
      {                                                                              \
        if (__builtin_constant_p (private) && (private) == LLL_PRIVATE)              \
          __lll_lock_wait_private (__futex);                                      \
        else                                                                      \
          __lll_lock_wait (__futex, private);                                      \
      }                                                                              \
  }))
#define lll_lock(futex, private) __lll_lock (&(futex), private)

void
__lll_lock_wait_private (int *futex)
{
//第一次进来的时候futex==1,所以不会走这个if
  if (*futex == 2)
    lll_futex_wait (futex, 2, LLL_PRIVATE);
//在这里会把futex设置成2，并调用futex_wait让当前线程等待
  while (atomic_exchange_acq (futex, 2) != 0)
    lll_futex_wait (futex, 2, LLL_PRIVATE);
}
```



```c
pthread_mutex_unlock
int
internal_function attribute_hidden
__pthread_mutex_unlock_usercnt (mutex, decr)
     pthread_mutex_t *mutex;
     int decr;
{
    if (__builtin_expect (type, PTHREAD_MUTEX_TIMED_NP)
      == PTHREAD_MUTEX_TIMED_NP)
    {
      /* Always reset the owner field.  */
    normal:
      mutex->__data.__owner = 0;
      if (decr)
        /* One less user.  */
        --mutex->__data.__nusers;

      /* Unlock.  */
      lll_unlock (mutex->__data.__lock, PTHREAD_MUTEX_PSHARED (mutex));
      return 0;
    }
 }
 
 
```

#### 谈谈 Linux RCU



### 条件变量

#### 什么是条件变量

**条件变量是线程的另外一种同步机制**，这些同步对象为线程提供了会合的场所，理解起来就是两个（或者多个）线程需要碰头（或者说进行交互-一个线程给另外的一个或者多个线程发送消息），**我们指定在条件变量这个地方发生，一个线程用于修改这个变量使其满足其它线程继续往下执行的条件，其它线程则接收条件已经发生改变的信号。**

条件变量同锁一起使用使得线程可以以一种**无竞争**的方式等待任意条件的发生。所谓无竞争就是，条件改变这个信号会发送到所有等待这个信号的线程。而不是说一个线程接受到这个消息而其它线程就接收不到了。

**条件变量的优点：**

相较于mutex而言，条件变量可以减少竞争。如直接使用mutex，除了生产者、消费者之间要竞争互斥量以外，消费者之间也需要竞争互斥量，但如果汇聚（链表）中没有数据，消费者之间竞争互斥锁是无意义的。有了条件变量机制以后，只有生产者完成生产，才会引起消费者之间的竞争。提高了程序效率。

具体的函数介绍就不说了，详细参考APUE

* pthread_cond_init()函数               功能：初始化一个条件变量
* pthread_cond_wait()函数             功能：阻塞等待一个条件变量
* pthread_cond_timedwait()函数    功能：限时等待一个条件变量
* pthread_cond_signal()函数          功能：唤醒至少一个阻塞在条件变量上的线程
* pthread_cond_broadcast()函数    功能：唤醒全部阻塞在条件变量上的线程
* pthread_cond_destroy()函数        功能：销毁一个条件变量

以上6 个函数的返回值都是：成功返回0， 失败直接返回错误号。

pthread_cond_t 类型，其本质是一个结构体。为简化理解，应用时可忽略其实现细节，简单当成整数看待。如：

pthread_cond_t  cond; 变量cond只有两种取值1、0。

下面通过一个例子来详细说一下正确使用条件变量的方法。下例实现了生产者和消费者模型，生产者向队列中插入数据，消费者则在生产者发出队列准备好（有数据了）后接收消息，然后取出数据进行处理。实现的关键点在以下几个方面：

* 生产者和消费者都对条件变量的使用加了锁
* 消费者调用pthread_cond_wait,等待队列是否准备好的信息，注意参数有两个，一个是pthread_cond_t，另外一个是pthread_mutex_t.

代码：

```c++

#include <pthread.h>
struct msg {
struct msg *m_next;
/* ... more stuff here ... */
};
struct msg *workq;
pthread_cond_t qready = PTHREAD_COND_INITIALIZER;
pthread_mutex_t qlock = PTHREAD_MUTEX_INITIALIZER;
void
process_msg(void) // 消费者
{
 struct msg *mp;
 for (;;) {
        pthread_mutex_lock(&qlock);
        while (workq == NULL)
            pthread_cond_wait(&qready, &qlock);
        mp = workq;
        workq = mp->m_next;
        pthread_mutex_unlock(&qlock);
        /* now process the message mp */
 }
}
void
enqueue_msg(struct msg *mp) // 生产者
{
 pthread_mutex_lock(&qlock);
 mp->m_next = workq;
 workq = mp;
 pthread_cond_signal(&qready);
    pthread_mutex_unlock(&qlock);
}

```

#### 为什么 pthread_cond_wait 需要加锁？

pthread_cond_wait(cond, mutex)的功能有3个：

* 调用者线程首先释放mutex
* 然后阻塞，等待被别的线程唤醒
* 当调用者线程被唤醒后，调用者线程会再次获取mutex

pthread_cond_wait(cond)的功能只有1个：

* 调用者线程阻塞，等待被别的线程唤醒。

这里首先给一个简洁的回答：

* **通常的应用场景下，当前线程执行pthread_cond_wait时，处于临界区访问共享资源，存在一个mutex与该临界区相关联，这是理解pthread_cond_wait带有mutex参数的关键**
* 当前线程执行pthread_cond_wait前，已经获得了和临界区相关联的mutex；执行pthread_cond_wait会阻塞，但是在进入阻塞状态前，必须释放已经获得的mutex，让其它线程能够进入临界区
* 当前线程执行pthread_cond_wait后，阻塞等待的条件满足，条件满足时会被唤醒；被唤醒后，仍然处于临界区，因此被唤醒后必须再次获得和临界区相关联的mutex

综上，**调用pthread_cond_wait时，线程总是位于某个临界区，该临界区与mutex相关，pthread_cond_wait需要带有一个参数mutex，用于释放和再次获取mutex。**

本文的剩下部分将通过一个具体的应用场景来说明，为什么pthread_cond_wait需要一个看似多余的mutex参数。

#### 在生产者线程中修改条件时为什么要加 mutex？

**防止因为缺少锁造成唤醒丢失**

如果不这么做信号可能会丢失，看下面的例子：

```lisp
Thead A（消费者）                    Thread B（生产者）

pthread_mutex_lock(&qlock);
while (workq == NULL)
                                   mp->m_next = workq;
                                workq = mp;
                                   pthread_cond_signal(&cond);

pthread_cond_wait(&qready, &qlock);
```

在while判断之后向队列中插入数据，虽然已经有数据了，但线程A还是调用了pthread_cond_wait等待下一个信号到来。

#### 消费者线程中判断条件为什么要放在 while 中？

**防止虚假唤醒**

```lisp
while (workq == NULL)
    pthread_cond_wait(&qready, &qlock);
mp = workq;  
```

我们把while换成if可不可以呢？

```lisp
if (workq == NULL)
    pthread_cond_wait(&qready, &qlock);
mp = workq; 
```

答案是不可以，一个生产者可能对应着多个消费者，生产者向队列中插入一条数据之后发出signal，然后各个消费者线程的pthread_cond_wait获取mutex后返回，当然，这里只有一个线程获取到了mutex，然后进行处理，其它线程会pending在这里，处理线程处理完毕之后释放mutex，刚才等待的线程中有一个获取mutex，如果这里用if，就会在当前队列为空的状态下继续往下处理，这显然是不合理的。

#### signal 到底是放在 unlock 之前还是之后？

```cpp
void
enqueue_msg(struct msg *mp)
{
 pthread_mutex_lock(&qlock);
 mp->m_next = workq;
 workq = mp;
 pthread_mutex_unlock(&qlock);
 pthread_cond_signal(&qready);
}
```

如果先unlock，再signal,如果这时候有一个消费者线程恰好获取mutex，然后进入条件判断，这里就会判断成功，从而跳过pthread_cond_wait,下面的signal就会不起作用；另外一种情况，一个优先级更低的不需要条件判断的线程正好也需要这个mutex，这时候就会转去执行这个优先级低的线程，就违背了设计的初衷。

```cpp
void
enqueue_msg(struct msg *mp)
{
 pthread_mutex_lock(&qlock);
 mp->m_next = workq;
 workq = mp;
 pthread_cond_signal(&qready);
 pthread_mutex_unlock(&qlock);
}
```

如果把signal放在unlock之前，消费者线程会被唤醒，获取mutex发现获取不到，就又去sleep了。浪费了资源.但是在LinuxThreads或者NPTL里面，就不会有这个问题，因为在Linux 线程中，有两个队列，分别是cond_wait队列和mutex_lock队列，  cond_signal只是让线程从cond_wait队列移到mutex_lock队列，而不用返回到用户空间，不会有性能的损耗。所以在Linux中推荐使用这种模式。

#### 条件变量实现机制？

<https://blog.csdn.net/THEANARKH/article/details/117160793>

条件变量是线程间同步的一种机制，本文分析条件变量的实现和使用。我们先看一下条件变量的定义。

```c
typedef struct
{
  int c_spinlock;                  /* Spin lock to protect the queue.  */
  struct _pthread_queue c_waiting; /* Threads waiting on this condition.  */
} pthread_cond_t;

```

条件变量就是在条件不满足的时候，把线程插入等待队列，等待条件满足的时候再唤醒队列里的线程。我们看一下具体实现

`pthread_cond_wait`

```c
// 阻塞等待条件。进入该函数前，已经获得了互斥锁mutex
int pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex)
{
  volatile pthread_t self = thread_self();
  // 加锁操作队列
  acquire(&cond->c_spinlock);
  // 插入条件的等待队列
  enqueue(&cond->c_waiting, self);
  // 操作完释放锁
  release(&cond->c_spinlock);
  // 释放互斥变量，否则别人无法操作资源，导致条件一直无法满足
  pthread_mutex_unlock(mutex);
  // 挂起等待条件满足后被唤醒
  suspend_with_cancellation(self);
  // 被唤醒后重新获取互斥锁
  pthread_mutex_lock(mutex);
  /* This is a cancellation point */
  // 取消点，等待期间被取消了
  if (self->p_canceled && self->p_cancelstate == PTHREAD_CANCEL_ENABLE) {
    /* Remove ourselves from the waiting queue if we're still on it */
    acquire(&cond->c_spinlock);
    // 线程准备退出，从条件阻塞队列中移除
    remove_from_queue(&cond->c_waiting, self);
    release(&cond->c_spinlock);
    pthread_exit(PTHREAD_CANCELED);
  }
  return 0;
}

```

1. 插入条件等待队列
2. 释放互斥量
3. 挂起线程等待条件满足后被释放
4. 被唤醒后重新获得互斥锁
5. 取消点，等待期间被取消了，线程准备退出从阻塞队列中移除

`suspend_with_cancellation` 挂起逻辑

```c
static inline void suspend_with_cancellation(pthread_t self)
{
  sigset_t mask;
  sigjmp_buf jmpbuf;
  // 获取当前的信号屏蔽码
  sigprocmask(SIG_SETMASK, NULL, &mask); /* Get current signal mask */
  // 清除PTHREAD_SIG_RESTART的信号掩码，即允许处理该信号
  sigdelset(&mask, PTHREAD_SIG_RESTART); /* Unblock the restart signal */
  /* No need to save the signal mask, we'll restore it ourselves */
  /*
    直接调用返回0，从siglongjump回来返回非0,这里支持线程挂起时，
    收到restart信号被唤醒，或者在取消信号的处理函数中，通过siglongjmp返回这里
  */
  if (sigsetjmp(jmpbuf, 0) == 0) {
    self->p_cancel_jmp = &jmpbuf;
    // 已经被取消并且是可取消的则直接返回，否则挂起等待唤醒
    if (! (self->p_canceled && self->p_cancelstate == PTHREAD_CANCEL_ENABLE)) {
      do {
        // 挂起等待restart信号
        sigsuspend(&mask);               /* Wait for a signal */
      } while (self->p_signal != PTHREAD_SIG_RESTART);
    }
    self->p_cancel_jmp = NULL;
  } else {
    // 从cancel信号的处理函数中的siglongjmp返回，重新设置信号掩码,屏蔽restart信号
    sigaddset(&mask, PTHREAD_SIG_RESTART); /* Reblock the restart signal */
    sigprocmask(SIG_SETMASK, &mask, NULL);
  }
}

```

1. 获取信号屏蔽码
2. 清除PTHREAD_SIG_RESTART的信号掩码，即允许处理该信号
3. 直接调用返回0，从siglongjump回来返回非0,这里支持线程挂起时，收到restart信号被唤醒，或者在取消信号的处理函数中，通过siglongjmp返回

`pthread_cond_signal` ：从阻塞队列中获取一个线程，然后给他发一个唤醒信号

```c
// 条件满足，唤醒线程
int pthread_cond_signal(pthread_cond_t *cond)
{
  pthread_t th;

  acquire(&cond->c_spinlock);
  // 取出一个被被阻塞的线程
  th = dequeue(&cond->c_waiting);
  release(&cond->c_spinlock);
  // 发送信号唤醒他
  if (th != NULL) restart(th);
  return 0;
}

// 给pid进程发送唤醒信号
static inline void restart(pthread_t th)
{
  kill(th->p_pid, PTHREAD_SIG_RESTART);
}
```

`pthread_cond_broadcast`：给每一个等待的线程发送唤醒信号

```c
// 条件满足，唤醒所有线程
int pthread_cond_broadcast(pthread_cond_t *cond)
{
  pthread_queue tosignal;
  pthread_t th;

  acquire(&cond->c_spinlock);
  /* Copy the current state of the waiting queue and empty it */
  tosignal = cond->c_waiting;
  // 重置阻塞队列
  queue_init(&cond->c_waiting);
  release(&cond->c_spinlock);
  /* Now signal each process in the queue */
  // 发送信号唤醒所有线程
  while ((th = dequeue(&tosignal)) != NULL) restart(th);
  return 0;
}

```

使用例子。

```c
struct prodcons {
  int buffer[BUFFER_SIZE];      /* 环形数据缓冲区 */
  pthread_mutex_t lock;         /* 访问数据区的互斥锁 */
  int readpos, writepos;        /* 读写指针 */
  pthread_cond_t notempty;      /* 消费者使用的条件变量，非空即有数据消费 */
  pthread_cond_t notfull;       /* 生产者使用的条件变量，非满即可以生产数据 */
};

struct prodcons buffer;

void init(struct prodcons * b)
{
  pthread_mutex_init(&b->lock, NULL);
  pthread_cond_init(&b->notempty, NULL);
  pthread_cond_init(&b->notfull, NULL);
  b->readpos = 0;
  b->writepos = 0;
}

int main()
{
  pthread_t th_a, th_b;
  void * retval;
  // 初始化线程间共享的数据结构
  init(&buffer);
  // 创建两个线程
  pthread_create(&th_a, NULL, producer, 0);
  pthread_create(&th_b, NULL, consumer, 0);
  pthread_join(th_a, &retval);
  pthread_join(th_b, &retval);
  return 0;
}

void * producer(void * data)
{
  int n;
  for (n = 0; n < 10000; n++) {
    printf("%d --->\n", n);
    put(&buffer, n);
  }
  put(&buffer, OVER);
  return NULL;
}

void * consumer(void * data)
{
  int d;
  while (1) {
    d = get(&buffer);
    if (d == OVER) break;
    printf("---> %d\n", d);
  }
  return NULL;
}

void put(struct prodcons * b, int data)
{
  // 操作共享数据需要加锁
  pthread_mutex_lock(&b->lock);
  /* 写指针+1等于读指针，说明没有空闲可写了，等待空闲空间 */
  while ((b->writepos + 1) % BUFFER_SIZE == b->readpos) {
    pthread_cond_wait(&b->notfull, &b->lock);
  }
  // pthread_cond_wait中被唤醒后会重新获得互斥锁，所以这里直接操作就行
  b->buffer[b->writepos] = data;
  b->writepos++;
  // 到尾巴了，修正位置
  if (b->writepos >= BUFFER_SIZE) b->writepos = 0;
  /* 有数据可消费了，通知等待的消费者 */
  pthread_cond_signal(&b->notempty);
  pthread_mutex_unlock(&b->lock);
}


int get(struct prodcons * b)
{
  int data;
  pthread_mutex_lock(&b->lock);
  /* 读写指针相等说明没有数据读了，等待数据 */
  while (b->writepos == b->readpos) {
    pthread_cond_wait(&b->notempty, &b->lock);
  }
  data = b->buffer[b->readpos];
  b->readpos++;
  if (b->readpos >= BUFFER_SIZE) b->readpos = 0;
  /* 消费了数据，说明有空闲空间了，唤醒生产者 */
  pthread_cond_signal(&b->notfull);
  pthread_mutex_unlock(&b->lock);
  return data;
}

```

#### 什么是唤醒丢失问题？

<https://blog.csdn.net/qq_39354847/article/details/126432944>

唤醒丢失：唤醒信号是一次性的且存在丢失可能，要根据实际情况决定是否等待

**唤醒丢失情况1：缺少条件**

```c
std::mutex mutex;
std::condition_variable cv;
std::vector<int> vec;

void Consume() {
  std::unique_lock<std::mutex> lock(mutex);
  cv.wait(lock);
  std::cout << "consume " << vec.size() << "\n";
}

void Produce() {
  std::unique_lock<std::mutex> lock(mutex);
  vec.push_back(1);
  cv.notify_all();
  std::cout << "produce \n";
}

int main() {
  std::thread t(Consume);
  t.detach();
  Produce();
  return 0;
}
```

如果先执行的Produce()，后执行的Consume()，生产者提前生产出了数据，去通知消费者，但是此时消费者线程如果还没有执行到wait语句，即线程还没有处于挂起等待状态，线程没有等待此条件变量上，那通知的信号就丢失了，后面Consume()中才执行wait处于等待状态，但此时生产者已经不会再触发notify，那消费者线程就会始终阻塞下去，出现bug。

如果先执行的Produce()，后执行的Consume()，生产者提前生产出了数据，去通知消费者，但是此时消费者线程如果还没有执行到wait语句，即线程还没有处于挂起等待状态，线程没有等待此条件变量上，那通知的信号就丢失了，后面Consume()中才执行wait处于等待状态，但此时生产者已经不会再触发notify，那消费者线程就会始终阻塞下去，出现bug。
---- 程序喵大人

**唤醒信号是一次性的且存在丢失可能，因此我们需要根据实际情况决定是否要等待（就是根据条件判断对方有没有准备好等待接受唤醒信号了，如果准备好了你再发送）**

* 如果先执行的Produce()，后执行的Consume()，队列中已经有东西了，那么就不用等待了
* 否则，队列开始的那么还是要等待

因此我们需要给等待加上条件（下面的代码是有问题的！！！）

```c++
std::mutex mutex;
std::condition_variable cv;
std::vector<int> vec;

void Consume() {
  std::unique_lock<std::mutex> lock(mutex);
  if (vec.empty()) { // 加入此判断条件，但这样虚假唤醒的问题！！！
      cv.wait(lock);
  }
  std::cout << "consume " << vec.size() << "\n";
}

void Produce() {
  std::unique_lock<std::mutex> lock(mutex);
  vec.push_back(1);
  cv.notify_all();
  std::cout << "produce \n";
}

int main() {
  std::thread t(Consume);
  t.detach();
  Produce();
  return 0;
}

```

这样就能解决信号丢失的问题，但是使用 `if`来进行条件判断虚假唤醒的问题，这个在后面虚假唤醒的部分解决。**建议暂时跳过`唤醒丢失情况 2：没有搭配锁` 这部分内容，先去看`虚假唤醒`这部分内容**

**唤醒丢失情况2：没有搭配锁**

虽然可能设置了条件，但你根据条件决定等待的前一刻，条件变了（因为没有锁），导致等了个寂寞。

```c++
class Foo {
    mutex mtx;
    condition_variable cv;
    int k = 0;
  
public:
    Foo() {
    }

    void first(function<void()> printFirst) {
        // printFirst() outputs "first". Do not change or remove this line.
        lock_guard<mutex> lock(mtx);
        printFirst();
        k = 1;
        cv.notify_all();
    }

    void second(function<void()> printSecond) {
    
        // printSecond() outputs "second". Do not change or remove this line.
        unique_lock<mutex> lock(mtx);
        cv.wait(lock, [this](){ return k == 1;});
        printSecond();
        k = 2;
        cv.notify_one();
        
    }

    void third(function<void()> printThird) {
        
        // printThird() outputs "third". Do not change or remove this line.
        unique_lock<mutex> lock(mtx);
        cv.wait(lock, [this](){ return k == 2;});
        printThird();
    }
};

```

上面是力扣多线程题目 1114. 按序打印 的一片题解的条件变量解法部分，评论区中有人指出这个做法存在问题。
考虑下面的情况

![在这里插入图片描述](https://img-blog.csdnimg.cn/f2c22f74dd6f406e858ac814e00c1008.png)

由于在修改 k 的共享内存的时候没有加锁，导致线程 2 检查条件 k == 1 发现结果为 false 然后决定等待的过程中 k 的值发生改变。条件变量开始等待之前 k = 1（检查条件失效） 并且唤醒信号已经被发送（唤醒丢失）

```c++
class Foo {
    mutex mtx;
    condition_variable cv;
    int k = 0;
  
public:
    Foo() {
    }

    void first(function<void()> printFirst) {
        // printFirst() outputs "first". Do not change or remove this line.
        lock_guard<mutex> lock(mtx);
        printFirst();
        k = 1;
        cv.notify_all();
    }

    void second(function<void()> printSecond) {
    
        // printSecond() outputs "second". Do not change or remove this line.
        unique_lock<mutex> lock(mtx);
        cv.wait(lock, [this](){ return k == 1;});
        printSecond();
        k = 2;
        cv.notify_one();
        
    }

    void third(function<void()> printThird) {
        
        // printThird() outputs "third". Do not change or remove this line.
        unique_lock<mutex> lock(mtx);
        cv.wait(lock, [this](){ return k == 2;});
        printThird();
    }
};

```

#### 什么是虚假唤醒？

**什么是虚假唤醒**

**当线程从等待已发出信号的条件变量中醒来，却发现它等待的条件未得到满足时，就会发生虚假唤醒。之所以称为虚假，是因为该线程似乎无缘无故地被唤醒了。**但是虚假唤醒不会无缘无故发生：它们通常是因为在发出条件变量信号和等待线程最终运行之间，另一个线程运行并更改了条件。线程之间存在竞争条件，典型的结果是有时，在条件变量上唤醒的线程首先运行，赢得竞争，有时它运行第二，失去竞争。
在许多系统上，尤其是多处理器系统上，虚假唤醒的问题更加严重，因为如果有多个线程在条件变量发出信号时等待它，系统可能会决定将它们全部唤醒，将每个signal( )唤醒一个线程视为broadcast( )唤醒所有这些，从而打破了信号和唤醒之间任何可能预期的 1:1 关系。如果有 10 个线程在等待，那么只有一个会获胜，另外 9 个会经历虚假唤醒。
为了让实现在处理操作系统内部的错误条件和竞争时具有灵活性，即使没有发出信号，也可以允许条件变量从等待中返回，尽管目前尚不清楚有多少实现实际上这样做了。在条件变量的 Solaris 实现中，如果进程发出信号，则可能发生虚假唤醒而没有发出条件信号；等待系统调用中止并返回EINTR。条件变量的 Linux p-thread 实现保证它不会那样做。
因为只要有竞争甚至可能在没有竞争或信号的情况下都可能发生虚假唤醒，因此当线程在条件变量上唤醒时，它应该始终检查它所寻求的条件是否得到满足。如果不是，它应该回到条件变量上睡觉，等待另一个机会。

**虚假唤醒情况 1：notify_one 但是多线程争抢**

```c++
std::condition_variable cv;
std::mutex mx;

void thread1()
{
    while (true) {
        // do some work ...
        std::unique_lock<std::mutex> lock(mx);
        cv.notify_one();    // wake other thread
    }
}

void thread2()
{
    while (true) {
        std::unique_lock<std::mutex> lock(mx);
        cv.wait(lock);    // might block forever
        // do work ...
    }
}
```

在这里，如果有其他thread消费者thread1的通知，thread2会永久等待

**虚假唤醒情况 2 ： 系统原因**

有些操作系统为了在处理内部的错误条件和竞争时具有灵活性，即使没有发出信号，也可以允许条件变量从等待中返回。因此下面的代码在某些操作系统会存在问题（也就是唤醒丢失情况 1 当中的改进代码)

> linux 系统提供的 pthread 保证不会发生这种情况的虚假唤醒

```c++
void Consume() {
  std::unique_lock<std::mutex> lock(mutex);
  if (vec.empty()) { // 加入此判断条件，但这样虚假唤醒的问题！！！
      cv.wait(lock);
  }
  std::cout << "consume " << vec.size() << "\n";
}

void Produce() {
  std::unique_lock<std::mutex> lock(mutex);
  vec.push_back(1);
  cv.notify_all();
  std::cout << "produce \n";
}

```

我们应当使用 while 而不是 if 来判断条件

```c++
void Consume() {
  std::unique_lock<std::mutex> lock(mutex);
  while (vec.empty()) { // 使用 while 来判断条件
      cv.wait(lock);
  }
  std::cout << "consume " << vec.size() << "\n";
}
```

对于 C++ 我们可以直接将条件通过 lambda 的方式传递给条件变量，C++ 内部会自动使用 while进行判断

```c++
 while (vec.empty()) { // 使用 while 来判断条件
     cv.wait(lock);
 }
// 和下面的等效
cv.wait(lock, [](){ return vec.empty();} );
```

### 信号量

#### 记录型和整型信号量的区别与使用方法？

信号量：信号量是基于软件互斥或硬件互斥方法实现的一种用于同步和互斥的机制。信号量只有两种操作原语：wait, signal

**一.整型信号量**
整型信号量用于描述临界资源的个数。

```c
s=10; 表示一个初始资源数量为10的信号量
wait(s) {
 while (s <= 0);
 s --;
}

signal(s) {
 s ++;
}
```

缺点：违背“让权等待”的同步原则，由于当信号量所表示的资源数目<=0时，而此时该进程的时间片还未用完，便会不断运行while(s<=0)，从而造成cpu资源的浪费，违背“让权等待”的原则。

**二.记录型信号量**
为了解决整型信号量中在wait原语中违背“让权等待”的原则的问题，记录型信号量提出新的想法，设置一个阻塞队列，当s.value<=0时，便将改成挂到阻塞队列队尾，以免造成对cpu时间的浪费。
记录型信号量是一个结构题，包含对临界资源数量的描述以及阻塞队列。

```c
typedef struct semaphore {
 int value; // 描述临界资源的数量
 queue<process*> blockQue; // 阻塞队列
}sem;


void wait(sem s) {
 s.value --;
 if (s.value < 0) {
  blockQue.push(this process);
  block(blockQue);
  调用block原语，进行自我阻塞，放弃CPU的使用权
 }
}

void signal(sem s) {
 s.value ++;
 if (s.value <= 0) {
  process* head=blockQue.front();blcokQue.pop();
  wakeup(head);
  使用wakeup原语唤醒进程   阻塞态->就绪态
 }
}
```

**三.用法**

当用于同步时，信号量的初始值设置为0。同步是为了规定不同进程的执行的先后顺序，在实际开发中，可能会有两个进程需要相互合作完成某项任务，比如，前者执行的结果是后者的初始值。

当用于互斥时，信号量的初始值设置为1。当信号量的初始值为1时，表示临界资源的格式为1个，当不同进程使用临界资源时，需要互斥的使用。

#### 信号量机制怎么实现的？

**依靠原子操作和中断屏蔽**

为了克服忙等待需要，可以这样修改信号量操作 wait() 和 signal() 的定义：当一个进程执行操作 wait() 并且发现信号量值不为正时，它必须等待。然而，该进程不是忙等待而是阻塞自己。阻塞操作将一个进程放到与信号量相关的等待队列中，并且将该进程状态切换成等待状态。然后，控制转到 CPU 调度程序，以便选择执行另一个进程。

等待信号量 S 而阻塞的进程，在其他进程执行操作 signal() 后，应被重新执行。进程的重新执行是通过操作 wakeup() 来进行的，它将进程从等待状态改为就绪状态。然而，进程被添加到就绪队列。（取决于 CPU 调度算法，CPU 可能会也可能不会从正在运行的进程切换到新的就绪进程。）

为了实现这样定义的信号量，我们按如下定义信号量：

```c
typedef struct {
   int value;
   struct process *list;
} semaphore;
```

每个信号量都有一个整数 value 和一个进程链表 list。当一个进程必须等待信号量时，就被添加到进程链表。操作 signal() 从等待、进程链表上取走一个进程，并加以唤醒。

现在，信号量操作 wait() 可以定义如下：

```c
wait(semaphore *S) {
 S->value--;
 if (S->value < 0) {
        add this process to S->list;
        block();
 }
}
```

而信号量操作 signal() 可定义如下：

```c
signal(semaphore *S) {
 S->value++;
 if (S->value <= 0) {
       remove a process P from S->list;
       wakeup(P);
   }
}
```

操作 block() 挂起调用它的进程。操作 wakeup(P) 重新启动阻塞进程 P 的执行。这两个操作都是由操作系统作为基本系统调用来提供的。

注意，这样实现的信号量的值可以是负数，而在具有忙等待的信号量经典定义下，信号量的值不能为负。如果信号量的值为负，那么它的绝对值就是等待它的进程数。出现这种情况源于，在实现操作 wait() 时互换了递减和测试的顺序。

通过每个进程控制块 PCB 的一个链接字段，等待进程的链表可以轻松实现。每个信号量包括一个整数和一个 PCB 链表指针。向链表中增加和删除进程以便确保有限等待的一种方法采用 FIFO 队列，这里的信号量包括队列的首指针和尾指针。然而，一般来说，链表可以使用任何排队策略。信号量的正确使用不依赖于信号量链表的特定排队策略。

**关键的是，信号量操作应原子执行。我们应保证：对同一信号量，没有两个进程可以同时执行操作 wait() 和 signal()。这是一个临界区问题。**

**对于单处理器环境，在执行操作 wait() 和 signal() 时，可以简单禁止中断**。这种方案在单处理器环境下能工作，这是因为一旦中断被禁用，不同进程指令不会交织在一起。只有当前运行进程一直执行，直到中断 被重新启用并且调度程序重新获得控制。

对于多处理器环境，每个处理器的中断都应被禁止；否则，在不同处理器上不同的运行进程可能会以任意不同方式一起交织执行。**每个处理器中断的禁止会很困难，也会严重影响性能。因此，SMP 系统应提供其他加锁技术，如 compare_and__swap() 或自旋锁，以确保 wait() 与 signal() 原子执行**。

重要的是，对于这里定义的操作 wait() 和 signal()，我们并没有完全取消忙等待。我们只是将忙等待从进入区移到临界区。此外，我们将忙等待限制在操作 wait() 和 signal() 的临界区内，这些区比较短（如经合理编码，它们不会超过 10 条指令）。因此，临界区几乎不被占用，忙等待很少发生，而且所需时间很短。对于应用程序，存在一种完全不同的情况，即临界区可能很长（数分钟或数小时）或几乎总是被占用。在这种情况下，忙等待极为低效。

#### 信号量与互斥量的区别

信号量同时具备同步互斥的功能，互斥量只有互斥功能。

* 互斥当我占有使用权的时候别人不能进入，独占式访问某段程序和内存。
* 同步就是**调度线程**，即一些线程生产一些线程消费，让生产和消费线程保持合理执行顺序。因为semaphore本意是信号灯，含量了通知的意味，只要是我的信号量值大于等于1，那么就可以有线程或者进程来使用。

 『同步』这个词也可以拆开看，一侧是等待数据的『事件』或者『通知』，一侧是保护数据的 『临界区』，所以同步也即**同步+互斥**。信号量可以满足这两个功能，但是可以注意到两个功能的应用场景还是蛮大的，有 **do one thing and do it best** 的空间。linux 内核曾将 semaphore 作为同步原语，后面代码变得较难维护，刷了一把 mutex 变简单了不少还变快了，需要『通知』 的场景则替换为了 completion variable。



### 原子操作

#### 原子操作的是如何实现的？

**处理器使用基于对缓存加锁或总线加锁的方式来实现多处理器之间的原子操作。**首先处理器会自动保证基本的内存操作的原子性。处理器保证从系统内存中读取或者写入一个字节是原子的，意思是当一个处理器读取一个字节时，其他处理器不能访问这个字节的内存地址。Pentium 6和最新的处理器能自动保证单处理器对同一个缓存行里进行16/32/64位的操作是原子的，但是复杂的内存操作处理器是不能自动保证其原子性的，比如跨总线宽度、跨多个缓存行和跨页表的访问。但是，处理器提供总线锁定和缓存锁定两个机制来保证复杂内存操作的原子性。

（1）使用总线锁保证原子性 第一个机制是通过总线锁保证原子性。如果多个处理器同时对共享变量进行读改写操作（i++就是经典的读改写操作），那么共享变量就会被多个处理器同时进行操作，这样读改写操作就不是原子的，操作完之后共享变量的值会和期望的不一致。举个例子，如果i=1，我们进行两次i++操作，我们期望的结果是3，但是有可能结果是2，如图下图所示。

```plaintext
CPU1    CPU2
 i=1     i=1
 i+1     i+1
 i=2     i=2
```

原因可能是多个处理器同时从各自的缓存中读取变量i，分别进行加1操作，然后分别写入系统内存中。那么，想要保证读改写共享变量的操作是原子的，就必须保证CPU1读改写共享变量的时候，CPU2不能操作缓存了该共享变量内存地址的缓存。

处理器使用总线锁就是来解决这个问题的。**所谓总线锁就是使用处理器提供的一个LOCK信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞住，那么该处理器可以独占共享内存。**

（2）使用缓存锁保证原子性 第二个机制是通过缓存锁定来保证原子性。**在同一时刻，我们只需保证对某个内存地址的操作是原子性即可，但总线锁定把CPU和内存之间的通信锁住了，这使得锁定期间，其他处理器不能操作其他内存地址的数据，所以总线锁定的开销比较大，目前处理器在某些场合下使用缓存锁定代替总线锁定来进行优化。**

频繁使用的内存会缓存在处理器的L1、L2和L3高速缓存里，那么原子操作就可以直接在处理器内部缓存中进行，并不需要声明总线锁，在Pentium 6和目前的处理器中可以使用“缓存锁定”的方式来实现复杂的原子性。

所谓“缓存锁定”是指内存区域如果被缓存在处理器的缓存行中，并且在Lock操作期间被锁定，那么当它执行锁操作回写到内存时，处理器不在总线上声言LOCK＃信号，而是修改内部的内存地址，并允许它的缓存一致性机制来保证操作的原子性，因为**缓存一致性机制会阻止同时修改由两个以上处理器缓存的内存区域数据，当其他处理器回写已被锁定的缓存行的数据时，会使缓存行无效，在如上图所示的例子中，当CPU1修改缓存行中的i时使用了缓存锁定，那么CPU2就不能使用同时缓存i的缓存行。**

但是有两种情况下处理器不会使用缓存锁定。 第一种情况是：当操作的数据不能被缓存在处理器内部，或操作的数据跨多个缓存行（cache line）时，则处理器会调用总线锁定。 第二种情况是：有些处理器不支持缓存锁定。对于Intel 486和Pentium处理器，就算锁定的内存区域在处理器的缓存行中也会调用总线锁定。

### 经典问题

#### 谈谈生产者-消费者模式？

<https://blog.csdn.net/xiaoqiu_cr/article/details/95756625#t5>

<https://blog.csdn.net/JMW1407/article/details/108487490#t0>

**什么是生产者-消费者模式**

比如有两个进程A和B，它们共享一个固定大小的缓冲区，A进程产生数据放入缓冲区，B进程从缓冲区中取出数据进行计算，那么这里其实就是一个生产者和消费者的模式，A相当于生产者，B相当于消费者

![在这里插入图片描述](https://img-blog.csdnimg.cn/2019071315322087.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9xaXVfY3I=,size_16,color_FFFFFF,t_70)

**为什么要使用生产者消费者模式**

在多线程开发中，如果生产者生产数据的速度很快，而消费者消费数据的速度很慢，那么生产者就必须等待消费者消费完了数据才能够继续生产数据，因为生产那么多也没有地方放啊；同理如果消费者的速度大于生产者那么消费者就会经常处理等待状态，所以为了达到生产者和消费者生产数据和消费数据之间的平衡，那么就需要一个缓冲区用来存储生产者生产的数据，所以就引入了生产者-消费者模式

简单来说这里的**缓冲区的作用就是为了平衡生产者和消费者的处理能力，起到一个数据缓存的作用，同时也达到了一个解耦的作用**

**生产者-消费者模式的特点**

* 保证生产者不会在缓冲区满的时候继续向缓冲区放入数据，而消费者也不会在缓冲区空的时候，消耗数据
* 当缓冲区满的时候，生产者会进入休眠状态，当下次消费者开始消耗缓冲区的数据时，生产者才会被唤醒，开始往缓冲区中添加数据；当缓冲区空的时候，费者也会进入休眠状态，直到生产者往缓冲区中添加数据时才会被唤醒

![在这里插入图片描述](https://img-blog.csdnimg.cn/2019071315322087.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9xaXVfY3I=,size_16,color_FFFFFF,t_70)

**生产者-消费者模式的应用场景**

生产者-消费者模式一般用于将生产数据的一方和消费数据的一方分割开来，将生产数据与消费数据的过程解耦开来

* Excutor任务执行框架：
 通过将任务的提交和任务的执行解耦开来，提交任务的操作相当于生产者，执行任务的操作相当于消费者
  例如使用Excutor构建web服务器，用于处理线程的请求：生产者将任务提交给线程池，线程池创建线程处理任务，如果需要运行的任务数大于线程池的基本线程数，那么就把任务扔到阻塞队列（通过线程池+阻塞队列的方式比只使用一个阻塞队列的效率高很多，因为消费者能够处理就直接处理掉了，不用每个消费者都要先从阻塞队列中取出任务再执行）
* 消息中间件activeMQ:
 双十一的时候，会产生大量的订单，那么不可能同时处理那么多的订单，需要将订单放入一个队列里面，然后由专门的线程处理订单。这里用户下单就是生产者，处理订单的线程就是消费者；再比如12306的抢票功能，先由一个容器存储用户提交的订单，然后再由专门处理订单的线程慢慢处理，这样可以在短时间内支持高并发服务
* 任务的处理时间比较长的情况下：
 比如上传附近并处理，那么这个时候可以将用户上传和处理附件分成两个过程，用一个队列暂时存储用户上传的附近，然后立刻返回用户上传成功，然后有专门的线程处理队列中的附近

**生产者-消费者模式的优点**

* 解耦：将生产者类和消费者类进行解耦，消除代码之间的依赖性，简化工作负载的管理
* 复用：通过将生产者类和消费者类独立开来，那么可以对生产者类和消费者类进行独立的复用与扩展
* 调整并发数：由于生产者和消费者的处理速度是不一样的，可以调整并发数，给予慢的一方多的并发数，来提高任务的处理速度
* 异步：对于生产者和消费者来说能够各司其职，生产者只需要关心缓冲区是否还有数据，不需要等待消费者处理完；同样的对于消费者来说，也只需要关注缓冲区的内容，不需要关注生产者，通过异步的方式支持高并发，将一个耗时的流程拆成生产和消费两个阶段，这样生产者因为执行put()的时间比较短，而支持高并发
* 支持分布式：生产者和消费者通过队列进行通讯，所以不需要运行在同一台机器上，在分布式环境中可以通过redis的list作为队列，而消费者只需要轮询队列中是否有数据。同时还能支持集群的伸缩性，当某台机器宕掉的时候，不会导致整个集群宕掉



#### 谈谈不同的生产者消费者模型

**单生产者单消费者 SPSC**

对于单生产者单消费者，**只用保证缓冲区满的时候，生产者不会继续向缓冲区放数据，缓冲区空的时候，消费者不会继续从缓冲区取数据，而不存在同时有两个生产者使用缓冲区资源，造成数据不一致的状态。**

所以对于单生产者单消费者，如果采用信号量模型来实现的话，那么只需要两个信号量：empytyCount和fullCount分别来表示缓冲区满或者空的状态，进而能够更加容易控制消费者和生产者到底什么时候处于阻塞状态，什么时候处于运行状态; 而不需要使用互斥信号量了

```C
emptyCount = N ; fullCount = 0 ;

produce:
    P(emptyCount)//信号量emptyCount减一
    putItemIntoQueue(item)//执行put操作
    V(fullCount)//信号量fullCount加一

consume:
    P(fullCount)//fullCount -= 1
    item ← getItemFromQueue()
    V(emptyCount)//emptyCount += 1
```

**多生产者单消费者 MPSC**

对于多生产者单消费者来说，**多生产者之间具有互斥关系，所以这里需要一个互斥锁来实现缓冲区的互斥访问，那么具体的实现方式就是在单生产者单消费者的基础之上，加一个互斥信号量useQueue**

如果采用信号量来实现的话可以如下：

```c
emptyCount = N ; fullCount = 0 ; useQueue = 1

produce:
    P(emptyCount)//信号量emptyCount减一
    P(useQueue)//二值信号量useQueue减一，变为0（其他线程不能进入缓冲区，阻塞状态）
    putItemIntoQueue(item)//执行put操作
    V(useQueue)//二值信号量useQueue加一，变为1（其他线程可以进入缓冲区）
    V(fullCount)//信号量fullCount加一

consume:
    P(fullCount)//fullCount -= 1   
    item ← getItemFromQueue()
    V(emptyCount)//emptyCount += 1
```

具体的实现和单生产者单消费者差不多，只不过在生产者类里面多加了一个互斥信号量useQueue

**单生产者多消费者 SPMC**

```C
emptyCount = N ; fullCount = 0 ; useQueue = 1
    
produce:
    P(emptyCount)//信号量emptyCount减一
    putItemIntoQueue(item)//执行put操作
    V(fullCount)//信号量fullCount加一

consume:
    P(fullCount)//fullCount -= 1   
    P(useQueue)//二值信号量useQueue减一，变为0（其他线程不能进入缓冲区，阻塞状态）
    item ← getItemFromQueue()
    V(useQueue)//二值信号量useQueue加一，变为1（其他线程可以进入缓冲区）
    V(emptyCount)//emptyCount += 1
```

具体的实现和单生产者单消费者差不多，只不过在消费者类里面多加了一个互斥信号量useQueue

**多生产者多消费者-单缓冲区 MPMC-SB**

对于多生产者多消费者问题，是一个同步+互斥问题，不仅需要生产者和消费者之间的同步协作，还需要实现对缓冲区资源的互斥访问；这个可以参考前面对生产者消费者4种实现方式

```C
emptyCount = N ; fullCount = 0 ; useQueue = 1
    
produce:
    P(emptyCount)//信号量emptyCount减一
    P(useQueue)//二值信号量useQueue减一，变为0（其他线程不能进入缓冲区，阻塞状态）
    putItemIntoQueue(item)//执行put操作
    V(useQueue)//二值信号量useQueue加一，变为1（其他线程可以进入缓冲区）
    V(fullCount)//信号量fullCount加一

consume:
    P(fullCount)//fullCount -= 1   
    P(useQueue)//二值信号量useQueue减一，变为0（其他线程不能进入缓冲区，阻塞状态）
    item ← getItemFromQueue()
    V(useQueue)//二值信号量useQueue加一，变为1（其他线程可以进入缓冲区）
    V(emptyCount)//emptyCount += 1
```

**多生产者多消费者-双缓冲区 MPMC-MB**

**为什么要用双缓冲区：读写分离减少释放锁和获取锁的开销**

用一个缓冲区，生产者和消费者需要先获取到缓冲区的锁才能进行put和take操作，每一次put和take都需要获取一次锁，这需要大量的同步与互斥操作，十分损耗性能。

所以如果采用双缓冲区的话，一个缓冲区bufferA用于生产者执行put操作，一个缓冲区bufferB用于消费者执行take操作；生产者线程和消费者线程在使用各自的缓冲区之前都需要先获取到缓冲区对应的锁，才能进行操作；

生产者和消费者各自使用自己独立的缓冲区，那么就不存在同一个缓冲区被put的同时进行take操作

所以一旦生产者和消费者一旦获取到了对应缓冲区的锁，那么每一次执行put/take操作时就不用再次重新获取锁了，从而减少了很多获取锁、释放锁的性能开销

**缓冲区的切换**

如果bufferA被put满了，那么生产者释放bufferA的锁，并等待消费者释放bufferB的锁；当bufferB被take空了，消费者释放bufferB的锁，此时生产者获取到bufferB的锁，对bufferB进行put;消费者获取到bufferA的锁，对bufferA进行take,那么就完成了一次缓冲区的切换

**双缓冲区的状态**

* 并发读写：bufferA和bufferB都处于工作状态，一个读一个写
* 单个缓冲区空闲：假设bufferA已经满了，那么生产者就会释放bufferA的锁，尝试获取bufferB，而此时bufferB还在执行take操作，消费者还没释放bufferB的锁，那么生产者进入等待状态
* 缓冲区的切换：当bufferB为空，那么此时消费者释放bufferB的锁，尝试获取bufferA的锁，此时消费者被唤醒，重新尝试获取bufferB的锁

**双缓冲区的死锁问题**

如果操作完当前的缓冲区之后，先获取另外一个缓冲区的锁，再释放当前缓冲区的锁，就会发生死锁问题。如果bufferA和bufferB的线程同时尝试获取对方的锁，那么就会一直循环等待下去（多几个缓冲区降低死锁概率）

**需要注意的问题**

由于双缓冲区是为了避免每次读写的时候不用进行同步与互斥操作，所以对于一些本来就是线程安全的类例如arrayblockingqueue就不适合作为双缓冲区，因为他们内部已经实现了每次读写操作的时候进行加锁和释放

**应用场景：**

* 共享内存和共享文件
* 逻辑处理线程和IO处理线程分离。 I/0处理线程负责网络数据的发送和接收，连接的建立和维护。 逻辑处理线程处理从IO线程接收到的包。

**多生产者多消费者-多缓冲区 MPMC-MB**

多个缓冲区构成一个缓冲池，同样需要两个同步信号量emtpyCount和fullCount，还有一个互斥信号量useQueue,同时还需要两个变量指示哪些是空缓冲区哪些是有数据的缓冲区，**多缓冲区和双缓冲区一样，同样是以空间换时间，减少单个读写操作的同步与互斥操作，对于同一个缓冲区而言，不可能同时会put和take**

**多生产者多消费者-环形缓冲区 MPMC-RingBuffer**

为什么要引入环形缓冲区
讨论为什么要引入环形缓冲区，其实也就是在讨论队列缓冲区有什么弊端，而环形缓冲区是如何解决这种弊端的

那么我们先认识一下什么是环形缓冲区

* 循环缓冲区的有用特性是，当使用一个循环缓冲区时，它不需要将其元素打乱。
* 所有的 push/pop 操作都是在一个固定的存储空间内进行，少掉了对于缓冲区元素所存储空间的分配、释放

队列缓冲区

* 如果使用非循环缓冲区，那么在使用一个缓冲区时，需要移动所有元素
* 在执行push和pop操作时，涉及到内存的分配与释放开销大

判空/判满

* 读写指针同一位置

* 区分：

  * 空出一个存储单元/镜像
  * 镜像指示位：如果写指针超出了读指针n位，在普通情况下，读写指针重合，但在这种情况下，写指针在镜像空间的读指针+n位，不和读指针重合

**buffer 满了怎么办 ?**

实时流：这种方案优先保证实时性。如果buffer满了，优先丢弃老的数据，让新数据存进来

存储流：这种方案优先保证稳定性。如果buffer满了，舍弃最新的数据，等待老的流处理完再进新的流进行处理

#### 哲学家就餐问题？

先来看看哲学家就餐的问题描述：

* `5` 个老大哥哲学家，闲着没事做，围绕着一张圆桌吃面；
* 巧就巧在，这个桌子只有 `5` 支叉子，每两个哲学家之间放一支叉子；
* 哲学家围在一起先思考，思考中途饿了就会想进餐；
* **奇葩的是，这些哲学家要两支叉子才愿意吃面，也就是需要拿到左右两边的叉子才进餐**；
* **吃完后，会把两支叉子放回原处，继续思考**；

那么问题来了，如何保证哲 学家们的动作有序进行，而不会出现有人永远拿不到叉子呢？

* 在拿叉子前，加个互斥信号量。只有一个人能吃。
* 奇数先拿左边后拿右边，偶数先拿右边后拿左边。
* 信号量给哲学家三个状态，进餐、思考、饥饿（准备拿叉子），一个哲学家只有在两个邻居都没有进餐时，才可以进入进餐状态。
* 集中式管理。

> 方案一

我们用信号量的方式，也就是 PV 操作来尝试解决它，代码如下：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/24-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E4%B8%80%E7%A4%BA%E4%BE%8B.jpg)

上面的程序，好似很自然。拿起叉子用 P 操作，代表有叉子就直接用，没有叉子时就等待其他哲学家放回叉子。

![方案一的问题](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/25-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E4%B8%80%E9%97%AE%E9%A2%98.jpg)

不过，这种解法存在一个极端的问题：**假设五位哲学家同时拿起左边的叉子，桌面上就没有叉子了， 这样就没有人能够拿到他们右边的叉子，也就说每一位哲学家都会在 `P(fork[(i + 1) % N ])` 这条语句阻塞了，很明显这发生了死锁的现象**。

> 方案二

既然「方案一」会发生同时竞争左边叉子导致死锁的现象，那么我们就在拿叉子前，加个互斥信号量，代码如下：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/26-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E4%BA%8C%E7%A4%BA%E4%BE%8B.jpg)

上面程序中的互斥信号量的作用就在于，**只要有一个哲学家进入了「临界区」，也就是准备要拿叉子时，其他哲学家都不能动，只有这位哲学家用完叉子了，才能轮到下一个哲学家进餐。**

![方案二的问题](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/27-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E4%BA%8C%E9%97%AE%E9%A2%98.jpg)

方案二虽然能让哲学家们按顺序吃饭，但是每次进餐只能有一位哲学家，而桌面上是有 5 把叉子，按道理是能可以有两个哲学家同时进餐的，所以从效率角度上，这不是最好的解决方案。

> 方案三

那既然方案二使用互斥信号量，会导致只能允许一个哲学家就餐，那么我们就不用它。

另外，方案一的问题在于，会出现所有哲学家同时拿左边刀叉的可能性，那我们就避免哲学家可以同时拿左边的刀叉，采用分支结构，根据哲学家的编号的不同，而采取不同的动作。

**即让偶数编号的哲学家「先拿左边的叉子后拿右边的叉子」，奇数编号的哲学家「先拿右边的叉子后拿左边的叉子」。**

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/28-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E4%B8%89%E7%A4%BA%E4%BE%8B.jpg)

上面的程序，在 P 操作时，根据哲学家的编号不同，拿起左右两边叉子的顺序不同。另外，V 操作是不需要分支的，因为 V 操作是不会阻塞的。

![方案三可解决问题](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/29-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E4%B8%89-%E5%9B%BE%E8%A7%A3.jpg)

方案三即不会出现死锁，也可以两人同时进餐。

> 方案四

在这里再提出另外一种可行的解决方案，我们**用一个数组 state 来记录每一位哲学家的三个状态，分别是在进餐状态、思考状态、饥饿状态（正在试图拿叉子）。**

那么，**一个哲学家只有在两个邻居都没有进餐时，才可以进入进餐状态。**

第 `i` 个哲学家的左邻右舍，则由宏 `LEFT` 和 `RIGHT` 定义：

* *LEFT* : ( i + 5 - 1 ) % 5
* *RIGHT* : ( i + 1 ) % 5

比如 i 为 2，则 `LEFT` 为 1，`RIGHT` 为 3。

具体代码实现如下：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/30-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E5%9B%9B%E7%A4%BA%E4%BE%8B.jpg)

上面的程序使用了一个信号量数组，每个信号量对应一位哲学家，这样在所需的叉子被占用时，想进餐的哲学家就被阻塞。

注意，每个进程/线程将 `smart_person` 函数作为主代码运行，而其他 `take_forks`、`put_forks` 和 `test` 只是普通的函数，而非单独的进程/线程。

![方案四也可解决问题](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/31-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E5%9B%9B-%E5%9B%BE%E8%A7%A3.jpg)

方案四同样不会出现死锁，也可以两人同时进餐

方案五：

上面都是分布式协作的方式，实际上可以采用集中式管理，引入一个服务员，实现起来可能更加方便

#### 读者写者问题？

读者只会读取数据，不会修改数据，而写者即可以读也可以修改数据。

读者-写者的问题描述：

* 「读-读」允许：同一时刻，允许多个读者同时读
* 「读-写」互斥：没有写者时读者才能读，没有读者时写者才能写
* 「写-写」互斥：没有其他写者时，写者才能写

接下来，提出几个解决方案来分析分析。

* 读优先
* 写优先
* 公平

> 方案一

使用信号量的方式来尝试解决：

* 信号量 `wMutex`：控制写操作的互斥信号量，初始值为 1 ；
* 读者计数 `rCount`：正在进行读操作的读者个数，初始化为 0；
* 信号量 `rCountMutex`：控制对 rCount 读者计数器的互斥修改，初始值为 1；

接下来看看代码的实现：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/32-%E8%AF%BB%E8%80%85%E5%86%99%E8%80%85-%E6%96%B9%E6%A1%88%E4%B8%80%E7%A4%BA%E4%BE%8B.jpg)

上面的这种实现，是读者优先的策略，因为只要有读者正在读的状态，后来的读者都可以直接进入，如果读者持续不断进入，则写者会处于饥饿状态。

> 方案二

那既然有读者优先策略，自然也有写者优先策略：

* 只要有写者准备要写入，写者应尽快执行写操作，后来的读者就必须阻塞；
* 如果有写者持续不断写入，则读者就处于饥饿；

在方案一的基础上新增如下变量：

* 信号量 `rMutex`：控制读者进入的互斥信号量，初始值为 1；
* 信号量 `wDataMutex`：控制写者写操作的互斥信号量，初始值为 1；
* 写者计数 `wCount`：记录写者数量，初始值为 0；
* 信号量 `wCountMutex`：控制 wCount 互斥修改，初始值为 1；

具体实现如下代码：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/33-%E8%AF%BB%E8%80%85%E5%86%99%E8%80%85-%E6%96%B9%E6%A1%88%E4%BA%8C%E7%A4%BA%E4%BE%8B.jpg)

注意，这里 `rMutex` 的作用，开始有多个读者读数据，它们全部进入读者队列，此时来了一个写者，执行了 `P(rMutex)` 之后，后续的读者由于阻塞在 `rMutex` 上，都不能再进入读者队列，而写者到来，则可以全部进入写者队列，因此保证了写者优先。

同时，第一个写者执行了 `P(rMutex)` 之后，也不能马上开始写，必须等到所有进入读者队列的读者都执行完读操作，通过 `V(wDataMutex)` 唤醒写者的写操作。

> 方案三

既然读者优先策略和写者优先策略都会造成饥饿的现象，那么我们就来实现一下公平策略。

公平策略：

* 优先级相同；
* 写者、读者互斥访问；
* 只能一个写者访问临界区；
* 可以有多个读者同时访问临界资源；

具体代码实现：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/34-%E8%AF%BB%E8%80%85%E5%86%99%E8%80%85-%E6%96%B9%E6%A1%88%E4%B8%89%E7%A4%BA%E4%BE%8B.jpg)

看完代码不知你是否有这样的疑问，为什么加了一个信号量 `flag`，就实现了公平竞争？

对比方案一的读者优先策略，可以发现，读者优先中只要后续有读者到达，读者就可以进入读者队列， 而写者必须等待，直到没有读者到达。

没有读者到达会导致读者队列为空，即 `rCount==0`，此时写者才可以进入临界区执行写操作。

而这里 `flag` 的作用就是阻止读者的这种特殊权限（特殊权限是只要读者到达，就可以进入读者队列）。

比如：开始来了一些读者读数据，它们全部进入读者队列，此时来了一个写者，执行 `P(falg)` 操作，使得后续到来的读者都阻塞在 `flag` 上，不能进入读者队列，这会使得读者队列逐渐为空，即 `rCount` 减为 0。

这个写者也不能立马开始写（因为此时读者队列不为空），会阻塞在信号量 `wDataMutex` 上，读者队列中的读者全部读取结束后，最后一个读者进程执行 `V(wDataMutex)`，唤醒刚才的写者，写者则继续开始进行写操作。
